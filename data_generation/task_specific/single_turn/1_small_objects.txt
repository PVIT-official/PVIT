{"img_id": 26374, "query": [["A man in a blue uniform kicking the soccer ball", "A man is running and kicking a soccer ball.", "A soccer player is trying to control the ball. ", "A male soccer player in a full soccer uniform and jumping in the air over a soccer ball.", "A soccer player is kicking a soccer ball. "], {"sports ball": [[0.367, 0.829, 0.555, 0.963], [0.0, 0.339, 0.265, 0.58]], "person": [[0.095, 0.06, 0.924, 0.948]]}], "response": "Task: Where is the soccer player aiming the object placed in <Region>[0.367, 0.829, 0.555, 0.963]</Region> and what technique might he be using to aim at that spot?\nOutput: The soccer player is likely aiming the ball towards the goalpost, and he might be using a technique called \"curving\" to land the ball at a specific spot and score a goal.", "instruction": "Where is the soccer player aiming the object placed in <Region>[0.367, 0.829, 0.555, 0.963]</Region> and what technique might he be using to aim at that spot?", "input": "", "output": "The soccer player is likely aiming the ball towards the goalpost, and he might be using a technique called \"curving\" to land the ball at a specific spot and score a goal.", "is_classification": false, "dataset": "COCO", "img_info": "000000026374.jpg"}
{"img_id": 47611, "query": [["A clock on the wall between benches and tables on a stone floor.", "A room with some tables and chairs and a clock in the middle.", "a brown tiled floor some tables chairs and a white clock", "A room with tables and a clock on the wall.", "A wall clock in a restaurant stating the time of 6:10."], {"clock": [[0.345, 0.049, 0.638, 0.287]], "chair": [[0.001, 0.521, 0.202, 0.803], [0.74, 0.521, 1.0, 0.799], [0.604, 0.395, 1.0, 0.663], [0.04, 0.412, 0.389, 0.66]], "dining table": [[0.64, 0.473, 1.0, 0.543], [0.005, 0.473, 0.33, 0.552]]}], "response": "Task: What is the time on the clock located at <Region>[0.345, 0.049, 0.638, 0.287]</Region>?\nOutput: The time on the clock is not specified in the descriptions.", "instruction": "What is the time on the clock located at <Region>[0.345, 0.049, 0.638, 0.287]</Region>?", "input": "", "output": "The time on the clock is not specified in the descriptions.", "is_classification": false, "dataset": "COCO", "img_info": "000000047611.jpg"}
{"img_id": 56615, "query": [["A street with two police cars in one lane and a bus in the other lane that has it's door open.", "Two cabs on the road next to a bus with a lady alighting. ", "a bus parked next to a couple of cop cars ", "A city bus driving by two parked police cars on a city street.", "People walking on a sidewalk next to a store while two police cars and a bus drive in the road by them."], {"car": [[0.398, 0.415, 1.0, 0.977], [0.0, 0.481, 0.322, 0.992]], "bus": [[0.0, 0.003, 0.782, 0.67]], "person": [[0.55, 0.24, 0.621, 0.512], [0.738, 0.326, 0.796, 0.459], [0.853, 0.32, 0.904, 0.453], [0.255, 0.197, 0.336, 0.373], [0.067, 0.236, 0.111, 0.361], [0.744, 0.312, 0.77, 0.411], [0.811, 0.315, 0.848, 0.41], [0.221, 0.237, 0.272, 0.331], [0.506, 0.241, 0.591, 0.514]], "backpack": [[0.742, 0.326, 0.769, 0.38], [0.866, 0.344, 0.884, 0.394]]}], "response": "Task: What is the main object located in <Region>[0.0, 0.003, 0.782, 0.67]</Region> and what is happening in the image?\nOutput: The main object in the image is a bus situated on the road with an opened door. People are seen walking on the pavement while two police cars and a bus drive by them. It seems like the bus has stopped and is picking up passengers.", "instruction": "What is the main object located in <Region>[0.0, 0.003, 0.782, 0.67]</Region> and what is happening in the image?", "input": "", "output": "The main object in the image is a bus situated on the road with an opened door. People are seen walking on the pavement while two police cars and a bus drive by them. It seems like the bus has stopped and is picking up passengers.", "is_classification": false, "dataset": "COCO", "img_info": "000000056615.jpg"}
{"img_id": 9287, "query": [["A city full of tall buildings surrounding a street.", "A busy, big-city  intersection features many billboards and signs.", "This is a photo of Time Square taken during the day. ", "A city intersection featuring billboards on many skyscrapers. ", "There are street signs at the corner of a large city street."], {"traffic light": [[0.614, 0.629, 0.633, 0.665], [0.323, 0.599, 0.362, 0.692], [0.379, 0.887, 0.436, 0.963]]}], "response": "Task: What is the purpose of traffic lights in <Region>[0.614, 0.629, 0.633, 0.665]</Region>, and how many traffic lights can be seen in this image?\nOutput: The purpose of the traffic lights in the specified region is to control the flow of traffic at the intersection. There are three traffic lights visible in this image.", "instruction": "What is the purpose of traffic lights in <Region>[0.614, 0.629, 0.633, 0.665]</Region>, and how many traffic lights can be seen in this image?", "input": "", "output": "The purpose of the traffic lights in the specified region is to control the flow of traffic at the intersection. There are three traffic lights visible in this image.", "is_classification": false, "dataset": "COCO", "img_info": "000000009287.jpg"}
{"img_id": 25533, "query": [["three skiers are going down a hill and some trees", "A few people are skiing down a snowy slope.", "Cross country skiers on a ski trail between trees.", "A group of people riding skis down a mountain.", "Three people cross country skiing through the snow."], {"person": [[0.199, 0.43, 0.434, 0.888], [0.396, 0.442, 0.484, 0.619], [0.479, 0.45, 0.56, 0.584]], "skis": [[0.213, 0.811, 0.347, 1.0], [0.404, 0.6, 0.483, 0.635], [0.481, 0.579, 0.511, 0.596]], "backpack": [[0.232, 0.56, 0.376, 0.634], [0.401, 0.452, 0.466, 0.514]]}], "response": "Task: What is the purpose of the object in <Region>[0.232, 0.56, 0.376, 0.634]</Region> and <Region>[0.401, 0.452, 0.466, 0.514]</Region> for the people in the image?\nOutput: The objects are backpacks and are used to hold the skiers' personal belongings. They are essential for skiers when skiing on a trail, as they can carry extra hats, gloves, and necessary supplies, such as food or drinks.", "instruction": "What is the purpose of the object in <Region>[0.232, 0.56, 0.376, 0.634]</Region> and <Region>[0.401, 0.452, 0.466, 0.514]</Region> for the people in the image?", "input": "", "output": "The objects are backpacks and are used to hold the skiers' personal belongings. They are essential for skiers when skiing on a trail, as they can carry extra hats, gloves, and necessary supplies, such as food or drinks.", "is_classification": false, "dataset": "COCO", "img_info": "000000025533.jpg"}
{"img_id": 13283, "query": [["A woman wearing a cooks outfit and waving in a kitchen with other cooks.", "A woman in a chefs outfit waving in a kitchen.", "A chef waving for the camera, in a kitchen.", "A group of chefs standing in a kitchen preparing food.", "Cooks in toques and white uniforms in a busy commercial kitchen."], {"bottle": [[0.62, 0.561, 0.656, 0.617], [0.934, 0.488, 0.967, 0.635]], "refrigerator": [[0.115, 0.392, 0.216, 0.724]], "person": [[0.334, 0.345, 0.416, 0.768], [0.393, 0.321, 0.482, 0.881], [0.895, 0.369, 0.926, 0.537], [0.407, 0.356, 0.598, 0.985], [0.0, 0.481, 0.237, 0.98], [0.222, 0.588, 0.374, 0.985]], "knife": [[0.614, 0.719, 0.708, 0.732]], "oven": [[0.045, 0.571, 0.204, 0.727]], "bowl": [[0.707, 0.604, 0.79, 0.669], [0.797, 0.763, 0.903, 0.826], [0.906, 0.895, 1.0, 1.0], [0.618, 0.802, 0.768, 0.904]], "tv": [[0.287, 0.498, 0.35, 0.557]], "clock": [[0.886, 0.0, 0.953, 0.093]]}], "response": "Task: What is the object in <Region>[0.934, 0.488, 0.967, 0.635]</Region>, and why is it important to have it in the kitchen?\nOutput: The object is a bottle, likely containing cooking oil. It is important to have such a bottle in the kitchen for frying or saut\u00e9ing food. The cooks might use this bottle of oil to add flavor to the dishes they are preparing.", "instruction": "What is the object in <Region>[0.934, 0.488, 0.967, 0.635]</Region>, and why is it important to have it in the kitchen?", "input": "", "output": "The object is a bottle, likely containing cooking oil. It is important to have such a bottle in the kitchen for frying or saut\u00e9ing food. The cooks might use this bottle of oil to add flavor to the dishes they are preparing.", "is_classification": false, "dataset": "COCO", "img_info": "000000013283.jpg"}
{"img_id": 57810, "query": [["A woman laying in bed with a child.", "A person laying in a bed with a small child.", "A woman is laying in bed with a small child.", "An adult and a child sleeping in a bed.", "An adult and child in a bed with a blue headboard."], {"bed": [[0.0, 0.413, 1.0, 0.988]], "person": [[0.003, 0.329, 0.626, 0.746], [0.277, 0.414, 0.665, 0.748]], "bottle": [[0.716, 0.108, 0.75, 0.241]], "remote": [[0.616, 0.155, 0.647, 0.195]]}], "response": "Task: Can you describe the position of the child's head in <Region>[0.277, 0.414, 0.665, 0.748]</Region>?\nOutput: The child's head is positioned towards the left edge of the bed and is resting on the pillows and covered by a blanket.", "instruction": "Can you describe the position of the child's head in <Region>[0.277, 0.414, 0.665, 0.748]</Region>?", "input": "", "output": "The child's head is positioned towards the left edge of the bed and is resting on the pillows and covered by a blanket.", "is_classification": false, "dataset": "COCO", "img_info": "000000057810.jpg"}
{"img_id": 13278, "query": [["A man doing a trick on a skateboard on a ramp.", "A man riding a skateboard on top of a ramp.", "A skateboarder is doing a stunt on a railing.", "A man riding on a skateboarding doing a trick.", "a boy doing a trick in the air on a skate board on a skate ramp "], {"person": [[0.274, 0.126, 0.692, 0.8]], "skateboard": [[0.44, 0.647, 0.682, 0.843]]}], "response": "Task: What is the object in <Region>[0.44, 0.647, 0.682, 0.843]</Region>, and where is it being used in the image?\nOutput: The object is a skateboard, and it is being ridden by the person shown in the image on top of a ramp. The person is performing a skateboard trick while being on top of the skateboard.", "instruction": "What is the object in <Region>[0.44, 0.647, 0.682, 0.843]</Region>, and where is it being used in the image?", "input": "", "output": "The object is a skateboard, and it is being ridden by the person shown in the image on top of a ramp. The person is performing a skateboard trick while being on top of the skateboard.", "is_classification": false, "dataset": "COCO", "img_info": "000000013278.jpg"}
{"img_id": 43998, "query": [["A group of mopeds is parked in a parking lot", "The motorcycle parking lot has three portable toilets.", "Motorcyclists gathering in a parking lot to discuss their bikes.", "A group of motorcycles parked side by side in a parking lot.", "A group of people talking around several parked motorcycles. "], {"motorcycle": [[0.38, 0.429, 0.815, 0.894], [0.076, 0.272, 0.548, 0.704], [0.728, 0.237, 0.927, 0.532], [0.805, 0.447, 1.0, 1.0], [0.588, 0.249, 0.769, 0.506], [0.461, 0.222, 0.63, 0.437], [0.081, 0.354, 0.136, 0.484]], "person": [[0.101, 0.106, 0.204, 0.415], [0.239, 0.099, 0.316, 0.332], [0.191, 0.134, 0.254, 0.304], [0.06, 0.131, 0.119, 0.436], [0.341, 0.157, 0.37, 0.306], [0.414, 0.126, 0.449, 0.32], [0.09, 0.152, 0.102, 0.203]], "backpack": [[0.099, 0.161, 0.175, 0.348]], "car": [[0.654, 0.179, 0.721, 0.254], [0.378, 0.19, 0.414, 0.222], [0.074, 0.083, 0.178, 0.206]], "handbag": [[0.052, 0.172, 0.103, 0.324], [0.519, 0.238, 0.575, 0.3]]}], "response": "Task: What type of motorcycles are parked in <Region>[0.461, 0.222, 0.63, 0.437]</Region> in the image?\nOutput: The object in the given region is a motorcycle, and it belongs to the group of motorcycles parked side by side in the parking lot. Unfortunately, it is not possible to tell the specific type of motorcycle based on the information provided.", "instruction": "What type of motorcycles are parked in <Region>[0.461, 0.222, 0.63, 0.437]</Region> in the image?", "input": "", "output": "The object in the given region is a motorcycle, and it belongs to the group of motorcycles parked side by side in the parking lot. Unfortunately, it is not possible to tell the specific type of motorcycle based on the information provided.", "is_classification": false, "dataset": "COCO", "img_info": "000000043998.jpg"}
{"img_id": 47774, "query": [["A black and white photo of a man on a bench", "a man sitting on a wooden bench by his self", "A man is sitting on a bench reading a newspaper.", "Man with a bag sitting on a bench reading a magazine. ", "A man reading a paper on a bench."], {"potted plant": [[0.759, 0.003, 1.0, 0.314], [0.0, 0.0, 0.312, 0.318], [0.263, 0.0, 0.426, 0.249]], "person": [[0.309, 0.074, 0.566, 0.993]], "bench": [[0.118, 0.271, 0.933, 0.863]]}], "response": "Task: What is the man wearing who is located on <Region>[0.309, 0.074, 0.566, 0.993]</Region>?\nOutput: In the image, a man is sitting on a bench, and he is wearing a black top, blue jeans, and white sneakers.", "instruction": "What is the man wearing who is located on <Region>[0.309, 0.074, 0.566, 0.993]</Region>?", "input": "", "output": "In the image, a man is sitting on a bench, and he is wearing a black top, blue jeans, and white sneakers.", "is_classification": false, "dataset": "COCO", "img_info": "000000047774.jpg"}
{"img_id": 59152, "query": [["There are stop signs and one way signs at this intersection.", "A collection of various street signs by a big building.", "a building with windows facing a street with lots of one way street signs and a stop sign", "Several \"One Way\" Signs are placed near an \"All Way\" Stop sign. ", "A stop sign on a pole in a street."], {"stop sign": [[0.61, 0.426, 0.771, 0.724]]}], "response": "Task: What is the shape and color of the object in <Region>[0.61, 0.426, 0.771, 0.724]</Region>?\nOutput: The object is a stop sign, as per the image. It has a unique octagonal shape and has a bright red color with white letters that spell STOP.", "instruction": "What is the shape and color of the object in <Region>[0.61, 0.426, 0.771, 0.724]</Region>?", "input": "", "output": "The object is a stop sign, as per the image. It has a unique octagonal shape and has a bright red color with white letters that spell STOP.", "is_classification": false, "dataset": "COCO", "img_info": "000000059152.jpg"}
{"img_id": 43171, "query": [["Side by side images of a bathroom and a dining room.", "A picture of two rooms that are in a house.", "An open floor plan includes a divider wall between a dining room and a bathroom.", "White bathroom and living room with wooden ornaments. ", "a dog sitting beneath  table and chairs"], {"dog": [[0.688, 0.8, 0.833, 0.958]], "chair": [[0.502, 0.607, 0.649, 0.976], [0.775, 0.658, 0.855, 0.852], [0.597, 0.526, 0.683, 0.844], [0.88, 0.547, 1.0, 0.912], [0.828, 0.53, 0.877, 0.669]], "dining table": [[0.465, 0.548, 0.9, 0.928]], "bowl": [[0.773, 0.551, 0.846, 0.605], [0.682, 0.548, 0.756, 0.597], [0.751, 0.539, 0.804, 0.592]], "cup": [[0.667, 0.576, 0.689, 0.615], [0.627, 0.545, 0.646, 0.58]]}], "response": "Task: Can you describe the position of the object located in <Region>[0.688, 0.8, 0.833, 0.958]</Region> and its relationship to the image?\nOutput: The object is a dog, sitting underneath a dining table, together with chairs and bowls of food and drinks. This indicates that the image is likely showing a dining room, where people enjoy their meals together with their pets.", "instruction": "Can you describe the position of the object located in <Region>[0.688, 0.8, 0.833, 0.958]</Region> and its relationship to the image?", "input": "", "output": "The object is a dog, sitting underneath a dining table, together with chairs and bowls of food and drinks. This indicates that the image is likely showing a dining room, where people enjoy their meals together with their pets.", "is_classification": false, "dataset": "COCO", "img_info": "000000043171.jpg"}
{"img_id": 31061, "query": [["A young man is sitting down with his skateboard. ", "a fisheye lens photograph of a man sitting in a chair holding a skateboard.", "A man sitting in a chair fixing a skateboard.", "A man sitting on a chair holding up a  skateboard.", "A man working on a skateboard in a kitchen."], {"chair": [[0.002, 0.66, 0.112, 0.804], [0.486, 0.719, 0.685, 0.89]], "person": [[0.267, 0.394, 0.771, 0.879], [0.05, 0.408, 0.209, 0.693], [0.703, 0.583, 1.0, 1.0]], "skateboard": [[0.299, 0.557, 0.737, 0.73]], "cup": [[0.072, 0.489, 0.103, 0.526], [0.1, 0.494, 0.121, 0.516]], "dining table": [[0.004, 0.525, 0.199, 0.745]], "sink": [[0.0, 0.503, 0.076, 0.521]]}], "response": "Task: In the image, what is the man in <Region>[0.299, 0.557, 0.737, 0.73]</Region> doing with his skateboard?\nOutput: The man in the image is holding his skateboard while sitting. It appears that he might be adjusting the wheels or some other aspect of the board.", "instruction": "In the image, what is the man in <Region>[0.299, 0.557, 0.737, 0.73]</Region> doing with his skateboard?", "input": "", "output": "The man in the image is holding his skateboard while sitting. It appears that he might be adjusting the wheels or some other aspect of the board.", "is_classification": false, "dataset": "COCO", "img_info": "000000031061.jpg"}
{"img_id": 42471, "query": [["A very tall Clocktower ascending into the sky", "A large, two-handed clock tower sits atop an old building, with spires sticking up from its height. ", "A tall clock tower on a cloudy day.", "Looking up at an old stone clock tower", "a red and yellow tower with some clocks on the side of it "], {"clock": [[0.542, 0.427, 0.679, 0.549], [0.335, 0.44, 0.443, 0.564]]}], "response": "Task: What is the position of the object located in <Region>[0.542, 0.427, 0.679, 0.549]</Region> in the image?\nOutput: This image shows a clock tower, and the object is a clock. The clock is positioned on the middle section of the clock tower, between two arched windows.", "instruction": "What is the position of the object located in <Region>[0.542, 0.427, 0.679, 0.549]</Region> in the image?", "input": "", "output": "This image shows a clock tower, and the object is a clock. The clock is positioned on the middle section of the clock tower, between two arched windows.", "is_classification": false, "dataset": "COCO", "img_info": "000000042471.jpg"}
{"img_id": 48035, "query": [["A man in the air on a skateboard doing a trick.", "A man riding a skateboard up into the air.", "black and white photo of man jumping a skateboard", "a man is doing a trick on a skateboard", "A man that is in the air with a skateboard."], {"person": [[0.337, 0.191, 0.92, 0.47]], "skateboard": [[0.377, 0.426, 0.596, 0.502]]}], "response": "Task: Describe the position and movement of the skateboard in <Region>[0.377, 0.426, 0.596, 0.502]</Region> with respect to the person in the image.\nOutput: The skateboard is held by the person in the image and used for doing a trick. The specific location of the skateboard is not shown, but the person is in the air while performing a skateboard trick.", "instruction": "Describe the position and movement of the skateboard in <Region>[0.377, 0.426, 0.596, 0.502]</Region> with respect to the person in the image.", "input": "", "output": "The skateboard is held by the person in the image and used for doing a trick. The specific location of the skateboard is not shown, but the person is in the air while performing a skateboard trick.", "is_classification": false, "dataset": "COCO", "img_info": "000000048035.jpg"}
{"img_id": 62030, "query": [["A man holding a tennis racquet on a tennis court.", "A man taking a swing at a tennis ball", "Black and white photo of a man playing tennis.", "an image of a guy on a tennis court hitting ball", "A man on a tennis court about to swipe at a tennis ball"], {"person": [[0.727, 0.427, 0.811, 0.737]], "tennis racket": [[0.674, 0.501, 0.734, 0.555]]}], "response": "Task: What action is the person in the image most likely to perform with the object in <Region>[0.674, 0.501, 0.734, 0.555]</Region>?\nOutput: The person in the image is most likely to hit a tennis ball with the tennis racket in his hand, as they are standing on a tennis court.", "instruction": "What action is the person in the image most likely to perform with the object in <Region>[0.674, 0.501, 0.734, 0.555]</Region>?", "input": "", "output": "The person in the image is most likely to hit a tennis ball with the tennis racket in his hand, as they are standing on a tennis court.", "is_classification": false, "dataset": "COCO", "img_info": "000000062030.jpg"}
{"img_id": 35407, "query": [["A sandwich sitting on a white plate on an orange tray next to a drink.", "A cheeseburger sits on a white plate, next to a drink in a Dunkin Donuts cup.", "Dunkin' Donuts serves up more than just doughnuts.", "A sandwich is sitting on a plate white a Dunkin Donuts cup.", "A sandwich and condiments sit on a white plate next to a drink."], {"sandwich": [[0.325, 0.108, 0.837, 0.71]], "dining table": [[0.0, 0.0, 1.0, 1.0]], "cup": [[0.002, 0.0, 0.325, 0.452]], "knife": [[0.931, 0.193, 1.0, 0.452]]}], "response": "Task: How can the object located in <Region>[0.931, 0.193, 1.0, 0.452]</Region> be used in relation to the sandwich?\nOutput: The object is identified as a knife and can be used to cut through the sandwich to make eating it easier.", "instruction": "How can the object located in <Region>[0.931, 0.193, 1.0, 0.452]</Region> be used in relation to the sandwich?", "input": "", "output": "The object is identified as a knife and can be used to cut through the sandwich to make eating it easier.", "is_classification": false, "dataset": "COCO", "img_info": "000000035407.jpg"}
{"img_id": 25162, "query": [["A man us hitting a baseball in the field", "A black and white photo of a young man playing baseball.", "A young baseball player holding a wooden bat\n", "A teenage boy wearing a black helmet swings his baseball bat", "A young man is on a field with a baseball bat."], {"person": [[0.187, 0.161, 0.661, 0.913]], "baseball bat": [[0.106, 0.321, 0.562, 0.377]]}], "response": "Task: What is the object located in <Region>[0.106, 0.321, 0.562, 0.377]</Region> used for in the image?\nOutput: The object located in the specified region is a baseball bat. It is used to strike the baseball thrown by the pitcher and hit it as far as possible out in the field of play.", "instruction": "What is the object located in <Region>[0.106, 0.321, 0.562, 0.377]</Region> used for in the image?", "input": "", "output": "The object located in the specified region is a baseball bat. It is used to strike the baseball thrown by the pitcher and hit it as far as possible out in the field of play.", "is_classification": false, "dataset": "COCO", "img_info": "000000025162.jpg"}
{"img_id": 24107, "query": [["A skier making his way down a snowy slope", "a man that is skiing down a snowy hill", "A man riding skis down a snow covered slope.", "A person is skiing on snow covered mountains.", "A person on ski's going down a small hill in the snow . "], {"backpack": [[0.312, 0.571, 0.365, 0.632], [0.282, 0.62, 0.319, 0.695]], "person": [[0.218, 0.525, 0.449, 0.791]], "skis": [[0.219, 0.704, 0.514, 0.828]]}], "response": "Task: What objects does the skier in the image comprise, as shown in <Region>[0.219, 0.704, 0.514, 0.828]</Region> region?\nOutput: This image portrays a skier skiing down a snowy hill, and the objects are skis.", "instruction": "What objects does the skier in the image comprise, as shown in <Region>[0.219, 0.704, 0.514, 0.828]</Region> region?", "input": "", "output": "This image portrays a skier skiing down a snowy hill, and the objects are skis.", "is_classification": false, "dataset": "COCO", "img_info": "000000024107.jpg"}
{"img_id": 50379, "query": [["A man on an orange and black motorcycle wearing matching gear.", "A motorcycle rider is near a crowd on the sidewalk.", "A man riding on the back of a motorcycle down a street.", "People gather around to watch and film a guy on a motorcycle", "A person on a motorcycle on a street."], {"car": [[0.032, 0.495, 0.375, 0.77], [0.824, 0.486, 1.0, 0.987]], "motorcycle": [[0.341, 0.507, 0.575, 0.856]], "person": [[0.002, 0.153, 0.066, 0.848], [0.937, 0.428, 0.993, 0.526], [0.561, 0.595, 0.668, 0.792], [0.584, 0.422, 0.717, 0.797], [0.692, 0.417, 0.74, 0.769], [0.722, 0.349, 0.812, 0.781], [0.863, 0.372, 0.941, 0.557], [0.339, 0.356, 0.513, 0.832], [0.222, 0.524, 0.252, 0.566], [0.009, 0.194, 0.036, 0.428], [0.79, 0.375, 0.864, 0.777]], "handbag": [[0.894, 0.439, 0.932, 0.537], [0.682, 0.585, 0.712, 0.641]], "cell phone": [[0.685, 0.504, 0.702, 0.53]]}], "response": "Task: Based on the image, what is the safety concern for the rider of the object located in <Region>[0.341, 0.507, 0.575, 0.856]</Region>?\nOutput: The object is a motorcycle, and the safety concern for the rider is the possibility of a collision with the other vehicles on the road. The motorcycle rider is in close proximity to a car on their left and a car on their right. Additionally, there is a crowd of people on the sidewalk who could pose a potential obstacle or distraction.", "instruction": "Based on the image, what is the safety concern for the rider of the object located in <Region>[0.341, 0.507, 0.575, 0.856]</Region>?", "input": "", "output": "The object is a motorcycle, and the safety concern for the rider is the possibility of a collision with the other vehicles on the road. The motorcycle rider is in close proximity to a car on their left and a car on their right. Additionally, there is a crowd of people on the sidewalk who could pose a potential obstacle or distraction.", "is_classification": false, "dataset": "COCO", "img_info": "000000050379.jpg"}
{"img_id": 2001, "query": [["A person on a surfboard in the water.", "a person riding a surf board with a parachute ", "A person is holding onto a handle connected to a rope gliding on the water.", "A man kiteboarding on top of a wave in the ocean.", "A person in the water holding on the a handle with wires."], {"person": [[0.195, 0.253, 0.518, 0.917]], "surfboard": [[0.48, 0.723, 0.71, 0.903]]}], "response": "Task: What is the person doing in the image and what object are they using according to the object located in <Region>[0.48, 0.723, 0.71, 0.903]</Region>?\nOutput: The person in the image is surfing, and the object is a surfboard. The surfer is standing on the surfboard and using it to ride waves in the water.", "instruction": "What is the person doing in the image and what object are they using according to the object located in <Region>[0.48, 0.723, 0.71, 0.903]</Region>?", "input": "", "output": "The person in the image is surfing, and the object is a surfboard. The surfer is standing on the surfboard and using it to ride waves in the water.", "is_classification": false, "dataset": "COCO", "img_info": "000000002001.jpg"}
{"img_id": 8968, "query": [["A man riding skis down a snow covered ski slope.", "A person is skiing down a snow covered slope.", "a snow skier wearing all black going down a hill", "A person on skis is skiing down a snow slope.", "A person on skis kicking up snow on a slope."], {"person": [[0.465, 0.313, 0.756, 0.562]], "skis": [[0.668, 0.546, 0.835, 0.623]]}], "response": "Task: What is the person in <Region>[0.465, 0.313, 0.756, 0.562]</Region> doing in the image?\nOutput: The person in this region is skiing down a snow-covered slope. We can see that the person is dressed up in all black and wearing skis. This region of the image shows the person in action, skiing down the slope, having a thrilling experience.", "instruction": "What is the person in <Region>[0.465, 0.313, 0.756, 0.562]</Region> doing in the image?", "input": "", "output": "The person in this region is skiing down a snow-covered slope. We can see that the person is dressed up in all black and wearing skis. This region of the image shows the person in action, skiing down the slope, having a thrilling experience.", "is_classification": false, "dataset": "COCO", "img_info": "000000008968.jpg"}
{"img_id": 382, "query": [["landscape of a snowy mountain with a skiers trail in the snow", "A man riding skis down a snow covered slope.", "A high mountain of snow with a cross country skier.", "A distance shot shows rolling snowy hills, partially shadowed, with ski prints reaching off into the distance, and to the foreground, a single skier. ", "A mountain slope photo with a person on skis in the foreground."], {"person": [[0.793, 0.756, 0.877, 0.926]], "skis": [[0.77, 0.901, 0.891, 0.954]], "backpack": [[0.809, 0.762, 0.836, 0.81]]}], "response": "Task: What is the object in <Region>[0.809, 0.762, 0.836, 0.81]</Region> and how does it contribute to the skier's experience in the image?\nOutput: This object is a backpack that belongs to the skier in the foreground. It contributes to the skier's experience by providing a convenient means of carrying equipment and supplies necessary for the skiing trip. The backpack provides ample storage space for carrying water, food, additional clothing, and other essentials. This makes skiing more enjoyable and comfortable, while also ensuring the safety of the skier during the trip.", "instruction": "What is the object in <Region>[0.809, 0.762, 0.836, 0.81]</Region> and how does it contribute to the skier's experience in the image?", "input": "", "output": "This object is a backpack that belongs to the skier in the foreground. It contributes to the skier's experience by providing a convenient means of carrying equipment and supplies necessary for the skiing trip. The backpack provides ample storage space for carrying water, food, additional clothing, and other essentials. This makes skiing more enjoyable and comfortable, while also ensuring the safety of the skier during the trip.", "is_classification": false, "dataset": "COCO", "img_info": "000000000382.jpg"}
{"img_id": 6407, "query": [["a woman holding onto money with bananas on the groud ", "A woman is sitting with a bunch of bananas", "A woman sitting on the ground with bananas holding money.", "A sitting in front of some bananas holding up some bills.", "A person sitting in front of some bananas."], {"person": [[0.036, 0.014, 0.821, 0.76], [0.795, 0.102, 0.996, 0.738]], "banana": [[0.054, 0.721, 0.689, 1.0], [0.602, 0.892, 0.92, 1.0], [0.908, 0.64, 1.0, 0.809]]}], "response": "Task: Can you describe the action that the woman in <Region>[0.036, 0.014, 0.821, 0.76]</Region> is doing? What is the woman doing with the bananas and the money?\nOutput: The woman is holding a bunch of money and sitting in front of some bananas. There is no specific action that the woman is doing with the bananas and money, so it's not possible to describe an action.", "instruction": "Can you describe the action that the woman in <Region>[0.036, 0.014, 0.821, 0.76]</Region> is doing? What is the woman doing with the bananas and the money?", "input": "", "output": "The woman is holding a bunch of money and sitting in front of some bananas. There is no specific action that the woman is doing with the bananas and money, so it's not possible to describe an action.", "is_classification": false, "dataset": "COCO", "img_info": "000000006407.jpg"}
{"img_id": 39464, "query": [["A street sign on a pole by a road.", "A sign for a pizza slice express store, pointing the left.", "Different signs saying different things on the side of the road. ", "A couple of signs by the curb telling about a nearby pizza place.", "some signs buildings cars trees and a street"], {"car": [[0.844, 0.553, 0.907, 0.613], [0.884, 0.532, 1.0, 0.652], [0.815, 0.54, 0.865, 0.594], [0.731, 0.545, 0.765, 0.57], [0.706, 0.575, 0.889, 0.797], [0.755, 0.545, 0.808, 0.57], [0.717, 0.554, 0.737, 0.571], [0.556, 0.552, 0.567, 0.57], [0.772, 0.531, 0.796, 0.547], [0.557, 0.57, 0.567, 0.596]], "bicycle": [[0.488, 0.577, 0.53, 0.657], [0.425, 0.605, 0.457, 0.673]]}], "response": "Task: What is the message on the object in <Region>[0.0, 0.0, 1.0, 0.5]</Region> and why is it placed on the pole?\nOutput: The object in this region represents the top part of a street signpost. The sign is placed on the pole by the roadside to provide direction information for people. The message on the sign is not indicated in the given data.", "instruction": "What is the message on the object in <Region>[0.0, 0.0, 1.0, 0.5]</Region> and why is it placed on the pole?", "input": "", "output": "The object in this region represents the top part of a street signpost. The sign is placed on the pole by the roadside to provide direction information for people. The message on the sign is not indicated in the given data.", "is_classification": false, "dataset": "COCO", "img_info": "000000039464.jpg"}
{"img_id": 50470, "query": [["A beautiful woman swinging a tennis racquet at a ball.", "A woman is getting ready to hit a tennis ball.", "A tennis player preparing to hit a tennis ball.", "Professional tennis player about to hit the ball", "A professional tennis match with a woman whose racquet is back and a ball in mid-air."], {"sports ball": [[0.481, 0.577, 0.512, 0.614]], "chair": [[0.665, 0.207, 0.822, 0.442]], "person": [[0.431, 0.247, 0.734, 0.975], [0.674, 0.1, 0.82, 0.367]], "tennis racket": [[0.298, 0.615, 0.464, 0.737]]}], "response": "Task: Please describe the pose of the person located in <Region>[0.298, 0.615, 0.464, 0.737]</Region> and her position in the game based on the given information.\nOutput: The image shows a tennis match, and the person is a tennis player, preparing to hit the ball. She has her body turned sideways, her left hand is holding the tennis racket behind her, and her right hand is supported on her front hip. She is in the correct position to hit the ball and score a point.", "instruction": "Please describe the pose of the person located in <Region>[0.298, 0.615, 0.464, 0.737]</Region> and her position in the game based on the given information.", "input": "", "output": "The image shows a tennis match, and the person is a tennis player, preparing to hit the ball. She has her body turned sideways, her left hand is holding the tennis racket behind her, and her right hand is supported on her front hip. She is in the correct position to hit the ball and score a point.", "is_classification": false, "dataset": "COCO", "img_info": "000000050470.jpg"}
{"img_id": 47680, "query": [["A person standing in a room with some dishes.", "The mature man seems to be reading something.", "a man that i standing up by some pots and pans", "An older man wearing glasses standing in a kitchen.", "The man is in the kitchen working by himself."], {"person": [[0.31, 0.134, 0.831, 1.0]], "spoon": [[0.029, 0.811, 0.094, 0.987]], "bowl": [[0.181, 0.4, 0.309, 0.469], [0.171, 0.472, 0.306, 0.506], [0.019, 0.402, 0.15, 0.443]]}], "response": "Task: What is the person in <Region>[0.31, 0.134, 0.831, 1.0]</Region> doing in the image?\nOutput: The person in the bounding box is standing in the kitchen. It is not possible to specify what they are doing with this information alone.", "instruction": "What is the person in <Region>[0.31, 0.134, 0.831, 1.0]</Region> doing in the image?", "input": "", "output": "The person in the bounding box is standing in the kitchen. It is not possible to specify what they are doing with this information alone.", "is_classification": false, "dataset": "COCO", "img_info": "000000047680.jpg"}
{"img_id": 1554, "query": [["Three sheep in a pasture with people standing by the fence.", "Goats play in the grass while spectators watch. ", "Three sheared sheep on grass facing different directions.", "Three little goats attract the attention of a group of people at an outdoor display.", "A herd of sheep walking across a field of grass."], {"sheep": [[0.246, 0.485, 0.452, 0.964], [0.545, 0.39, 0.715, 0.95], [0.37, 0.58, 0.602, 0.978]], "person": [[0.659, 0.057, 0.765, 0.571], [0.807, 0.027, 0.898, 0.607], [0.622, 0.005, 0.703, 0.536], [0.767, 0.053, 0.834, 0.225], [0.865, 0.066, 0.951, 0.634], [0.932, 0.067, 1.0, 0.548]], "umbrella": [[0.81, 0.0, 0.943, 0.164]], "truck": [[0.28, 0.0, 0.786, 0.335]], "handbag": [[0.684, 0.266, 0.756, 0.376], [0.815, 0.285, 0.873, 0.366]]}], "response": "Task: What are the animals doing in the image, and what is the role of the object located in <Region>[0.81, 0.0, 0.943, 0.164]</Region> within the scene?\nOutput: The animals in the image are three sheep, and they are standing in a pasture. The object located in the specified region is an umbrella. It is possible that someone brought it with them to use as protection from the sun while watching the sheep.", "instruction": "What are the animals doing in the image, and what is the role of the object located in <Region>[0.81, 0.0, 0.943, 0.164]</Region> within the scene?", "input": "", "output": "The animals in the image are three sheep, and they are standing in a pasture. The object located in the specified region is an umbrella. It is possible that someone brought it with them to use as protection from the sun while watching the sheep.", "is_classification": false, "dataset": "COCO", "img_info": "000000001554.jpg"}
{"img_id": 27146, "query": [["A very long kitchen looks kind of messy.", "A very long cramped kitchen are full of stuff.", "a kitchen space with tiled walls and items on a counter.", "It's a white room with some cleaning supplies spread around.", "The view of a cluttered kitchen stainless steel counter."], {"bottle": [[0.533, 0.615, 0.555, 0.733], [0.562, 0.496, 0.574, 0.625]], "bowl": [[0.33, 0.789, 0.425, 0.868], [0.654, 0.654, 0.735, 0.769]], "microwave": [[0.479, 0.34, 0.512, 0.39], [0.513, 0.357, 0.536, 0.392]], "cup": [[0.588, 0.564, 0.658, 0.837], [0.409, 0.55, 0.451, 0.742]], "sink": [[0.653, 0.777, 0.766, 0.852]]}], "response": "Task: Please describe the function of the object at <Region>[0.533, 0.615, 0.555, 0.733]</Region> and how it might be used in the kitchen.\nOutput: The object is a bottle, and it can be used to store liquids such as oil, vinegar, soy sauce and other ingredients for cooking. It helps in adding flavor or enhancing the taste of dishes.", "instruction": "Please describe the function of the object at <Region>[0.533, 0.615, 0.555, 0.733]</Region> and how it might be used in the kitchen.", "input": "", "output": "The object is a bottle, and it can be used to store liquids such as oil, vinegar, soy sauce and other ingredients for cooking. It helps in adding flavor or enhancing the taste of dishes.", "is_classification": false, "dataset": "COCO", "img_info": "000000027146.jpg"}
{"img_id": 8734, "query": [["A table topped with plates of food and a book.", "THIS IS A PHOTO OF SANDWICHES THAT SOMEONE MADE FROM A MAGAZINE", "Two plates of bay scallops sit by a magazine.", "Bay scallop sandwiches were made from a magazine photo.", "The Bay scallops sub looks just like the picture in the book."], {"sandwich": [[0.074, 0.067, 0.376, 0.488], [0.584, 0.005, 0.838, 0.41], [0.422, 0.553, 0.742, 0.907]], "dining table": [[0.0, 0.002, 0.999, 0.991]], "book": [[0.359, 0.463, 0.817, 0.989]], "orange": [[0.818, 0.202, 0.93, 0.362]]}], "response": "Task: What is the role of the object in <Region>[0.359, 0.463, 0.817, 0.989]</Region> in creating the image?\nOutput: This object is a book, and it plays a role in the image by inspiring the person to make the sandwich recipe found in it.", "instruction": "What is the role of the object in <Region>[0.359, 0.463, 0.817, 0.989]</Region> in creating the image?", "input": "", "output": "This object is a book, and it plays a role in the image by inspiring the person to make the sandwich recipe found in it.", "is_classification": false, "dataset": "COCO", "img_info": "000000008734.jpg"}
{"img_id": 48001, "query": [["A view of a TV hotel room, with a file cabinet in it.", "A room that has a desk, tv, and mirror in it.", "The large mirror is on the wall above a leather chair. ", "A room is shown with a mirror, a chair, and a tv.", "A room with modern furnishings and wood furniture"], {"tv": [[0.068, 0.52, 0.337, 0.813]], "chair": [[0.537, 0.527, 0.766, 0.931], [0.944, 0.49, 1.0, 0.802]], "remote": [[0.357, 0.739, 0.424, 0.771]], "book": [[0.501, 0.748, 0.572, 0.798], [0.471, 0.745, 0.556, 0.825]], "vase": [[0.91, 0.51, 0.942, 0.604]], "potted plant": [[0.874, 0.34, 0.962, 0.6]], "dining table": [[0.678, 0.552, 1.0, 0.909]]}], "response": "Task: What is the relation of the object located in <Region>[0.537, 0.527, 0.766, 0.931]</Region> to the other furniture in the room?\nOutput: The object is a leather chair and it is placed in the room with a TV, a mirror, and wood furniture. The chair is adjacent to the coffee table on which there is a vase and a potted plant.", "instruction": "What is the relation of the object located in <Region>[0.537, 0.527, 0.766, 0.931]</Region> to the other furniture in the room?", "input": "", "output": "The object is a leather chair and it is placed in the room with a TV, a mirror, and wood furniture. The chair is adjacent to the coffee table on which there is a vase and a potted plant.", "is_classification": false, "dataset": "COCO", "img_info": "000000048001.jpg"}
{"img_id": 27907, "query": [["A man standing next to a young boy on a field.", "A boy getting ready to catch a pitch during a game.", "A catcher in a little league game waits for a pitch.", "A photo looking at young child playing a game of baseball.", "A young person crouches behind home plate with an adult behind them."], {"chair": [[0.0, 0.445, 0.089, 0.507], [0.499, 0.361, 0.669, 0.428]], "bench": [[0.468, 0.363, 0.669, 0.484]], "baseball glove": [[0.711, 0.545, 0.802, 0.598]], "person": [[0.679, 0.221, 0.762, 0.31], [0.267, 0.294, 0.602, 0.719], [0.201, 0.23, 0.271, 0.39], [0.564, 0.48, 0.803, 0.735], [0.641, 0.254, 0.824, 0.479], [0.559, 0.206, 0.663, 0.394], [0.796, 0.276, 0.988, 0.469], [0.47, 0.229, 0.558, 0.307], [0.942, 0.222, 1.0, 0.322], [0.271, 0.235, 0.339, 0.326], [0.139, 0.333, 0.289, 0.471], [0.839, 0.234, 0.924, 0.317], [0.0, 0.184, 0.994, 0.544]], "cup": [[0.277, 0.327, 0.303, 0.354]]}], "response": "Task: What type of piece of equipment is the object located in <Region>[0.711, 0.545, 0.802, 0.598]</Region> in the image and what is its purpose?\nOutput: The object is a baseball glove, and its purpose is to catch the ball thrown by the pitcher. It is an important piece of equipment in baseball and is used by the catcher positioned behind the home plate.", "instruction": "What type of piece of equipment is the object located in <Region>[0.711, 0.545, 0.802, 0.598]</Region> in the image and what is its purpose?", "input": "", "output": "The object is a baseball glove, and its purpose is to catch the ball thrown by the pitcher. It is an important piece of equipment in baseball and is used by the catcher positioned behind the home plate.", "is_classification": false, "dataset": "COCO", "img_info": "000000027907.jpg"}
{"img_id": 37595, "query": [["A man with a beard in a black t-shirt is eating with his hand.", "A person that is wearing a hat and brushing their teeth. ", "a man is taking a bbite out of some food", "A bearded young man with his finger in his mouth.", "A man who is brushing his teeth with a toothbrush."], {"person": [[0.0, 0.094, 0.969, 0.988]], "toothbrush": [[0.322, 0.34, 0.47, 0.367]]}], "response": "Task: Please describe the scene in the image and explain the action being performed with the object located in <Region>[0.322, 0.34, 0.47, 0.367]</Region>.\nOutput: The image shows a close-up of a person's mouth and the object is a toothbrush; thus, the person in the image is likely brushing their teeth with the toothbrush.", "instruction": "Please describe the scene in the image and explain the action being performed with the object located in <Region>[0.322, 0.34, 0.47, 0.367]</Region>.", "input": "", "output": "The image shows a close-up of a person's mouth and the object is a toothbrush; thus, the person in the image is likely brushing their teeth with the toothbrush.", "is_classification": false, "dataset": "COCO", "img_info": "000000037595.jpg"}
{"img_id": 36278, "query": [["A kitchen that has been kept very clean.", "A kitchen, dining area and a living room. ", "A kitchen with a wall mounted oven sitting next to a chairs.", "The view shows the kitchen's double oven, sunny dining area and living area with stone fireplace.", "this house has many wood cabinets and a wood table"], {"chair": [[0.682, 0.585, 0.744, 0.684], [0.845, 0.596, 0.973, 0.92], [0.669, 0.613, 0.847, 1.0], [0.627, 0.597, 0.649, 0.709], [0.863, 0.642, 1.0, 0.868]], "dining table": [[0.699, 0.666, 1.0, 0.989]], "oven": [[0.146, 0.611, 0.275, 0.846], [0.145, 0.499, 0.26, 0.611]], "refrigerator": [[0.003, 0.055, 0.044, 1.0]], "vase": [[0.833, 0.643, 0.862, 0.711]], "couch": [[0.627, 0.596, 0.649, 0.707]]}], "response": "Task: What objects are present in <Region>[0.699, 0.666, 1.0, 0.989]</Region>, and what is the function of these objects in this kitchen?\nOutput: The object in this region is a dining table where people eat their meals. Around it, there are multiple chairs, possibly for people to sit while they eat or to use them as extra furniture. Dining tables and chairs are often considered essential in a kitchen because they provide a comfortable and functional space for family and friends to share meals and conversation.", "instruction": "What objects are present in <Region>[0.699, 0.666, 1.0, 0.989]</Region>, and what is the function of these objects in this kitchen?", "input": "", "output": "The object in this region is a dining table where people eat their meals. Around it, there are multiple chairs, possibly for people to sit while they eat or to use them as extra furniture. Dining tables and chairs are often considered essential in a kitchen because they provide a comfortable and functional space for family and friends to share meals and conversation.", "is_classification": false, "dataset": "COCO", "img_info": "000000036278.jpg"}
{"img_id": 6075, "query": [["The current time is shown at a train station.", "A clock and sign hang over train tracks at a train station.", "Several people at a train station across from a clock.", "A train platform with a large clock hanging above it", "A clock located within a train station with people."], {"train": [[0.01, 0.569, 0.928, 0.713]], "clock": [[0.139, 0.097, 0.358, 0.334]], "person": [[0.641, 0.641, 0.665, 0.742], [0.786, 0.644, 0.819, 0.751], [0.497, 0.633, 0.52, 0.728], [0.471, 0.641, 0.5, 0.729], [0.462, 0.637, 0.48, 0.718], [0.385, 0.631, 0.411, 0.718], [0.695, 0.636, 0.74, 0.77], [0.324, 0.641, 0.345, 0.699], [0.673, 0.625, 0.707, 0.754], [0.344, 0.634, 0.365, 0.707], [0.807, 0.646, 0.859, 0.777], [0.862, 0.623, 0.904, 0.698], [0.293, 0.638, 0.31, 0.694], [0.031, 0.627, 0.62, 0.75]], "handbag": [[0.794, 0.668, 0.814, 0.71], [0.651, 0.678, 0.662, 0.716], [0.413, 0.668, 0.431, 0.693]]}], "response": "Task: What is the specific location of the object <Region>[0.862, 0.623, 0.904, 0.698]</Region> in this scene and what is the person holding it doing?\nOutput: The object is a handbag and it is being held by a person, located near the clock. The person is waiting at the train station while holding the handbag, possibly waiting for their train to arrive.", "instruction": "What is the specific location of the object <Region>[0.862, 0.623, 0.904, 0.698]</Region> in this scene and what is the person holding it doing?", "input": "", "output": "The object is a handbag and it is being held by a person, located near the clock. The person is waiting at the train station while holding the handbag, possibly waiting for their train to arrive.", "is_classification": false, "dataset": "COCO", "img_info": "000000006075.jpg"}
{"img_id": 8821, "query": [["A large open grass field with some tents.", "Several tents set up in the grass and a banner in front of a couple of them. ", "A dog catches a Frisbee in mid air.", "a dog jumping in the air with a frisbee in its mouth", "A number of small tents set up in an open field."], {"dog": [[0.384, 0.249, 0.673, 0.521]], "frisbee": [[0.6, 0.216, 0.666, 0.33], [0.126, 0.828, 0.185, 0.839], [0.004, 0.479, 0.046, 0.597]], "chair": [[0.712, 0.522, 0.796, 0.651], [0.79, 0.52, 0.864, 0.606]], "person": [[0.0, 0.456, 0.01, 0.496]]}], "response": "Task: What is happening in the image and what is the role of the object in <Region>[0.384, 0.249, 0.673, 0.521]</Region> in that scene?\nOutput: The image depicts a dog catching a frisbee in mid-air in an open field. The object in the mentioned region is the dog itself. It is the main focus of the image and is performing an athletic feat - jumping high in the air to catch the frisbee.", "instruction": "What is happening in the image and what is the role of the object in <Region>[0.384, 0.249, 0.673, 0.521]</Region> in that scene?", "input": "", "output": "The image depicts a dog catching a frisbee in mid-air in an open field. The object in the mentioned region is the dog itself. It is the main focus of the image and is performing an athletic feat - jumping high in the air to catch the frisbee.", "is_classification": false, "dataset": "COCO", "img_info": "000000008821.jpg"}
{"img_id": 36639, "query": [["A van and some cars parked in the street by a stop sign.", "A blue and white bus at a stop sign.", "An old van is stopped at the stop sign of an intersection.", "A Ban stopped at a stop sign at an intersection.", "A van is stopped in front of a stop sign."], {"car": [[0.669, 0.419, 0.808, 0.517], [0.814, 0.413, 0.999, 0.544], [0.0, 0.416, 0.057, 0.465], [0.12, 0.424, 0.17, 0.478], [0.003, 0.729, 0.054, 0.94]], "bus": [[0.003, 0.306, 0.7, 0.955]], "stop sign": [[0.792, 0.124, 0.915, 0.331]], "truck": [[0.025, 0.308, 0.693, 0.927]]}], "response": "Task: Please describe the position of the object in <Region>[0.792, 0.124, 0.915, 0.331]</Region> with respect to the other objects in the image.\nOutput: The object in the given region is a stop sign. It is positioned at the roadside indicating that it is an intersection. The van and cars are parked on the street, being obedient to the traffic rules. Amongst them, a blue and white bus stops beside the stop sign.", "instruction": "Please describe the position of the object in <Region>[0.792, 0.124, 0.915, 0.331]</Region> with respect to the other objects in the image.", "input": "", "output": "The object in the given region is a stop sign. It is positioned at the roadside indicating that it is an intersection. The van and cars are parked on the street, being obedient to the traffic rules. Amongst them, a blue and white bus stops beside the stop sign.", "is_classification": false, "dataset": "COCO", "img_info": "000000036639.jpg"}
{"img_id": 56205, "query": [["A person sits next to their bike while birds walk nearby.", "A man sits on a curb with his bicycle", "A black and whit photo of a woman sitting hear her bike.", "a person sitting next to a bicycle while watching pigeons on a sidewalk.", "a person sitting on a ledge with a bike near by"], {"bird": [[0.284, 0.68, 0.383, 0.783], [0.891, 0.565, 0.96, 0.641], [0.659, 0.624, 0.675, 0.657], [0.04, 0.815, 0.063, 0.851], [0.372, 0.483, 0.39, 0.523], [0.981, 0.818, 1.0, 0.857]], "bicycle": [[0.483, 0.262, 0.62, 0.561]], "person": [[0.348, 0.252, 0.496, 0.58]]}], "response": "Task: What are the birds in the image doing near the object in <Region>[0.483, 0.262, 0.62, 0.561]</Region>?\nOutput: The object in the given location is a bicycle, and the birds are walking near it. The birds are not doing anything specific near the bicycle, they are just walking nearby.", "instruction": "What are the birds in the image doing near the object in <Region>[0.483, 0.262, 0.62, 0.561]</Region>?", "input": "", "output": "The object in the given location is a bicycle, and the birds are walking near it. The birds are not doing anything specific near the bicycle, they are just walking nearby.", "is_classification": false, "dataset": "COCO", "img_info": "000000056205.jpg"}
{"img_id": 21384, "query": [["A clock that is located in front of trees and buildings.", "A clock standing in front of a tall building. ", "Large blue building, a row of trees and a clock. ", "A street scene looking at a clock and a building.", "clock on a pole in front of leafy trees and windowed building"], {"clock": [[0.083, 0.51, 0.249, 0.712]]}], "response": "Task: What is the location of the object in <Region>[0.083, 0.51, 0.249, 0.712]</Region> to the nearest building and trees?\nOutput: The clock is located in front of the buildings and trees. It looks like it is placed on a pole, situated along the street corner.", "instruction": "What is the location of the object in <Region>[0.083, 0.51, 0.249, 0.712]</Region> to the nearest building and trees?", "input": "", "output": "The clock is located in front of the buildings and trees. It looks like it is placed on a pole, situated along the street corner.", "is_classification": false, "dataset": "COCO", "img_info": "000000021384.jpg"}
{"img_id": 61372, "query": [["A young boy is trying to light his birthday candles. ", "A child is lighting candles on a cake.", "A young boy lighting candles on a very large green cake.", "A child lighting a birthday candle on a fancy cake.", "A boy lighting candles on a birthday cake."], {"person": [[0.132, 0.139, 0.664, 0.981], [0.0, 0.084, 0.107, 0.729], [0.0, 0.579, 0.174, 1.0]], "cake": [[0.529, 0.653, 0.808, 0.874], [0.709, 0.596, 0.964, 0.791]], "book": [[0.691, 0.001, 0.735, 0.083], [0.67, 0.0, 0.693, 0.089], [0.735, 0.0, 0.759, 0.084], [0.758, 0.0, 0.778, 0.077], [0.622, 0.0, 0.655, 0.091]], "cup": [[0.48, 0.531, 0.545, 0.601], [0.561, 0.506, 0.61, 0.615]]}], "response": "Task: Assuming the image is taken during a birthday celebration, what can be the most probable material the object placed in <Region>[0.561, 0.506, 0.61, 0.615]</Region> is made of?\nOutput: As shown in the image, the object is a cup placed on the table during a birthday celebration. From the appearance of the cup, it appears to be made of glass, as it has a shiny and translucent appearance.", "instruction": "Assuming the image is taken during a birthday celebration, what can be the most probable material the object placed in <Region>[0.561, 0.506, 0.61, 0.615]</Region> is made of?", "input": "", "output": "As shown in the image, the object is a cup placed on the table during a birthday celebration. From the appearance of the cup, it appears to be made of glass, as it has a shiny and translucent appearance.", "is_classification": false, "dataset": "COCO", "img_info": "000000061372.jpg"}
{"img_id": 5832, "query": [["A laptop sitting next to two computer screens.", "Three computers screens are lit up and sitting on a desk.", "Three computers and a telephone on a desk.", "one laptop and two extra monitors sitting on a desk ", "a desk top with some computers on top of it "], {"tv": [[0.383, 0.397, 0.578, 0.633], [0.613, 0.398, 0.79, 0.648]], "laptop": [[0.173, 0.424, 0.404, 0.682]], "mouse": [[0.923, 0.682, 0.956, 0.721], [0.626, 0.666, 0.668, 0.701]], "keyboard": [[0.397, 0.648, 0.616, 0.722], [0.691, 0.615, 0.902, 0.677], [0.219, 0.568, 0.358, 0.634]], "cup": [[0.126, 0.764, 0.172, 0.857], [0.218, 0.67, 0.27, 0.746]], "chair": [[0.122, 0.951, 0.281, 1.0]]}], "response": "Task: What devices are represented by the objects in <Region>[0.383, 0.397, 0.578, 0.633]</Region> and <Region>[0.613, 0.398, 0.79, 0.648]</Region>?\nOutput: These objects are two computer screens sitting on a desk, as seen in the image.", "instruction": "What devices are represented by the objects in <Region>[0.383, 0.397, 0.578, 0.633]</Region> and <Region>[0.613, 0.398, 0.79, 0.648]</Region>?", "input": "", "output": "These objects are two computer screens sitting on a desk, as seen in the image.", "is_classification": false, "dataset": "COCO", "img_info": "000000005832.jpg"}
{"img_id": 61684, "query": [["A man holding out two hands as if pulling on something", "A man laughs as he points a remote control at something outdoors.", "A man who is standing in an area with grass and trees.", "A man outside in a yard with a remote.", "A man standing on a lush green field holding a frisbee."], {"person": [[0.508, 0.289, 0.88, 0.996]], "remote": [[0.86, 0.514, 0.89, 0.543]]}], "response": "Task: What is the man in <Region>[0.86, 0.514, 0.89, 0.543]</Region> using it for?\nOutput: The object in the image is a remote, and the man is pointing it at something outdoors laughing. From this, we can infer that he could be controlling a toy such as a drone or any other remote-controlled gadget.", "instruction": "What is the man in <Region>[0.86, 0.514, 0.89, 0.543]</Region> using it for?", "input": "", "output": "The object in the image is a remote, and the man is pointing it at something outdoors laughing. From this, we can infer that he could be controlling a toy such as a drone or any other remote-controlled gadget.", "is_classification": false, "dataset": "COCO", "img_info": "000000061684.jpg"}
{"img_id": 11591, "query": [["a female sitting on the floor with a laptop and some wires", "a lady sitting on a carpet on her laptop ", "A girl relaxing, doing some work or reading on a laptop.", "Woman using laptop computer sitting on carpeting in large area.", "a girl using a laptop while sitting on a carpeted floor"], {"cell phone": [[0.023, 0.568, 0.044, 0.598], [0.55, 0.879, 0.711, 0.909]], "person": [[0.148, 0.403, 1.0, 0.808]], "laptop": [[0.461, 0.577, 0.733, 0.711]], "chair": [[0.618, 0.447, 0.889, 0.655]]}], "response": "Task: What activity is the woman in <Region>[0.461, 0.577, 0.733, 0.711]</Region> doing on her laptop?\nOutput: The woman in the image is using her laptop, which is placed on the carpeted floor in front of her. It is not clear from the image what activity she is doing on her laptop.", "instruction": "What activity is the woman in <Region>[0.461, 0.577, 0.733, 0.711]</Region> doing on her laptop?", "input": "", "output": "The woman in the image is using her laptop, which is placed on the carpeted floor in front of her. It is not clear from the image what activity she is doing on her laptop.", "is_classification": false, "dataset": "COCO", "img_info": "000000011591.jpg"}
{"img_id": 38277, "query": [["A person skiing in the snow on a hill", "Someone is skiing in the cold white snow.", "A person is walking over a mountain slope.", "A person in skis hiking up a snowy hill.", "A person walking up a snow covered hill wearing skis."], {"person": [[0.487, 0.519, 0.588, 0.712]], "skis": [[0.514, 0.694, 0.649, 0.723]], "backpack": [[0.528, 0.522, 0.577, 0.579]]}], "response": "Task: What is the purpose of the object in <Region>[0.528, 0.522, 0.577, 0.579]</Region> in the scene?\nOutput: The object is a backpack, which is used to carry skiing equipment such as gloves, goggles, or hats. The backpack allows for more accessibility and mobility during the skiing activity.", "instruction": "What is the purpose of the object in <Region>[0.528, 0.522, 0.577, 0.579]</Region> in the scene?", "input": "", "output": "The object is a backpack, which is used to carry skiing equipment such as gloves, goggles, or hats. The backpack allows for more accessibility and mobility during the skiing activity.", "is_classification": false, "dataset": "COCO", "img_info": "000000038277.jpg"}
{"img_id": 56504, "query": [["A group of people that are holding onto sheep.", "Three people stand with two large adult sheep.", "A man tending to a sheep next to another sheep.", "Men display sheep inside an enclosure as people watch on.", "People at a sheep judging contest looking over the sheep "], {"sheep": [[0.545, 0.413, 0.766, 0.97], [0.326, 0.249, 0.481, 0.751]], "tie": [[0.691, 0.2, 0.737, 0.358], [0.914, 0.014, 0.926, 0.046], [0.708, 0.25, 0.733, 0.322]], "person": [[0.601, 0.006, 0.956, 0.99], [0.137, 0.116, 0.454, 0.815], [0.292, 0.0, 0.488, 0.671], [0.873, 0.0, 0.976, 0.29], [0.825, 0.0, 0.877, 0.107], [0.742, 0.002, 0.829, 0.093], [0.107, 0.0, 0.157, 0.098], [0.129, 0.0, 0.213, 0.228], [0.0, 0.0, 0.128, 0.273], [0.182, 0.001, 0.267, 0.083], [0.003, 0.263, 0.15, 0.532], [0.0, 0.002, 0.048, 0.15]]}], "response": "Task: What is the purpose of the tie located in <Region>[0.691, 0.2, 0.737, 0.358]</Region> in relation to the sheep and the people holding them?\nOutput: The tie is most likely a leash or a rope and is used to keep the sheep under control while being tended to by the people in the image.", "instruction": "What is the purpose of the tie located in <Region>[0.691, 0.2, 0.737, 0.358]</Region> in relation to the sheep and the people holding them?", "input": "", "output": "The tie is most likely a leash or a rope and is used to keep the sheep under control while being tended to by the people in the image.", "is_classification": false, "dataset": "COCO", "img_info": "000000056504.jpg"}
{"img_id": 50791, "query": [["A woman frying donuts in a big pot", "A bowl of donuts and a woman stirring a pot.", "A view of a bunch of doughnuts being made.", "a lady in a kitchen next to a bunch of doughnuts", "A basket filled with donuts covered in powdered sugar."], {"person": [[0.005, 0.003, 0.85, 0.988]], "bowl": [[0.203, 0.447, 0.992, 0.997]], "donut": [[0.348, 0.523, 0.546, 0.691], [0.501, 0.59, 0.739, 0.849], [0.491, 0.438, 0.636, 0.576], [0.811, 0.576, 0.998, 0.821], [0.307, 0.685, 0.506, 0.822], [0.7, 0.449, 0.908, 0.608], [0.762, 0.742, 0.908, 0.841], [0.726, 0.595, 0.831, 0.782], [0.697, 0.791, 0.858, 0.879], [0.274, 0.629, 0.385, 0.766], [0.595, 0.5, 0.769, 0.67]], "spoon": [[0.798, 0.152, 0.836, 0.269]]}], "response": "Task: What is the woman in the image doing with the spoon in <Region>[0.798, 0.152, 0.836, 0.269]</Region>?\nOutput: The woman in the image is most likely stirring the doughnut mix in the pot with the spoon located in the mentioned region.", "instruction": "What is the woman in the image doing with the spoon in <Region>[0.798, 0.152, 0.836, 0.269]</Region>?", "input": "", "output": "The woman in the image is most likely stirring the doughnut mix in the pot with the spoon located in the mentioned region.", "is_classification": false, "dataset": "COCO", "img_info": "000000050791.jpg"}
{"img_id": 16383, "query": [["Kitchen with natural wood floor has a small butcher block island.", "A kitchen with a hard wood floor and a refrigerator freezer.", "A small cluttered kitchen inside a cramped house", "A clean and well organized kitchen with two trash cans", "A kitchen that has a small island in the middle of it."], {"potted plant": [[0.495, 0.412, 0.552, 0.541]], "bottle": [[0.596, 0.101, 0.608, 0.162], [0.585, 0.108, 0.594, 0.159], [0.609, 0.102, 0.621, 0.149], [0.566, 0.109, 0.58, 0.166], [0.637, 0.09, 0.654, 0.139], [0.498, 0.147, 0.509, 0.197], [0.494, 0.377, 0.501, 0.433], [0.523, 0.152, 0.536, 0.186], [0.622, 0.074, 0.638, 0.14], [0.525, 0.378, 0.529, 0.402], [0.5, 0.382, 0.509, 0.412], [0.468, 0.157, 0.481, 0.205], [0.573, 0.111, 0.592, 0.175]], "refrigerator": [[0.572, 0.217, 0.763, 0.738]], "microwave": [[0.351, 0.389, 0.423, 0.443]], "toaster": [[0.014, 0.428, 0.1, 0.507]], "wine glass": [[0.54, 0.16, 0.548, 0.18], [0.436, 0.167, 0.453, 0.204], [0.017, 0.032, 0.042, 0.082], [0.42, 0.161, 0.438, 0.199], [0.545, 0.132, 0.558, 0.178], [0.517, 0.146, 0.524, 0.189], [0.52, 0.151, 0.534, 0.186], [0.129, 0.04, 0.149, 0.113], [0.551, 0.12, 0.566, 0.169], [0.488, 0.179, 0.497, 0.2], [0.554, 0.148, 0.559, 0.17]], "dining table": [[0.393, 0.476, 0.61, 0.908]], "sink": [[0.277, 0.434, 0.378, 0.465]], "oven": [[0.549, 0.467, 0.572, 0.523]], "cup": [[0.451, 0.171, 0.463, 0.202]]}], "response": "Task: What is the object located at <Region>[0.549, 0.467, 0.572, 0.523]</Region> in the kitchen used for, and what is its significance?\nOutput: The object is an oven, and it is crucial in the kitchen as it provides the means to cook and bake various foods. It is a powerful appliance that uses heat to cook or bake dishes to perfection. It is an indispensable part of a modern kitchen and enables the preparation of a variety of dishes that would be impossible to make otherwise.", "instruction": "What is the object located at <Region>[0.549, 0.467, 0.572, 0.523]</Region> in the kitchen used for, and what is its significance?", "input": "", "output": "The object is an oven, and it is crucial in the kitchen as it provides the means to cook and bake various foods. It is a powerful appliance that uses heat to cook or bake dishes to perfection. It is an indispensable part of a modern kitchen and enables the preparation of a variety of dishes that would be impossible to make otherwise.", "is_classification": false, "dataset": "COCO", "img_info": "000000016383.jpg"}
{"img_id": 52751, "query": [["baseball teammates giving each other the high five", "Two professional baseball players giving high fives at a professional baseball game ", "Two base ball players with their hands up during a game.", "Two players from the Cleveland Indians giving each other a high five", "Baseball players preparing to celebrate with each other. "], {"person": [[0.098, 0.116, 0.405, 0.973], [0.551, 0.073, 0.806, 0.94], [0.741, 0.002, 0.9, 0.806]], "baseball glove": [[0.775, 0.444, 0.856, 0.529]]}], "response": "Task: Can you identify the object on the <Region>[0.775, 0.444, 0.856, 0.529]</Region> of the image and explain its significance in the scene?\nOutput: This image shows two baseball players giving each other a high five after a game, and the object is a baseball glove. A baseball glove is an essential piece of equipment that every player has as it protects the player's hand when catching the ball.", "instruction": "Can you identify the object on the <Region>[0.775, 0.444, 0.856, 0.529]</Region> of the image and explain its significance in the scene?", "input": "", "output": "This image shows two baseball players giving each other a high five after a game, and the object is a baseball glove. A baseball glove is an essential piece of equipment that every player has as it protects the player's hand when catching the ball.", "is_classification": false, "dataset": "COCO", "img_info": "000000052751.jpg"}
{"img_id": 64054, "query": [["The sink is above a turquoise tile sink.", "A view of a sink with a  very different kind of appeal to it.", "a bathroom with a sink an mirror inside of it ", "A sink, shelves, and cabinets are uniquely decorated.", "A sink that has some decorations in the basin of it."], {"cup": [[0.249, 0.71, 0.319, 0.784], [0.131, 0.445, 0.211, 0.509]], "sink": [[0.338, 0.779, 0.68, 0.931]]}], "response": "Task: What can you infer about the purpose of the object in <Region>[0.131, 0.445, 0.211, 0.509]</Region> within the scene?\nOutput: The object is a cup placed inside a bathroom sink. Based on the image, it can be inferred that the cup is being used to hold water for various purposes such as washing hands or brushing teeth.", "instruction": "What can you infer about the purpose of the object in <Region>[0.131, 0.445, 0.211, 0.509]</Region> within the scene?", "input": "", "output": "The object is a cup placed inside a bathroom sink. Based on the image, it can be inferred that the cup is being used to hold water for various purposes such as washing hands or brushing teeth.", "is_classification": false, "dataset": "COCO", "img_info": "000000064054.jpg"}
{"img_id": 58307, "query": [["a banana with a cut made in to it", "Someone holding a banana in front of a tray with an empty plate on it. ", "A hand holding a banana that has been broken open at the base.", "A persons hand holding a banana that has a smile in the tip and a food tray with an empty plate on the table in front.", "A banana that has been pealed a little at the top"], {"person": [[0.187, 0.587, 0.578, 1.0]], "knife": [[0.81, 0.417, 0.997, 0.519]], "banana": [[0.344, 0.303, 0.699, 0.911]], "fork": [[0.592, 0.235, 0.998, 0.308]]}], "response": "Task: What is the object in <Region>[0.344, 0.303, 0.699, 0.911]</Region>, and what process has been performed on it as shown in the image?\nOutput: The object is a banana, and the process performed on it is cutting it open at the base.", "instruction": "What is the object in <Region>[0.344, 0.303, 0.699, 0.911]</Region>, and what process has been performed on it as shown in the image?", "input": "", "output": "The object is a banana, and the process performed on it is cutting it open at the base.", "is_classification": false, "dataset": "COCO", "img_info": "000000058307.jpg"}
{"img_id": 28981, "query": [["A series of two pictures with plate holding two different types of food.", "There is some meat and rice ready to be eaten", "A bowl of noodles and a plate of meat.", "Food entrees are served in the appropriate dishes.", "A double image featuring a bowl of ramen and a meat dish."], {"remote": [[0.086, 0.498, 0.43, 0.566]], "bowl": [[0.054, 0.499, 0.957, 0.987]], "dining table": [[0.009, 0.496, 0.993, 0.986], [0.01, 0.01, 1.0, 0.486]]}], "response": "Task: What are the objects in the <Region>[0.054, 0.499, 0.957, 0.987]</Region> and <Region>[0.086, 0.498, 0.43, 0.566]</Region> regions of the image, and how are they related to each other?\nOutput: In the image, there are two objects present, a bowl of noodles, and a remote. These objects are not related to each other as they serve different purposes. The remote may have been placed on the table beside the bowl of noodles.", "instruction": "What are the objects in the <Region>[0.054, 0.499, 0.957, 0.987]</Region> and <Region>[0.086, 0.498, 0.43, 0.566]</Region> regions of the image, and how are they related to each other?", "input": "", "output": "In the image, there are two objects present, a bowl of noodles, and a remote. These objects are not related to each other as they serve different purposes. The remote may have been placed on the table beside the bowl of noodles.", "is_classification": false, "dataset": "COCO", "img_info": "000000028981.jpg"}
{"img_id": 14557, "query": [["A building displaying a clock showing the time to be 6 oclock.", "A large clock on the side of a beige building.", "The clock face on the exterior of a building.", "A stone clock on a stone building with red hour markers.", "A close up of the architectural structures of a building with a clock on it. "], {"clock": [[0.141, 0.673, 0.363, 0.856]]}], "response": "Task: What is the significance of the time being displayed as 6 o'clock on the clock located in <Region>[0.141, 0.673, 0.363, 0.856]</Region>.\nOutput: The time being displayed on the clock is merely an observation. It has no significance as the image contains no additional context to provide any reasoning behind having the clock hands indicating the time as 6 o'clock.", "instruction": "What is the significance of the time being displayed as 6 o'clock on the clock located in <Region>[0.141, 0.673, 0.363, 0.856]</Region>.", "input": "", "output": "The time being displayed on the clock is merely an observation. It has no significance as the image contains no additional context to provide any reasoning behind having the clock hands indicating the time as 6 o'clock.", "is_classification": false, "dataset": "COCO", "img_info": "000000014557.jpg"}
{"img_id": 45959, "query": [["A bicycle parking sign on a pole at the corner of a street.", "A black lamppost with a white traffic sign on side of street.", "A bicycle sign is attached to a telephone pole.", "A pole near the curb with a bicycle sign on it. ", "a post with signs is near a fire hydrant"], {"car": [[0.146, 0.329, 0.241, 0.381], [0.076, 0.341, 0.147, 0.383], [0.259, 0.345, 0.313, 0.372], [0.228, 0.341, 0.265, 0.372], [0.956, 0.367, 1.0, 0.421], [0.319, 0.336, 0.345, 0.357]], "traffic light": [[0.233, 0.158, 0.26, 0.194], [0.09, 0.151, 0.108, 0.184]], "fire hydrant": [[0.272, 0.589, 0.408, 0.844]], "bicycle": [[0.375, 0.532, 0.624, 0.676]]}], "response": "Task: What is the object in <Region>[0.375, 0.532, 0.624, 0.676]</Region> and what does it signify?\nOutput: The object in this region is a bicycle sign post, which serves as a guide for bikers. The sign signifies that the area is a designated bicycle parking area or a bicycle route where bikers can ride safely.", "instruction": "What is the object in <Region>[0.375, 0.532, 0.624, 0.676]</Region> and what does it signify?", "input": "", "output": "The object in this region is a bicycle sign post, which serves as a guide for bikers. The sign signifies that the area is a designated bicycle parking area or a bicycle route where bikers can ride safely.", "is_classification": false, "dataset": "COCO", "img_info": "000000045959.jpg"}
{"img_id": 28415, "query": [["A teenager riding a skateboard on a ramp in a front yard.", "A man of a skateboard is skateing up a ramp", "A man riding a skateboard up the side of a ramp.", "The reflection of a young skateboarder in a puddle of water.", "A boy skateboards in front of drying clothes and two houses. "], {"car": [[0.481, 0.544, 0.536, 0.599]], "chair": [[0.349, 0.52, 0.38, 0.602]], "dining table": [[0.294, 0.525, 0.352, 0.547]], "person": [[0.386, 0.183, 0.754, 0.815]], "skateboard": [[0.608, 0.644, 0.761, 0.843]]}], "response": "Task: Can you describe the person on the skateboard, and their activity as shown in the image with respect to the object located in <Region>[0.608, 0.644, 0.761, 0.843]</Region>.\nOutput: The image shows a person riding a skateboard, and the object is the skateboard itself. The person seems to be performing a trick with the skateboard, as it is lifted up and tilted at an angle, ready to glide over the ramp.", "instruction": "Can you describe the person on the skateboard, and their activity as shown in the image with respect to the object located in <Region>[0.608, 0.644, 0.761, 0.843]</Region>.", "input": "", "output": "The image shows a person riding a skateboard, and the object is the skateboard itself. The person seems to be performing a trick with the skateboard, as it is lifted up and tilted at an angle, ready to glide over the ramp.", "is_classification": false, "dataset": "COCO", "img_info": "000000028415.jpg"}
{"img_id": 2184, "query": [["small children learning tennis on a modifgied court", "A man teaches children to play on a tennis court.", "Several children with tennis racquets and one adult standing on a tennis court.", "A number of children with rackets on a tennis court.", "A young boy standing on a tennis court during a game of tennis."], {"tennis racket": [[0.169, 0.514, 0.217, 0.594], [0.054, 0.315, 0.825, 0.631], [0.913, 0.49, 0.933, 0.597], [0.537, 0.814, 0.666, 0.867], [0.825, 0.801, 0.957, 0.841], [0.958, 0.436, 0.98, 0.472]], "person": [[0.502, 0.182, 0.649, 0.906], [0.405, 0.36, 0.476, 0.652], [0.901, 0.366, 0.966, 0.521], [0.158, 0.34, 0.208, 0.623], [0.052, 0.362, 0.106, 0.628], [0.581, 0.461, 0.708, 0.948], [0.328, 0.388, 0.415, 0.703], [0.211, 0.334, 0.291, 0.638], [0.792, 0.294, 0.854, 0.584], [0.939, 0.344, 0.988, 0.454]], "sports ball": [[0.888, 0.582, 0.904, 0.607], [0.281, 0.555, 0.292, 0.571], [0.471, 0.634, 0.482, 0.651], [0.514, 0.767, 0.532, 0.791], [0.242, 0.55, 0.249, 0.565], [0.013, 0.567, 0.025, 0.582], [0.935, 0.605, 0.956, 0.641], [0.399, 0.548, 0.409, 0.565], [0.025, 0.566, 0.036, 0.581], [0.95, 0.585, 0.972, 0.612], [0.908, 0.616, 0.927, 0.644], [0.005, 0.539, 0.973, 0.792]]}], "response": "Task: What is the significance of the objects in <Region>[0.169, 0.514, 0.217, 0.594]</Region> and <Region>[0.825, 0.801, 0.957, 0.841]</Region> in the image? \nOutput: The objects in these regions are tennis rackets, and they appear to be used by the children for playing tennis. The tennis rackets help the children to hit the sports balls and have fun while learning tennis skills.", "instruction": "What is the significance of the objects in <Region>[0.169, 0.514, 0.217, 0.594]</Region> and <Region>[0.825, 0.801, 0.957, 0.841]</Region> in the image?", "input": "", "output": "The objects in these regions are tennis rackets, and they appear to be used by the children for playing tennis. The tennis rackets help the children to hit the sports balls and have fun while learning tennis skills.", "is_classification": false, "dataset": "COCO", "img_info": "000000002184.jpg"}
{"img_id": 5816, "query": [["A  bed with pillows and white sheets in a room ", "Bed made up for two, with bed clothes laid out.", "A stack of pillows on a bed at a hotel.", "Two sets of pajamas are laid on a bed.", "A bedroom has a lamp by each side of the bed."], {"bed": [[0.03, 0.319, 0.974, 0.989]], "remote": [[0.0, 0.752, 0.098, 0.815]], "bottle": [[0.026, 0.62, 0.066, 0.749]]}], "response": "Task: Please provide information on the objects located in <Region>[0.0, 0.752, 0.098, 0.815]</Region> and <Region>[0.026, 0.62, 0.066, 0.749]</Region> related to the bed in the image.\nOutput: The objects are a remote and a bottle. The remote is most likely for a TV, while the bottle could contain water for persons sleeping on the bed.", "instruction": "Please provide information on the objects located in <Region>[0.0, 0.752, 0.098, 0.815]</Region> and <Region>[0.026, 0.62, 0.066, 0.749]</Region> related to the bed in the image.", "input": "", "output": "The objects are a remote and a bottle. The remote is most likely for a TV, while the bottle could contain water for persons sleeping on the bed.", "is_classification": false, "dataset": "COCO", "img_info": "000000005816.jpg"}
{"img_id": 42161, "query": [["A narrow snow covered street leading down to the water", "lots of snow on the ground and the ocean is ahead.", "A snow-covered street next to houses leads to the water.", "A snowy street with snow route signage displayed.", "A road that has some snow on it."], {"fire hydrant": [[0.073, 0.58, 0.166, 0.897]]}], "response": "Task: What is the significance of the object marked as <Region>[0.073, 0.58, 0.166, 0.897]</Region> in the image?\nOutput: The object in the image is a fire hydrant, which provides access to water for firefighters to extinguish fires in the area.", "instruction": "What is the significance of the object marked as <Region>[0.073, 0.58, 0.166, 0.897]</Region> in the image?", "input": "", "output": "The object in the image is a fire hydrant, which provides access to water for firefighters to extinguish fires in the area.", "is_classification": false, "dataset": "COCO", "img_info": "000000042161.jpg"}
{"img_id": 48299, "query": [["a photo of a stampede of cows in the street", "Cowboys on horses herding cattle down a blacktopped road.", "A couple of ranchers herding cattle down a country road.", "A herd of cattle being herded by cowboys.", "People on horses are herding cows down a street."], {"horse": [[0.378, 0.359, 0.411, 0.494], [0.002, 0.438, 0.044, 0.53], [0.678, 0.329, 0.697, 0.365], [0.601, 0.336, 0.616, 0.359]], "cow": [[0.135, 0.423, 0.182, 0.503], [0.18, 0.41, 0.223, 0.526], [0.727, 0.374, 0.752, 0.425], [0.285, 0.38, 0.328, 0.501], [0.413, 0.348, 0.463, 0.483], [0.465, 0.342, 0.503, 0.477], [0.512, 0.356, 0.556, 0.473], [0.599, 0.356, 0.64, 0.464], [0.679, 0.356, 0.719, 0.44], [0.632, 0.357, 0.667, 0.463], [0.322, 0.369, 0.35, 0.5], [0.723, 0.369, 0.755, 0.428], [0.194, 0.318, 0.814, 0.521]], "person": [[0.356, 0.306, 0.425, 0.435], [0.508, 0.306, 0.533, 0.36], [0.541, 0.313, 0.569, 0.37], [0.593, 0.307, 0.618, 0.354], [0.0, 0.393, 0.053, 0.528], [0.669, 0.303, 0.704, 0.366], [0.266, 0.362, 0.282, 0.396]]}], "response": "Task: Describe the action being taken by the <Region>[0.135, 0.423, 0.182, 0.503]</Region> in the photo.\nOutput: This object is a cow, and it is a part of a stampede of cows that are being herded by cowboys on horses. Hence, the cow is in the process of running down the street being a part of the herd or attempting to break free from it.", "instruction": "Describe the action being taken by the <Region>[0.135, 0.423, 0.182, 0.503]</Region> in the photo.", "input": "", "output": "This object is a cow, and it is a part of a stampede of cows that are being herded by cowboys on horses. Hence, the cow is in the process of running down the street being a part of the herd or attempting to break free from it.", "is_classification": false, "dataset": "COCO", "img_info": "000000048299.jpg"}
{"img_id": 47975, "query": [["A dog stands on the beach next to a bag.", "a dog on a beach near a body of water ", "A GOLD DOG STANDING ON THE BEACH NEAR A PAIR OF JEANS ", "A large brown dog standing on top of a beach.", "A dog standing on the beach looking out at the ocean "], {"dog": [[0.019, 0.244, 0.561, 0.786]], "backpack": [[0.1, 0.677, 0.246, 0.764]]}], "response": "Task: What can you infer about the relationship between the dog and the object in <Region>[0.1, 0.677, 0.246, 0.764]</Region> based on the image?\nOutput: The object is a backpack, and based on its location in the image, it's likely to belong to someone who is with the dog at the beach.", "instruction": "What can you infer about the relationship between the dog and the object in <Region>[0.1, 0.677, 0.246, 0.764]</Region> based on the image?", "input": "", "output": "The object is a backpack, and based on its location in the image, it's likely to belong to someone who is with the dog at the beach.", "is_classification": false, "dataset": "COCO", "img_info": "000000047975.jpg"}
{"img_id": 42860, "query": [["Two people are flying kites on a beach. ", "Stairs lead down to a beach where children play with kites. ", "a number of people on a beach near the water", "Stairs leading down to the beach area . ", "People are on the beach by the water with kites."], {"person": [[0.427, 0.621, 0.496, 0.762], [0.35, 0.54, 0.381, 0.602]], "kite": [[0.22, 0.486, 0.293, 0.553], [0.435, 0.54, 0.487, 0.598]]}], "response": "Task: Please describe the scene in the image and explain the relationship between the object located in <Region>[0.22, 0.486, 0.293, 0.553]</Region> and the object in <Region>[0.427, 0.621, 0.496, 0.762]</Region> in that scene.\nOutput: This image shows people on a beach by the water, and it appears they are enjoying flying kites. One of the objects is a kite located in the upper-left part of the image, and the other object is a person located in the center-bottom of the image. Therefore, the person is most likely flying the kite.", "instruction": "Please describe the scene in the image and explain the relationship between the object located in <Region>[0.22, 0.486, 0.293, 0.553]</Region> and the object in <Region>[0.427, 0.621, 0.496, 0.762]</Region> in that scene.", "input": "", "output": "This image shows people on a beach by the water, and it appears they are enjoying flying kites. One of the objects is a kite located in the upper-left part of the image, and the other object is a person located in the center-bottom of the image. Therefore, the person is most likely flying the kite.", "is_classification": false, "dataset": "COCO", "img_info": "000000042860.jpg"}
{"img_id": 33642, "query": [["The double decked bus is driving down the road.", "The city bus is traveling down the road this time of day. ", "A double decker bus is driving on the street.", "A red double decker bus travels through the city.", "A red tour bus passing through the city."], {"car": [[0.186, 0.489, 0.287, 0.588]], "bus": [[0.236, 0.091, 1.0, 0.97]], "person": [[0.157, 0.411, 0.216, 0.525], [0.071, 0.367, 0.122, 0.479], [0.631, 0.576, 0.673, 0.641], [0.572, 0.553, 0.603, 0.6], [0.001, 0.336, 0.027, 0.417], [0.756, 0.613, 0.824, 0.703]]}], "response": "Task: What is the purpose of the object located in <Region>[0.186, 0.489, 0.287, 0.588]</Region> as shown in the image?\nOutput: The object in the given region is a car seen on the road, and its purpose is to transport people and goods. It is traveling on the same road as the double-decker bus, suggesting that both vehicles are on the same route at the same time.", "instruction": "What is the purpose of the object located in <Region>[0.186, 0.489, 0.287, 0.588]</Region> as shown in the image?", "input": "", "output": "The object in the given region is a car seen on the road, and its purpose is to transport people and goods. It is traveling on the same road as the double-decker bus, suggesting that both vehicles are on the same route at the same time.", "is_classification": false, "dataset": "COCO", "img_info": "000000033642.jpg"}
{"img_id": 27348, "query": [["A man in an open air market sells vegetables.", "a person standing at a stand with many bananas ", "A man standing at a banana stand during the night.", "A man is standing at an outdoor stall selling bananas.", "A man is standing at his stand as another looks on. "], {"chair": [[0.003, 0.604, 0.199, 0.984]], "person": [[0.177, 0.16, 0.322, 0.692], [0.831, 0.252, 0.924, 0.76], [0.714, 0.251, 0.747, 0.388]], "banana": [[0.607, 0.634, 0.755, 0.741], [0.458, 0.59, 0.586, 0.74], [0.473, 0.532, 0.593, 0.626], [0.328, 0.558, 0.464, 0.646], [0.316, 0.632, 0.46, 0.751], [0.295, 0.667, 0.324, 0.701], [0.616, 0.522, 0.753, 0.654], [0.556, 0.599, 0.624, 0.662]], "motorcycle": [[0.643, 0.294, 0.788, 0.563]]}], "response": "Task: What is the object in <Region>[0.607, 0.634, 0.755, 0.741]</Region> and <Region>[0.458, 0.59, 0.586, 0.74]</Region> used for, and how might it be useful for the man selling vegetables?\nOutput: The object in the given bounding boxes is a bunch of bananas, a type of fruit. It can be used to attract customers to the man's vegetable stand. The bananas might catch people's attention, and they might come to buy the stands' other products leading to an increase in sales.", "instruction": "What is the object in <Region>[0.607, 0.634, 0.755, 0.741]</Region> and <Region>[0.458, 0.59, 0.586, 0.74]</Region> used for, and how might it be useful for the man selling vegetables?", "input": "", "output": "The object in the given bounding boxes is a bunch of bananas, a type of fruit. It can be used to attract customers to the man's vegetable stand. The bananas might catch people's attention, and they might come to buy the stands' other products leading to an increase in sales.", "is_classification": false, "dataset": "COCO", "img_info": "000000027348.jpg"}
{"img_id": 58987, "query": [["A small yellow car with a driver sitting on the right side of the vehicle.", "A yellow car travels down the road with a surf board on top.", "A yellow car with a surfboard tied to the roof", "A man is driving a car near the sea.", "A yellow car driving away from a house near the beach."], {"car": [[0.252, 0.486, 0.963, 0.989], [0.0, 0.59, 0.049, 0.677], [0.003, 0.545, 0.108, 0.584], [0.787, 0.53, 0.805, 0.543]], "person": [[0.404, 0.546, 0.494, 0.668], [0.901, 0.496, 0.931, 0.588], [0.802, 0.573, 0.812, 0.602]], "surfboard": [[0.421, 0.43, 0.686, 0.526]]}], "response": "Task: Provide details about the location of the object in <Region>[0.421, 0.43, 0.686, 0.526]</Region> in the scene.\nOutput: The scene shows a yellow car with a surfboard tied to the roof, and the object in the specified region is the surfboard. The surfboard is located on top of the car's roof and is tightly secured to it. The car is most likely heading towards the ocean, and the surfboard will be used by the driver to surf the waves.", "instruction": "Provide details about the location of the object in <Region>[0.421, 0.43, 0.686, 0.526]</Region> in the scene.", "input": "", "output": "The scene shows a yellow car with a surfboard tied to the roof, and the object in the specified region is the surfboard. The surfboard is located on top of the car's roof and is tightly secured to it. The car is most likely heading towards the ocean, and the surfboard will be used by the driver to surf the waves.", "is_classification": false, "dataset": "COCO", "img_info": "000000058987.jpg"}
{"img_id": 34786, "query": [["A man dressed as a town fryer walking across the street using a cell phone", "a man dressed in a red jacket and tri point hat walking down the road talking on a cell phone\n", "An older man in a pirate outfit is walking down the street in a big city while talking on the phone", "a man wearing a small pirates hat walks across the street while on his cellphone", "The man is wearing a pirate hat, talking on the phone, and walking down the street."], {"person": [[0.475, 0.348, 0.676, 1.0], [0.217, 0.438, 0.296, 0.581], [0.854, 0.427, 0.887, 0.561], [0.912, 0.423, 0.944, 0.56], [0.0, 0.678, 0.078, 0.754], [0.943, 0.419, 0.978, 0.556], [0.98, 0.429, 1.0, 0.633], [0.878, 0.428, 0.899, 0.559], [0.832, 0.418, 0.856, 0.5], [0.465, 0.422, 0.478, 0.437], [0.344, 0.555, 0.403, 0.641], [0.957, 0.525, 1.0, 0.573]], "cell phone": [[0.583, 0.416, 0.605, 0.454]], "traffic light": [[0.226, 0.297, 0.239, 0.335], [0.288, 0.295, 0.3, 0.328], [0.17, 0.274, 0.186, 0.317], [0.236, 0.347, 0.249, 0.367], [0.276, 0.344, 0.284, 0.369], [0.166, 0.37, 0.172, 0.378], [0.275, 0.398, 0.283, 0.419], [0.223, 0.37, 0.23, 0.396], [0.186, 0.382, 0.192, 0.391], [0.071, 0.287, 0.086, 0.321]], "fire hydrant": [[0.798, 0.531, 0.837, 0.681]], "backpack": [[0.203, 0.481, 0.267, 0.552], [0.918, 0.44, 0.931, 0.473]], "car": [[0.002, 0.506, 0.822, 0.91], [0.645, 0.432, 0.87, 0.641], [0.023, 0.444, 0.066, 0.488], [0.189, 0.425, 0.255, 0.48], [0.222, 0.433, 0.264, 0.486], [0.115, 0.439, 0.162, 0.48], [0.273, 0.429, 0.288, 0.436], [0.087, 0.442, 0.121, 0.478], [0.402, 0.448, 0.444, 0.505], [0.059, 0.444, 0.074, 0.485]], "bus": [[0.147, 0.389, 0.201, 0.47], [0.283, 0.37, 0.384, 0.493]], "truck": [[0.28, 0.37, 0.373, 0.496], [0.419, 0.43, 0.542, 0.51]], "parking meter": [[0.894, 0.5, 0.908, 0.542], [0.643, 0.445, 0.652, 0.476]]}], "response": "Task: Describe the surroundings of the man shown in <Region>[0.475, 0.348, 0.676, 1.0]</Region>. \nOutput: The man in this bounding box is walking across the street while talking on the phone. He is surrounded by other pedestrians, several cars, and buses. There are traffic lights, parking meters, and fire hydrants on the street. The buildings and shops on either side of the street are visible, and there is a backpack visible on someone's back in the background.", "instruction": "Describe the surroundings of the man shown in <Region>[0.475, 0.348, 0.676, 1.0]</Region>.", "input": "", "output": "The man in this bounding box is walking across the street while talking on the phone. He is surrounded by other pedestrians, several cars, and buses. There are traffic lights, parking meters, and fire hydrants on the street. The buildings and shops on either side of the street are visible, and there is a backpack visible on someone's back in the background.", "is_classification": false, "dataset": "COCO", "img_info": "000000034786.jpg"}
{"img_id": 48558, "query": [["A man is holding a plastic bat near the tree. ", "The man wants to hit the pinata on the tree.", "A person standing by a tree in a yard.", "A man holding a yellow bat while standing next to a  tree.", "A man in a blue shirt swings a yellow bat at a red pinata."], {"person": [[0.518, 0.591, 0.834, 1.0]], "baseball bat": [[0.517, 0.825, 0.709, 0.962]]}], "response": "Task: What is the person in <Region>[0.518, 0.591, 0.834, 1.0]</Region> trying to accomplish with the object in <Region>[0.517, 0.825, 0.709, 0.962]</Region>?\nOutput: The man is holding a yellow plastic bat and standing next to a tree. The object in his hand is a baseball bat, and he is using it to try to hit a pinata that is hanging on the tree for a fun activity or game.", "instruction": "What is the person in <Region>[0.518, 0.591, 0.834, 1.0]</Region> trying to accomplish with the object in <Region>[0.517, 0.825, 0.709, 0.962]</Region>?", "input": "", "output": "The man is holding a yellow plastic bat and standing next to a tree. The object in his hand is a baseball bat, and he is using it to try to hit a pinata that is hanging on the tree for a fun activity or game.", "is_classification": false, "dataset": "COCO", "img_info": "000000048558.jpg"}
{"img_id": 24600, "query": [["A double decker bus passes a blue car and buildings on the street.", "A red double decker bus is shown driving on the street.", "Red double-decker bus driving on a street near highrise office buildings.", "A red double decker bus drives through a city,", "A red double decker bus driving in front of a building"], {"car": [[0.167, 0.652, 0.333, 0.792], [0.915, 0.615, 0.963, 0.701], [0.968, 0.629, 1.0, 0.679]], "bus": [[0.327, 0.135, 0.919, 0.924]], "person": [[0.068, 0.655, 0.082, 0.681], [0.107, 0.655, 0.129, 0.731], [0.05, 0.666, 0.066, 0.727], [0.656, 0.274, 0.717, 0.375], [0.411, 0.568, 0.455, 0.652]]}], "response": "Task: Can you describe the position of the blue car <Region>[0.167, 0.652, 0.333, 0.792]</Region> in relation to the red double-decker bus <Region>[0.327, 0.135, 0.919, 0.924]</Region> in the image?\nOutput: In the image, the blue car is passing the red double-decker bus on the street. The position of the car is to the left of the bus.", "instruction": "Can you describe the position of the blue car <Region>[0.167, 0.652, 0.333, 0.792]</Region> in relation to the red double-decker bus <Region>[0.327, 0.135, 0.919, 0.924]</Region> in the image?", "input": "", "output": "In the image, the blue car is passing the red double-decker bus on the street. The position of the car is to the left of the bus.", "is_classification": false, "dataset": "COCO", "img_info": "000000024600.jpg"}
{"img_id": 27104, "query": [["A sign with a sad face that reads my tummy hurts.with a pile of junk sitting around it.", "There is a big pile of garbage on the curb.", "Chair sitting atop refrigerator and other assorted used household items at curb.", "A bunch of boxes that are sitting near the curb.", "An old refrigerator with duct tape all over it sits in a pile of trash with an old wicker chair on top of it.  "], {"refrigerator": [[0.372, 0.311, 0.728, 0.761]], "car": [[0.231, 0.432, 0.301, 0.47], [0.711, 0.41, 0.818, 0.5], [0.252, 0.422, 0.281, 0.432], [0.277, 0.423, 0.302, 0.443], [0.183, 0.429, 0.207, 0.436]], "chair": [[0.425, 0.069, 0.669, 0.319]]}], "response": "Task: What is the role of the object located in <Region>[0.372, 0.311, 0.728, 0.761]</Region> in the scene depicted in the image?\nOutput: The object is an old refrigerator with duct tape all over it, and it is part of the pile of trash. It is likely that the refrigerator is being discarded and the sign next to it indicates that someone is experiencing an upset stomach due to eating unhealthy food.", "instruction": "What is the role of the object located in <Region>[0.372, 0.311, 0.728, 0.761]</Region> in the scene depicted in the image?", "input": "", "output": "The object is an old refrigerator with duct tape all over it, and it is part of the pile of trash. It is likely that the refrigerator is being discarded and the sign next to it indicates that someone is experiencing an upset stomach due to eating unhealthy food.", "is_classification": false, "dataset": "COCO", "img_info": "000000027104.jpg"}
{"img_id": 16823, "query": [["A woman on a deck having just finished eating", "A cat laying on a banister next to a woman sitting at a table.", "A cat on top of a rail next to a woman sitting at a table.", "A lady has just about finished her plate with a knife. ", "A woman with a cat and an empty plate."], {"cat": [[0.665, 0.26, 0.869, 0.394]], "dining table": [[0.003, 0.706, 0.998, 0.998], [0.139, 0.28, 0.291, 0.413]], "cup": [[0.551, 0.89, 0.78, 1.0], [0.185, 0.267, 0.217, 0.288]], "knife": [[0.089, 0.783, 0.463, 0.878]], "person": [[0.648, 0.172, 0.678, 0.227], [0.608, 0.187, 0.643, 0.229], [0.581, 0.185, 0.611, 0.231], [0.27, 0.142, 0.351, 0.346], [0.03, 0.094, 0.677, 0.802]], "bowl": [[0.742, 0.833, 0.976, 0.988], [0.175, 0.283, 0.21, 0.298]], "chair": [[0.0, 0.516, 0.069, 0.727], [0.638, 0.371, 0.687, 0.545], [0.228, 0.255, 0.351, 0.363]]}], "response": "Task: What is the woman doing in <Region>[0.742, 0.833, 0.976, 0.988]</Region> of the image?\nOutput: The woman has finished eating, and the object is a bowl, which seems to be empty now, so she might be cleaning up after having enjoyed her meal.", "instruction": "What is the woman doing in <Region>[0.742, 0.833, 0.976, 0.988]</Region> of the image?", "input": "", "output": "The woman has finished eating, and the object is a bowl, which seems to be empty now, so she might be cleaning up after having enjoyed her meal.", "is_classification": false, "dataset": "COCO", "img_info": "000000016823.jpg"}
{"img_id": 32877, "query": [["A person riding a wave on top of a surfboard.", "A man surfing on a wave in the ocean.", "a man surfing in the water on a large wave", "person surfing a large wave in the ocean", "A man in a wet suit riding on a surfboard. "], {"person": [[0.657, 0.276, 0.78, 0.471]], "surfboard": [[0.626, 0.397, 0.714, 0.51]]}], "response": "Task: Which object is bigger in the image, <Region>[0.657, 0.276, 0.78, 0.471]</Region> or <Region>[0.626, 0.397, 0.714, 0.51]</Region>, and how do the two objects fit together in the image?\nOutput: The two objects are a person and a surfboard, and the surfboard is bigger than the person. The person is riding on top of the surfboard while riding a wave in the ocean.", "instruction": "Which object is bigger in the image, <Region>[0.657, 0.276, 0.78, 0.471]</Region> or <Region>[0.626, 0.397, 0.714, 0.51]</Region>, and how do the two objects fit together in the image?", "input": "", "output": "The two objects are a person and a surfboard, and the surfboard is bigger than the person. The person is riding on top of the surfboard while riding a wave in the ocean.", "is_classification": false, "dataset": "COCO", "img_info": "000000032877.jpg"}
{"img_id": 52614, "query": [["A man wearing a black wetsuit surfing on a wave.", "A man in black wetsuit surfing on a wave.", "A picture of a surfer as he catches a wave.", "a man on a surf board in the water", "A person in a wetsuit on a surfboard on a wave in the ocean."], {"person": [[0.442, 0.398, 0.529, 0.549]], "surfboard": [[0.39, 0.526, 0.51, 0.566]]}], "response": "Task: How is the person in <Region>[0.442, 0.398, 0.529, 0.549]</Region> maintaining balance on the surfboard as shown in the image?\nOutput: The person in the image is maintaining balance on the surfboard by shifting their weight while riding the wave. This is done by using the legs and torso to move the center of gravity while continuously adjusting the position on the board. They also use their arms to paddle and maintain stability.", "instruction": "How is the person in <Region>[0.442, 0.398, 0.529, 0.549]</Region> maintaining balance on the surfboard as shown in the image?", "input": "", "output": "The person in the image is maintaining balance on the surfboard by shifting their weight while riding the wave. This is done by using the legs and torso to move the center of gravity while continuously adjusting the position on the board. They also use their arms to paddle and maintain stability.", "is_classification": false, "dataset": "COCO", "img_info": "000000052614.jpg"}
{"img_id": 24002, "query": [["Woman cleaning the teeth of a dog lying on its back.", "A woman holding down a dog with a swab in her hand. ", "a woman lays a dog sown as is gets cleaned ", "a close up of a dog laying down with a person", "A person who is holding a dog in her lap."], {"dog": [[0.002, 0.203, 0.927, 0.656]], "person": [[0.002, 0.005, 0.998, 0.989]], "toothbrush": [[0.704, 0.319, 0.913, 0.636], [0.879, 0.35, 0.898, 0.357]]}], "response": "Task: What is the woman doing with the object located in <Region>[0.704, 0.319, 0.913, 0.636]</Region> and where is the dog in this process?\nOutput: The woman is using a toothbrush on the dog's teeth. The dog is lying down on its back while this process is taking place.", "instruction": "What is the woman doing with the object located in <Region>[0.704, 0.319, 0.913, 0.636]</Region> and where is the dog in this process?", "input": "", "output": "The woman is using a toothbrush on the dog's teeth. The dog is lying down on its back while this process is taking place.", "is_classification": false, "dataset": "COCO", "img_info": "000000024002.jpg"}
{"img_id": 26665, "query": [["A herd of zebras standing together in a field.", "Herd of zebras standing on a rocky field.", "A herd of zebras standing in a dirt field.", "A group of zebras stand together on the rocky ground.", "a close up of a number of zebras on a dirt ground"], {"zebra": [[0.116, 0.224, 0.277, 0.688], [0.224, 0.204, 0.445, 0.949], [0.369, 0.063, 0.628, 0.23], [0.521, 0.276, 0.893, 0.983], [0.419, 0.17, 0.729, 0.373], [0.0, 0.327, 0.112, 0.871], [0.715, 0.506, 0.825, 0.948], [0.136, 0.092, 0.398, 0.243], [0.419, 0.262, 0.558, 0.911], [0.154, 0.0, 0.252, 0.111], [0.545, 0.057, 0.875, 0.385]]}], "response": "Task: How would you describe the positioning of the zebras in <Region>[0.116, 0.224, 0.277, 0.688]</Region> and <Region>[0.419, 0.262, 0.558, 0.911]</Region> in relation to the rest of the zebras in the image?\nOutput: In the given image, there are multiple zebras, and the two zebras are positioned in a group, standing closely together, but not necessarily close to the entire herd present.", "instruction": "How would you describe the positioning of the zebras in <Region>[0.116, 0.224, 0.277, 0.688]</Region> and <Region>[0.419, 0.262, 0.558, 0.911]</Region> in relation to the rest of the zebras in the image?", "input": "", "output": "In the given image, there are multiple zebras, and the two zebras are positioned in a group, standing closely together, but not necessarily close to the entire herd present.", "is_classification": false, "dataset": "COCO", "img_info": "000000026665.jpg"}
{"img_id": 37527, "query": [["A man is holding a glass while surrounded by bottle of wine.", "A man holding a wine glass standing in a winery.", "A man holding a glass stands in front of a large rack of wine bottles.", "A man standing in a room with lots of bottles of wine.", "a man is standing near a bunch of bottles"], {"person": [[0.114, 0.31, 0.566, 0.884]], "bottle": [[0.517, 0.364, 0.562, 0.484], [0.731, 0.346, 0.791, 0.5], [0.655, 0.362, 0.691, 0.492], [0.819, 0.35, 0.873, 0.51], [0.872, 0.354, 0.911, 0.502], [0.952, 0.539, 1.0, 0.706], [0.842, 0.531, 0.885, 0.674], [0.611, 0.522, 0.657, 0.64], [0.655, 0.517, 0.699, 0.649], [0.447, 0.37, 0.495, 0.466], [0.595, 0.349, 0.653, 0.494], [0.553, 0.364, 0.602, 0.482], [0.689, 0.353, 0.716, 0.499], [0.002, 0.014, 0.998, 0.637]], "wine glass": [[0.301, 0.522, 0.353, 0.574]], "cup": [[0.149, 0.947, 0.198, 1.0], [0.298, 0.902, 0.346, 0.957], [0.206, 0.936, 0.251, 0.967], [0.095, 0.885, 0.126, 0.911], [0.131, 0.875, 0.165, 0.906], [0.079, 0.941, 0.121, 0.977], [0.184, 0.882, 0.267, 0.957], [0.048, 0.932, 0.079, 0.969], [0.255, 0.937, 0.298, 0.976], [0.022, 0.923, 0.054, 0.952], [0.0, 0.917, 0.033, 0.949]]}], "response": "Task: What is the man holding in <Region>[0.301, 0.522, 0.353, 0.574]</Region> of the image?\nOutput: The man is holding a wine glass. He is most likely drinking wine surrounded by bottles of wine in a winery.", "instruction": "What is the man holding in <Region>[0.301, 0.522, 0.353, 0.574]</Region> of the image?", "input": "", "output": "The man is holding a wine glass. He is most likely drinking wine surrounded by bottles of wine in a winery.", "is_classification": false, "dataset": "COCO", "img_info": "000000037527.jpg"}
{"img_id": 14683, "query": [["Two people are riding horses in field with sheep.", "Two people ride on horseback through a grassy field", "Two horseback riders gallop swiftly alongside each other.", "Two jockeys rid inning in a pasture with sheep.", "Two men riding their horses on a grassy hill with sheep in the background."], {"horse": [[0.554, 0.305, 0.929, 0.7], [0.058, 0.326, 0.361, 0.728]], "sheep": [[0.466, 0.157, 0.525, 0.226], [0.587, 0.153, 0.626, 0.234], [0.174, 0.101, 0.233, 0.145], [0.537, 0.008, 0.561, 0.031], [0.127, 0.244, 0.214, 0.305], [0.384, 0.219, 0.44, 0.257], [0.579, 0.015, 0.609, 0.037]], "person": [[0.226, 0.264, 0.305, 0.57], [0.72, 0.209, 0.81, 0.533]]}], "response": "Task: Based on the information provided, can you identify the object in <Region>[0.554, 0.305, 0.929, 0.7]</Region> and describe its use in the scenario depicted in the image?\nOutput: The object in the given region is a horse. The image shows two people riding horses together in a grassy field with sheep. The horses are the primary mode of transportation used in this scenario. They are ridden by the people to move around the field more efficiently and enjoyably.", "instruction": "Based on the information provided, can you identify the object in <Region>[0.554, 0.305, 0.929, 0.7]</Region> and describe its use in the scenario depicted in the image?", "input": "", "output": "The object in the given region is a horse. The image shows two people riding horses together in a grassy field with sheep. The horses are the primary mode of transportation used in this scenario. They are ridden by the people to move around the field more efficiently and enjoyably.", "is_classification": false, "dataset": "COCO", "img_info": "000000014683.jpg"}
{"img_id": 4760, "query": [["A set of park benches near a lamp post", "A park with benches, a lamppost, and trees.", "Park scene, park bench, light pole and building in back ground, probably city park area.", "Two benches are separated by a pole front of a brick wall.", "A fire hydrant with a bench and building in a background."], {"fire hydrant": [[0.431, 0.488, 0.521, 0.76]], "bench": [[0.089, 0.32, 0.569, 0.525], [0.678, 0.273, 0.899, 0.398]]}], "response": "Task: In this park scene, what is the distance between the two benches present in <Region>[0.089, 0.32, 0.569, 0.525]</Region> and <Region>[0.678, 0.273, 0.899, 0.398]</Region>? \nOutput: The two benches present in the image are separated by a significant distance, and their distance is approximately 2 meters.", "instruction": "In this park scene, what is the distance between the two benches present in <Region>[0.089, 0.32, 0.569, 0.525]</Region> and <Region>[0.678, 0.273, 0.899, 0.398]</Region>?", "input": "", "output": "The two benches present in the image are separated by a significant distance, and their distance is approximately 2 meters.", "is_classification": false, "dataset": "COCO", "img_info": "000000004760.jpg"}
{"img_id": 41342, "query": [["The windshield of a bus that is in the street.", "large bus with a reflection of the sky and clouds on its windshield", "The front of a bus in a closeup shot ", "There is a large passenger bus and the sky is reflected in its front window.", "A large white and black bus drives down the road."], {"car": [[0.004, 0.534, 0.056, 0.572]], "bus": [[0.285, 0.002, 1.0, 0.989]]}], "response": "Task: What is the role of the object in <Region>[0.285, 0.002, 1.0, 0.989]</Region> in the image?\nOutput: The object is a bus that takes up the majority of the image. The image shows the bus driving down a road while the sky and clouds are reflected in its front window.", "instruction": "What is the role of the object in <Region>[0.285, 0.002, 1.0, 0.989]</Region> in the image?", "input": "", "output": "The object is a bus that takes up the majority of the image. The image shows the bus driving down a road while the sky and clouds are reflected in its front window.", "is_classification": false, "dataset": "COCO", "img_info": "000000041342.jpg"}
{"img_id": 16283, "query": [["People that are making a pizza from start to finish. ", "Four frames of different people doing various activity's.", "a man is holding some food in a kitchen", "A series of four photos showing a group of people preparing food in a kitchen.", "four different pictures of men making homemade pizzas"], {"pizza": [[0.237, 0.826, 0.41, 0.891]], "oven": [[0.296, 0.767, 0.498, 0.991]], "person": [[0.036, 0.53, 0.357, 0.948], [0.715, 0.695, 0.782, 0.858], [0.69, 0.019, 0.993, 0.497], [0.802, 0.713, 0.889, 0.98], [0.198, 0.009, 0.498, 0.505], [0.795, 0.709, 0.849, 0.952], [0.005, 0.622, 0.064, 0.96], [0.102, 0.613, 0.143, 0.661], [0.164, 0.043, 0.314, 0.495]], "bottle": [[0.293, 0.63, 0.323, 0.706], [0.271, 0.647, 0.289, 0.704], [0.7, 0.804, 0.719, 0.85], [0.324, 0.647, 0.349, 0.701]], "dining table": [[0.932, 0.905, 0.986, 0.989]]}], "response": "Task: What step of the pizza making process is shown in the <Region>[0.237, 0.826, 0.41, 0.891]</Region> region of the image?\nOutput: This image shows people making a pizza, and the object is a pizza. The region shows the almost complete pizza, with the toppings and the cheese evenly spread on the pizza dough.", "instruction": "What step of the pizza making process is shown in the <Region>[0.237, 0.826, 0.41, 0.891]</Region> region of the image?", "input": "", "output": "This image shows people making a pizza, and the object is a pizza. The region shows the almost complete pizza, with the toppings and the cheese evenly spread on the pizza dough.", "is_classification": false, "dataset": "COCO", "img_info": "000000016283.jpg"}
{"img_id": 8179, "query": [["A green traffic light above a street with a car on coming.", "A green traffic light above a street with one car driving by.", "The traffic lite is green as a car approaches.", "Picture of an outdoor place that is very beautiful.\n", "a close up of a traffic light above a city street "], {"car": [[0.188, 0.528, 0.297, 0.652]], "traffic light": [[0.585, 0.0, 0.977, 0.542], [0.372, 0.0, 0.606, 0.443]]}], "response": "Task: What is the current color of the traffic light shown in the image at <Region>[0.372, 0.0, 0.606, 0.443]</Region>?\nOutput: The traffic light shown in the image is currently green.", "instruction": "What is the current color of the traffic light shown in the image at <Region>[0.372, 0.0, 0.606, 0.443]</Region>?", "input": "", "output": "The traffic light shown in the image is currently green.", "is_classification": false, "dataset": "COCO", "img_info": "000000008179.jpg"}
{"img_id": 40534, "query": [["There is a person in the picture alone.\n", "Someone is in the water preparing to water ski.", "There is a little girl wearing water skis holding onto a line.", "A young person being towed by a boat while riding water skis.", "Young female water skier just before starting a run."], {"person": [[0.483, 0.436, 0.547, 0.551]], "skis": [[0.485, 0.507, 0.554, 0.562]]}], "response": "Task: What seem to be the activities being carried out by the person in the image, and what is their relationship to the object located in <Region>[0.485, 0.507, 0.554, 0.562]</Region>?\nOutput: The person is in the water, and the object is a pair of water skis. The person is most likely water skiing as the skis are being used as a means to glide across the surface of the water.", "instruction": "What seem to be the activities being carried out by the person in the image, and what is their relationship to the object located in <Region>[0.485, 0.507, 0.554, 0.562]</Region>?", "input": "", "output": "The person is in the water, and the object is a pair of water skis. The person is most likely water skiing as the skis are being used as a means to glide across the surface of the water.", "is_classification": false, "dataset": "COCO", "img_info": "000000040534.jpg"}
{"img_id": 26900, "query": [["A bus parked on the side of the road with a man getting stuff out of the back.", "A large bus parked on a city street at night.", "People at a tour bus parked at the airport", "The bus is setting at the station ready to board.", "A bus stop with a bus parked at night and people standing outside of the bus under a covered area."], {"bus": [[0.279, 0.009, 0.922, 0.861], [0.243, 0.407, 0.281, 0.583], [0.215, 0.392, 0.278, 0.542]], "person": [[0.251, 0.475, 0.319, 0.693], [0.045, 0.409, 0.102, 0.602], [0.001, 0.484, 0.05, 1.0]], "handbag": [[0.022, 0.624, 0.104, 0.846], [0.045, 0.549, 0.103, 0.651], [0.091, 0.644, 0.14, 0.758]], "suitcase": [[0.291, 0.547, 0.325, 0.658]]}], "response": "Task: What is the object located in <Region>[0.045, 0.549, 0.103, 0.651]</Region> and why is it significant in the context of the image?\nOutput: The object is a handbag, and it is significant because it is being carried by one of the people pictured near the bus and is possibly their possession.", "instruction": "What is the object located in <Region>[0.045, 0.549, 0.103, 0.651]</Region> and why is it significant in the context of the image?", "input": "", "output": "The object is a handbag, and it is significant because it is being carried by one of the people pictured near the bus and is possibly their possession.", "is_classification": false, "dataset": "COCO", "img_info": "000000026900.jpg"}
{"img_id": 30205, "query": [["A couple of people standing on a beach.", "Adults at beach with child with surfer with sail in background ", "a person that is flying a kite in the air", "People on the beach with a dome tent and a large oil tanker in the background.", "A picture of a man and woman on a beach and the man is sitting in a tent. "], {"boat": [[0.598, 0.402, 0.753, 0.472], [0.575, 0.442, 0.784, 0.472], [0.337, 0.453, 0.352, 0.474]], "person": [[0.469, 0.789, 0.587, 0.915], [0.419, 0.713, 0.474, 0.945]], "kite": [[0.536, 0.081, 0.611, 0.195]]}], "response": "Task: What is the action being performed with the object in <Region>[0.536, 0.081, 0.611, 0.195]</Region> and what is the scenery in the background?\nOutput: The image shows a person flying a kite, and in the background we can see the beach.", "instruction": "What is the action being performed with the object in <Region>[0.536, 0.081, 0.611, 0.195]</Region> and what is the scenery in the background?", "input": "", "output": "The image shows a person flying a kite, and in the background we can see the beach.", "is_classification": false, "dataset": "COCO", "img_info": "000000030205.jpg"}
{"img_id": 24802, "query": [["A trailer with a couple of single sized beds and a couch.", "A picture of a apartment back in the day.", "The narrow room has a bed, a mirror, and a couch in it.", "The room is small and has a bed with pillows. ", "The narrow room has several beds in it."], {"bed": [[0.207, 0.661, 0.342, 0.913], [0.0, 0.517, 0.168, 0.719], [0.527, 0.524, 0.841, 0.719], [0.551, 0.697, 0.985, 0.928]]}], "response": "Task: How many beds are present in the room as shown in <Region>[0.207, 0.661, 0.342, 0.913]</Region>?\nOutput: There is only one bed present in the given region as the coordinates correspond to a single <b>bed</b> according to the bounding box.", "instruction": "How many beds are present in the room as shown in <Region>[0.207, 0.661, 0.342, 0.913]</Region>?", "input": "", "output": "There is only one bed present in the given region as the coordinates correspond to a single <b>bed</b> according to the bounding box.", "is_classification": false, "dataset": "COCO", "img_info": "000000024802.jpg"}
{"img_id": 52729, "query": [["A group of people riding skis down a snow covered slope.", "A line of young people are on snow skis.", "A group of people on snow skis participating in a race.", "Three skiers are racing in a straight line.", "some people in uniforms skiing along a track"], {"person": [[0.435, 0.092, 1.0, 0.885], [0.095, 0.097, 0.592, 0.738], [0.034, 0.074, 0.24, 0.631], [0.913, 0.203, 0.972, 0.239], [0.826, 0.175, 0.928, 0.239]], "skis": [[0.261, 0.685, 0.819, 0.818], [0.045, 0.576, 0.648, 0.681], [0.562, 0.8, 0.98, 0.989]]}], "response": "Task: Please describe the motion of the object in <Region>[0.261, 0.685, 0.819, 0.818]</Region> during the moment captured in the image.\nOutput: The object in this region is a pair of skis, and it is gliding downhill on the snow covered slope along with the person standing on them. They are skiing down the slope with a forward, side-to-side motion, gently turning left and right to control speed and direction.", "instruction": "Please describe the motion of the object in <Region>[0.261, 0.685, 0.819, 0.818]</Region> during the moment captured in the image.", "input": "", "output": "The object in this region is a pair of skis, and it is gliding downhill on the snow covered slope along with the person standing on them. They are skiing down the slope with a forward, side-to-side motion, gently turning left and right to control speed and direction.", "is_classification": false, "dataset": "COCO", "img_info": "000000052729.jpg"}
{"img_id": 16251, "query": [["A very nice and large modern style rest room.", "A view of a bathroom tub, sink and mirror.", "A sink and mirror in a very small bathroom.", "Nice spacious bathroom with marble tiling and dual sinks", "Clean bathroom with white towels, and dark vanity with marble top."], {"sink": [[0.255, 0.535, 0.367, 0.589], [0.0, 0.597, 0.046, 0.668]], "cup": [[0.266, 0.398, 0.297, 0.448]]}], "response": "Task: What is the object located in <Region>[0.266, 0.398, 0.297, 0.448]</Region> used for, and where is it found in the image?\nOutput: The image shows a sink, and the object is a cup. It is located on the sink, which may indicate that it is used for mouth rinsing or tooth brushing.", "instruction": "What is the object located in <Region>[0.266, 0.398, 0.297, 0.448]</Region> used for, and where is it found in the image?", "input": "", "output": "The image shows a sink, and the object is a cup. It is located on the sink, which may indicate that it is used for mouth rinsing or tooth brushing.", "is_classification": false, "dataset": "COCO", "img_info": "000000016251.jpg"}
{"img_id": 53949, "query": [["Men holding tennis rackets are shaking hands above a net.", "Three tennis players are talking over the net.", "A group of men standing on top of a tennis court.", "Tennis players shake hands over a tennis net. ", "Three men stand around a tennis net while holding tennis rackets, attempting to shake hands with each other."], {"person": [[0.0, 0.001, 0.309, 0.99], [0.497, 0.046, 1.0, 0.97], [0.168, 0.09, 0.776, 0.724], [0.312, 0.43, 0.41, 0.584]], "tennis racket": [[0.611, 0.571, 0.788, 0.782], [0.164, 0.68, 1.0, 0.967]]}], "response": "Task: Who is most likely to have served the ball based on the position of the object in <Region>[0.611, 0.571, 0.788, 0.782]</Region> in the image?\nOutput: The object in the region is a tennis racket. Based on its position, it is most likely that the person holding the racket has served the ball.", "instruction": "Who is most likely to have served the ball based on the position of the object in <Region>[0.611, 0.571, 0.788, 0.782]</Region> in the image?", "input": "", "output": "The object in the region is a tennis racket. Based on its position, it is most likely that the person holding the racket has served the ball.", "is_classification": false, "dataset": "COCO", "img_info": "000000053949.jpg"}
{"img_id": 57377, "query": [["A train that is sitting on the tracks.", "stream running beside a railroad track in the mountains.", "A stream with rocks outside by train, with hills and evergreens.", "A mountains hil is littered with tall trees", "A train rolling by a mountain stream and trees."], {"train": [[0.775, 0.622, 0.999, 0.76]], "person": [[0.837, 0.659, 0.85, 0.679]]}], "response": "Task: What is the object located in <Region>[0.837, 0.659, 0.85, 0.679]</Region> doing in the scene?\nOutput: The image shows a train, and the object is a person. They are most likely a passenger on the train, either boarding or leaving the train.", "instruction": "What is the object located in <Region>[0.837, 0.659, 0.85, 0.679]</Region> doing in the scene?", "input": "", "output": "The image shows a train, and the object is a person. They are most likely a passenger on the train, either boarding or leaving the train.", "is_classification": false, "dataset": "COCO", "img_info": "000000057377.jpg"}
{"img_id": 25855, "query": [["A girl on her phone sitting on the floor.", "A woman sitting on the floor using her smartphone.", "a woman sits on the floor next to a tube on her phone", "A woman uses her iphone while sitting on the floor next to her yoga mat.", "The woman is sitting on the floor using her cell phone."], {"bottle": [[0.596, 0.746, 0.659, 0.845]], "cell phone": [[0.467, 0.463, 0.564, 0.532]], "person": [[0.087, 0.097, 0.661, 0.942]]}], "response": "Task: What is the object present in <Region>[0.467, 0.463, 0.564, 0.532]</Region> of the image, and what activity is the person in the image performing using the object?\nOutput: The object in this region is a cell phone, and the person in the image is using it to browse the internet or communicate with others.", "instruction": "What is the object present in <Region>[0.467, 0.463, 0.564, 0.532]</Region> of the image, and what activity is the person in the image performing using the object?", "input": "", "output": "The object in this region is a cell phone, and the person in the image is using it to browse the internet or communicate with others.", "is_classification": false, "dataset": "COCO", "img_info": "000000025855.jpg"}
{"img_id": 368, "query": [["A young boy playing soccer on a grassy field", "A group of kids playing soccer on a field.", "Small child in soccer uniform kicking at yellow ball. ", "A young man kicking a soccer ball around a field.", "There are several young boys practicing soccer, some with their dads."], {"sports ball": [[0.432, 0.372, 0.53, 0.45]], "person": [[0.344, 0.168, 0.823, 0.594], [0.329, 0.127, 0.441, 0.298], [0.134, 0.31, 0.252, 0.366], [0.282, 0.232, 0.365, 0.361], [0.564, 0.112, 0.641, 0.24], [0.885, 0.217, 0.977, 0.371], [0.594, 0.191, 0.754, 0.563], [0.755, 0.215, 0.833, 0.368], [0.168, 0.214, 0.229, 0.317], [0.135, 0.216, 0.175, 0.315], [0.446, 0.188, 0.488, 0.234], [0.0, 0.215, 0.039, 0.361]]}], "response": "Task: What is the function of the object located in <Region>[0.432, 0.372, 0.53, 0.45]</Region> in the scene? \nOutput: The object is a soccer ball, and the function of this object in the scene is to be kicked and passed among the players during the soccer game.", "instruction": "What is the function of the object located in <Region>[0.432, 0.372, 0.53, 0.45]</Region> in the scene?", "input": "", "output": "The object is a soccer ball, and the function of this object in the scene is to be kicked and passed among the players during the soccer game.", "is_classification": false, "dataset": "COCO", "img_info": "000000000368.jpg"}
{"img_id": 15968, "query": [["A man sitting on a couch in a black suit and tie looking impatient. ", "a person sitting down wearing a suit and tie", "A man sitting on a couch wearing a black jacket and tie.", "A man in all black attire setting on a light gray couch.", "A man with a full black suit and tie sitting on a couch."], {"couch": [[0.008, 0.348, 0.998, 0.989]], "tie": [[0.462, 0.428, 0.518, 0.797]], "person": [[0.05, 0.133, 0.94, 1.0]]}], "response": "Task: Can you describe how the person sitting in the <Region>[0.008, 0.348, 0.998, 0.989]</Region> is feeling and what in the image supports your conclusion?\nOutput: The person in the image is wearing a black suit with a black tie and sitting on a gray couch. Their expression appears impatient, or stressed. This is supported by their body language, being quite stiff and tense.", "instruction": "Can you describe how the person sitting in the <Region>[0.008, 0.348, 0.998, 0.989]</Region> is feeling and what in the image supports your conclusion?", "input": "", "output": "The person in the image is wearing a black suit with a black tie and sitting on a gray couch. Their expression appears impatient, or stressed. This is supported by their body language, being quite stiff and tense.", "is_classification": false, "dataset": "COCO", "img_info": "000000015968.jpg"}
{"img_id": 38828, "query": [["A woman riding a horse while it eats grass", "a young lady on horseback stops to let her horse eat a while", "a person riding a horse near trees in the background", "A woman smiles as her horse grazes on some greenery.", "A woman riding on the back of a brown horse."], {"horse": [[0.039, 0.391, 0.871, 0.908]], "person": [[0.329, 0.151, 0.615, 0.616]], "handbag": [[0.35, 0.306, 0.463, 0.452]]}], "response": "Task: What is the woman holding in <Region>[0.35, 0.306, 0.463, 0.452]</Region> while she is riding the horse?\nOutput: The image shows a woman riding a horse, and the object in the given region is a handbag. While she enjoying her leisure time riding a horse, she brought a handbag with her that might contain water, first aid kit or other important things that she might require during the ride.", "instruction": "What is the woman holding in <Region>[0.35, 0.306, 0.463, 0.452]</Region> while she is riding the horse?", "input": "", "output": "The image shows a woman riding a horse, and the object in the given region is a handbag. While she enjoying her leisure time riding a horse, she brought a handbag with her that might contain water, first aid kit or other important things that she might require during the ride.", "is_classification": false, "dataset": "COCO", "img_info": "000000038828.jpg"}
{"img_id": 23395, "query": [["A man riding skis over a snow covered ski slope.", "Skier standing on the snow with poles and hands raised in the air", "The woman has her hands up in the air as she skis.", "A man on ski's that is standing in the snow.", "a snow skier in a blue jacket is holding their arms up"], {"person": [[0.281, 0.232, 0.413, 0.805]], "skis": [[0.137, 0.784, 0.465, 0.836]]}], "response": "Task: Can you describe the pose of the person in <Region>[0.281, 0.232, 0.413, 0.805]</Region> as seen in the image?\nOutput: The person in the image is a skier in the snow, and he is in an animated pose. He is holding his hands up, as if he is cheering or celebrating his accomplishment of skiing down the slope. He is probably feeling elated from the exhilarating experience. The pose also suggests that the skier is in control and confident on his skis.", "instruction": "Can you describe the pose of the person in <Region>[0.281, 0.232, 0.413, 0.805]</Region> as seen in the image?", "input": "", "output": "The person in the image is a skier in the snow, and he is in an animated pose. He is holding his hands up, as if he is cheering or celebrating his accomplishment of skiing down the slope. He is probably feeling elated from the exhilarating experience. The pose also suggests that the skier is in control and confident on his skis.", "is_classification": false, "dataset": "COCO", "img_info": "000000023395.jpg"}
{"img_id": 40458, "query": [["A woman holding a pink umbrella next to a bald headed man.", "A woman holding an umbrella near a group of people.", "Woman in a bright pink costume and bright pink decorated umbrella.", "a person standing holding an open umbrella ", "A woman in a pink dress is under a bright pink umbrella as confetti falls in the air."], {"umbrella": [[0.15, 0.22, 0.933, 0.734]], "person": [[0.015, 0.148, 0.335, 1.0], [0.506, 0.144, 0.61, 0.224], [0.0, 0.209, 0.117, 0.405], [0.624, 0.225, 1.0, 1.0], [0.237, 0.622, 0.577, 1.0], [0.324, 0.208, 0.433, 0.245]]}], "response": "Task: What is the physical relationship between the object in <Region>[0.15, 0.22, 0.933, 0.734]</Region> and the people in the image?\nOutput: The object is an umbrella, and a woman is holding it next to a bald-headed man.", "instruction": "What is the physical relationship between the object in <Region>[0.15, 0.22, 0.933, 0.734]</Region> and the people in the image?", "input": "", "output": "The object is an umbrella, and a woman is holding it next to a bald-headed man.", "is_classification": false, "dataset": "COCO", "img_info": "000000040458.jpg"}
{"img_id": 12884, "query": [["A skateboarder doing tricks using a fence in a lot", "A man on a skateboard does a trick on a gate. ", "a person jumping a skate board in the air", "The skateboarder is using the fencing as a surface for riding.", "A person on a skateboard jumping on a fence."], {"person": [[0.32, 0.166, 0.621, 0.44]], "skateboard": [[0.366, 0.327, 0.565, 0.44]]}], "response": "Task: Can you describe what the person in <Region>[0.32, 0.166, 0.621, 0.44]</Region> is doing with the object in <Region>[0.366, 0.327, 0.565, 0.44]</Region>? \nOutput: The person in the image is riding a skateboard, and the object is the skateboard he is riding. He is using the fence as a surface for riding and performing tricks.", "instruction": "Can you describe what the person in <Region>[0.32, 0.166, 0.621, 0.44]</Region> is doing with the object in <Region>[0.366, 0.327, 0.565, 0.44]</Region>?", "input": "", "output": "The person in the image is riding a skateboard, and the object is the skateboard he is riding. He is using the fence as a surface for riding and performing tricks.", "is_classification": false, "dataset": "COCO", "img_info": "000000012884.jpg"}
{"img_id": 2270, "query": [["A living room that has a couch in it.", "A large living area with wood floors, chairs and a piano.", "A living room with wooden floors and tan walls. ", "A picture of a living room with a piano as the focal peice.", "Beautiful wooden living room with modern furniture and sunlit windows."], {"potted plant": [[0.0, 0.678, 0.192, 0.999]], "couch": [[0.254, 0.338, 0.6, 0.56]], "chair": [[0.24, 0.835, 0.385, 1.0], [0.589, 0.426, 0.847, 0.807], [0.08, 0.646, 0.23, 0.842]], "cup": [[0.443, 0.496, 0.466, 0.535], [0.364, 0.469, 0.386, 0.518]], "vase": [[0.623, 0.452, 0.656, 0.511], [0.66, 0.453, 0.684, 0.501]], "bowl": [[0.418, 0.489, 0.443, 0.514], [0.466, 0.501, 0.492, 0.518]], "book": [[0.258, 0.513, 0.325, 0.555], [0.236, 0.534, 0.321, 0.566]]}], "response": "Task: What is the object in <Region>[0.589, 0.426, 0.847, 0.807]</Region>, and how does it relate to the focal point of the image?\nOutput: The object in the specified region is a chair, and its relationship with the focal point of the image is that it, along with the other objects in the image, form a part of the furniture set that complements the interior of the living room. The chair provides additional seating in the living room, which is designed primarily to host guests and promote conversation.", "instruction": "What is the object in <Region>[0.589, 0.426, 0.847, 0.807]</Region>, and how does it relate to the focal point of the image?", "input": "", "output": "The object in the specified region is a chair, and its relationship with the focal point of the image is that it, along with the other objects in the image, form a part of the furniture set that complements the interior of the living room. The chair provides additional seating in the living room, which is designed primarily to host guests and promote conversation.", "is_classification": false, "dataset": "COCO", "img_info": "000000002270.jpg"}
{"img_id": 47333, "query": [["A gray tabby cat is curled in a red bowl that sits on a table near a window.", "a brown and black cat is sleeping in a bowl on a table", "A grey tiger cat sleeping in a brown bowl on a table.", "an image of a cat sitting inside of a bowl on the kitchen table", "a cat asleep in a fruit bowl on a dining room table"], {"cat": [[0.163, 0.221, 0.79, 0.683]], "bottle": [[0.089, 0.236, 0.162, 0.643]], "chair": [[0.151, 0.0, 0.356, 0.567], [0.75, 0.002, 0.999, 0.434]], "dining table": [[0.002, 0.41, 1.0, 0.988]], "bowl": [[0.121, 0.479, 0.852, 0.807]], "laptop": [[0.002, 0.004, 0.178, 0.789]]}], "response": "Task: What is the object <Region>[0.121, 0.479, 0.852, 0.807]</Region> in the image and what is the cat doing inside it?\nOutput: The object in this image is a bowl, and the cat is curled up inside it, sleeping peacefully.", "instruction": "What is the object <Region>[0.121, 0.479, 0.852, 0.807]</Region> in the image and what is the cat doing inside it?", "input": "", "output": "The object in this image is a bowl, and the cat is curled up inside it, sleeping peacefully.", "is_classification": false, "dataset": "COCO", "img_info": "000000047333.jpg"}
{"img_id": 12372, "query": [["A large umbrella and some chairs on a deck.", "Patio furniture complete with umbrella on a deck in a private backyard garden ", "An umbrella on a patio shades the  table and chairs.", "An umbrella is open able a table and chairs.", "The patio has some chairs where people can sit under a umbrella. "], {"umbrella": [[0.034, 0.096, 0.996, 0.765]], "chair": [[0.165, 0.621, 0.635, 0.92], [0.762, 0.621, 0.998, 0.759], [0.778, 0.697, 1.0, 0.999]], "dining table": [[0.652, 0.754, 0.967, 0.954]]}], "response": "Task: What is the function of the object located in <Region>[0.652, 0.754, 0.967, 0.954]</Region> and how is it related to the other objects in the image?\nOutput: The object in this region is a dining table. It functions as a surface to store food, drinks, and dining ware. It is related to the other objects in the image, such as chairs and umbrellas, as they are designed for outdoor recreation and relaxation. The chairs surround the dining table, allowing people to sit and enjoy a meal. The umbrella is placed to provide shade for the people sitting at the table.", "instruction": "What is the function of the object located in <Region>[0.652, 0.754, 0.967, 0.954]</Region> and how is it related to the other objects in the image?", "input": "", "output": "The object in this region is a dining table. It functions as a surface to store food, drinks, and dining ware. It is related to the other objects in the image, such as chairs and umbrellas, as they are designed for outdoor recreation and relaxation. The chairs surround the dining table, allowing people to sit and enjoy a meal. The umbrella is placed to provide shade for the people sitting at the table.", "is_classification": false, "dataset": "COCO", "img_info": "000000012372.jpg"}
{"img_id": 18841, "query": [["A woman holding a case with a  laptop inside", "A group of people sitting at a table around an open laptop computer.", "Three people gather around a laptop to talk.", "an open case holding an old keyboard in it ", "A woman opens the lid of a laptop built into a suitcase."], {"person": [[0.574, 0.001, 0.825, 0.666], [0.005, 0.004, 0.372, 0.988], [0.885, 0.0, 1.0, 0.258]], "laptop": [[0.233, 0.182, 0.69, 0.964]], "dining table": [[0.027, 0.49, 0.707, 0.983]]}], "response": "Task: Can you identify the object in <Region>[0.233, 0.182, 0.69, 0.964]</Region> of the image?\nOutput: Yes, the object in the specified region is a laptop.", "instruction": "Can you identify the object in <Region>[0.233, 0.182, 0.69, 0.964]</Region> of the image?", "input": "", "output": "Yes, the object in the specified region is a laptop.", "is_classification": false, "dataset": "COCO", "img_info": "000000018841.jpg"}
{"img_id": 17882, "query": [["There are bikes parked next to the cars.", "A row of bikes parked on the side of a street.", "A row of bicycles that are all parked next to each other.", "A row of bikes parked along a sidewalk beside some cars", "Bikes lined up on a bike rack beside cars on the street. "], {"bicycle": [[0.371, 0.498, 0.525, 0.594], [0.322, 0.522, 0.478, 0.612], [0.302, 0.599, 0.451, 0.671], [0.176, 0.722, 0.348, 0.796], [0.518, 0.289, 0.669, 0.37], [0.483, 0.412, 0.625, 0.466], [0.384, 0.448, 0.526, 0.533], [0.283, 0.544, 0.447, 0.629], [0.189, 0.674, 0.341, 0.755], [0.095, 0.793, 0.273, 0.883], [0.059, 0.813, 0.213, 0.907], [0.011, 0.896, 0.202, 1.0], [0.428, 0.417, 0.552, 0.505], [0.198, 0.639, 0.359, 0.741], [0.096, 0.289, 0.95, 0.958]], "person": [[0.753, 0.67, 0.815, 0.746], [0.818, 0.081, 0.869, 0.198], [0.883, 0.361, 0.964, 0.468], [0.923, 0.059, 0.986, 0.182], [0.891, 0.066, 0.937, 0.136], [0.527, 0.504, 0.62, 0.585], [0.931, 0.019, 0.985, 0.08], [0.921, 0.422, 0.999, 0.487], [0.872, 0.101, 0.952, 0.216]], "car": [[0.44, 0.058, 0.714, 0.296], [0.072, 0.343, 0.441, 0.672], [0.0, 0.708, 0.075, 0.861]], "umbrella": [[0.815, 0.475, 1.0, 0.668]], "chair": [[0.664, 0.865, 0.825, 0.987]]}], "response": "Task: How do the parked bikes affect the flow of movement through the sidewalk at the position of the <Region>[0.176, 0.722, 0.348, 0.796]</Region>?\nOutput: The parked bikes are blocking the sidewalk, and this may force the pedestrians to take a detour or walk on the street. This situation may affect the flow of movement through the sidewalk and create a potential safety hazard.", "instruction": "How do the parked bikes affect the flow of movement through the sidewalk at the position of the <Region>[0.176, 0.722, 0.348, 0.796]</Region>?", "input": "", "output": "The parked bikes are blocking the sidewalk, and this may force the pedestrians to take a detour or walk on the street. This situation may affect the flow of movement through the sidewalk and create a potential safety hazard.", "is_classification": false, "dataset": "COCO", "img_info": "000000017882.jpg"}
{"img_id": 2886, "query": [["A plate with dinner on it set on a tablecloth", "A plate full of vegetables, potatoes, and meat.", "THIS IS A LARGE PLATE OF FOOD WITH A GLASS OF WINE", "A plate topped with meat, veggies and fruit.", "Thre is a plate with different kinds of food on it"], {"bottle": [[0.0, 0.132, 0.163, 0.413], [0.387, 0.001, 0.642, 0.351]], "wine glass": [[0.129, 0.002, 0.335, 0.395]], "fork": [[0.0, 0.316, 0.202, 0.81], [0.94, 0.169, 1.0, 0.258]], "spoon": [[0.808, 0.38, 0.911, 0.985]], "bowl": [[0.696, 0.0, 1.0, 0.157], [0.554, 0.026, 0.804, 0.266]], "carrot": [[0.484, 0.492, 0.569, 0.805], [0.385, 0.834, 0.595, 0.905], [0.331, 0.689, 0.5, 0.737], [0.262, 0.729, 0.388, 0.851], [0.443, 0.735, 0.545, 0.91], [0.283, 0.895, 0.535, 0.96], [0.342, 0.843, 0.399, 0.936], [0.541, 0.665, 0.598, 0.738]], "dining table": [[0.002, 0.007, 1.0, 1.0]], "remote": [[0.0, 0.006, 0.123, 0.157]]}], "response": "Task: What is the position of the object in <Region>[0.129, 0.002, 0.335, 0.395]</Region> and what is its function?\nOutput: The object is a wine glass, placed on the table to the right of the plate. It is used to hold and drink wine while dining.", "instruction": "What is the position of the object in <Region>[0.129, 0.002, 0.335, 0.395]</Region> and what is its function?", "input": "", "output": "The object is a wine glass, placed on the table to the right of the plate. It is used to hold and drink wine while dining.", "is_classification": false, "dataset": "COCO", "img_info": "000000002886.jpg"}
{"img_id": 7596, "query": [["A man in a baseball uniform getting ready to bat the ball.", "MAN WITH THE NUMBER 11 STANDING ON HOME PLATE", "A baseball player gets ready to swing at the ball.", "a baseball player on a field is holding a bat", "A baseball player with a bat at the plate."], {"person": [[0.284, 0.049, 0.917, 0.783]], "baseball bat": [[0.302, 0.002, 0.39, 0.257]]}], "response": "Task: What is the position of the object in <Region>[0.302, 0.002, 0.39, 0.257]</Region> in relation to the subject of the photo?\nOutput: The object in the image is a bat, which is an integral part of the game of baseball. The position of the bat in the image is in the hands of the baseball player standing on home plate, who is preparing to hit the ball.", "instruction": "What is the position of the object in <Region>[0.302, 0.002, 0.39, 0.257]</Region> in relation to the subject of the photo?", "input": "", "output": "The object in the image is a bat, which is an integral part of the game of baseball. The position of the bat in the image is in the hands of the baseball player standing on home plate, who is preparing to hit the ball.", "is_classification": false, "dataset": "COCO", "img_info": "000000007596.jpg"}
