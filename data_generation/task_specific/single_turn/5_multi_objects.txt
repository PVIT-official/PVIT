{"img_id": 32367, "query": [["a street with a street light hanging over it ", "some traffic lights a road some fields and a sign", "Lonely stretch of road with sun shining bright in sky", "A street light is in the middle of nowhere.", "A temporary stop light is set up on an empty highway."], {"traffic light": [[0.616, 0.395, 0.634, 0.482], [0.462, 0.225, 0.486, 0.306]], "stop sign": [[0.79, 0.323, 0.907, 0.547]]}], "response": "Task: What is the shape and color of the object located in <Region>[0.79, 0.323, 0.907, 0.547]</Region>?\nOutput: The object in the specified region is a red, octagonal stop sign.", "instruction": "What is the shape and color of the object located in <Region>[0.79, 0.323, 0.907, 0.547]</Region>?", "input": "", "output": "The object in the specified region is a red, octagonal stop sign.", "is_classification": false, "dataset": "COCO", "img_info": "000000032367.jpg"}
{"img_id": 57878, "query": [["a lady eating at a table holding up the peace sign", "A woman makes a peace sign in front of a fully set table.", "A lady sitting at a table with food and drink, holding up two fingers.", "A woman gives the peace sign at lunch with her friend", "A woman giving a peace sign at a table."], {"bottle": [[0.781, 0.314, 0.898, 0.694], [0.733, 0.269, 0.809, 0.548]], "chair": [[0.25, 0.083, 0.433, 0.35], [0.059, 0.015, 0.144, 0.175], [0.504, 0.094, 0.628, 0.152], [0.124, 0.006, 0.177, 0.162], [0.157, 0.006, 0.199, 0.15], [0.186, 0.006, 0.229, 0.14], [0.0, 0.009, 0.075, 0.225], [0.044, 0.068, 0.114, 0.197], [0.354, 0.054, 0.467, 0.129], [0.307, 0.074, 0.449, 0.22], [0.388, 0.034, 0.418, 0.072], [0.096, 0.012, 0.128, 0.064]], "dining table": [[0.0, 0.252, 1.0, 1.0]], "person": [[0.421, 0.011, 0.667, 0.399], [0.614, 0.0, 0.673, 0.127]], "wine glass": [[0.474, 0.492, 0.545, 0.683], [0.551, 0.488, 0.625, 0.707]], "cup": [[0.281, 0.432, 0.365, 0.645], [0.354, 0.326, 0.401, 0.394], [0.375, 0.331, 0.417, 0.435], [0.629, 0.593, 0.703, 0.751], [0.365, 0.463, 0.395, 0.536], [0.385, 0.477, 0.418, 0.555]], "fork": [[0.291, 0.749, 0.361, 0.99]], "knife": [[0.678, 0.75, 0.763, 1.0]], "spoon": [[0.891, 0.81, 0.976, 0.9], [0.356, 0.368, 0.371, 0.399]], "couch": [[0.304, 0.073, 0.454, 0.265], [0.252, 0.085, 0.428, 0.281]], "vase": [[0.484, 0.316, 0.619, 0.451]]}], "response": "Task: What are the objects located close to the hand of the lady in <Region>[0.421, 0.011, 0.667, 0.399]</Region>?\nOutput: It seems that there are no objects located close to the hand of the lady in the specified region.", "instruction": "What are the objects located close to the hand of the lady in <Region>[0.421, 0.011, 0.667, 0.399]</Region>?", "input": "", "output": "It seems that there are no objects located close to the hand of the lady in the specified region.", "is_classification": false, "dataset": "COCO", "img_info": "000000057878.jpg"}
{"img_id": 69654, "query": [["a train sitting next to  a  pedestrian sitting on a bench on a  railway platform.", "Train on the tracks at a station with people sitting on a bench.", "A yellow train pulling into a train station. ", "A train coming in to a train station.", "A public transit train stopped at a train station."], {"train": [[0.103, 0.313, 0.838, 0.688]], "person": [[0.076, 0.488, 0.184, 0.67], [0.032, 0.554, 0.046, 0.593]], "bench": [[0.046, 0.562, 0.113, 0.672], [0.0, 0.755, 0.061, 0.908], [0.898, 0.557, 0.996, 0.612]], "handbag": [[0.076, 0.536, 0.116, 0.613]]}], "response": "Task: Is the person in <Region>[0.076, 0.536, 0.116, 0.613]</Region> facing towards or away from the train?\nOutput: The person in the specified region is facing away from the train.", "instruction": "Is the person in <Region>[0.076, 0.536, 0.116, 0.613]</Region> facing towards or away from the train?", "input": "", "output": "The person in the specified region is facing away from the train.", "is_classification": false, "dataset": "COCO", "img_info": "000000069654.jpg"}
{"img_id": 11420, "query": [["A white toilet sitting next to a bath tub and a sink.", "A beige marble bath at a hotel with shower glass wall instead of curtains", "A bathroom with a toilet, bath tub, shower head and toilet roll.", "Open toilet, mirror, and bath tub in a small bathroom.", "A bath tub, toilet, sink and toilet  paper in a restroom."], {"toilet": [[0.513, 0.447, 0.931, 0.989]], "bottle": [[0.871, 0.292, 0.887, 0.357], [0.881, 0.294, 0.9, 0.344]]}], "response": "Task: What is the possible object represented by the bottles in <Region>[0.871, 0.292, 0.887, 0.357]</Region> and <Region>[0.881, 0.294, 0.9, 0.344]</Region> respectively?\nOutput: It is not clear what objects the bottles in those regions are, as there are no distinguishing features visible to identify the contents.", "instruction": "What is the possible object represented by the bottles in <Region>[0.871, 0.292, 0.887, 0.357]</Region> and <Region>[0.881, 0.294, 0.9, 0.344]</Region> respectively?", "input": "", "output": "It is not clear what objects the bottles in those regions are, as there are no distinguishing features visible to identify the contents.", "is_classification": false, "dataset": "COCO", "img_info": "000000011420.jpg"}
{"img_id": 31454, "query": [["This plate includes a sandwich, pickles, and a bowl of sauce.", "Various foods are sitting on the plate on the table", "The plate has a sandwich on it as well as a pickle", "Two plates of food with soup and sandwiches", "Soup, pickles and sandwiches sit on these plates"], {"sandwich": [[0.603, 0.477, 0.996, 0.866], [0.483, 0.099, 0.732, 0.246]], "bowl": [[0.0, 0.492, 0.488, 0.82]], "dining table": [[0.0, 0.04, 1.0, 0.58]], "person": [[0.0, 0.0, 1.0, 0.155]]}], "response": "Task: Does the plate in <Region>[0.603, 0.477, 0.996, 0.866]</Region> have any side dish in addition to pickles?\nOutput: Yes, there is also a bowl of sauce on the plate.", "instruction": "Does the plate in <Region>[0.603, 0.477, 0.996, 0.866]</Region> have any side dish in addition to pickles?", "input": "", "output": "Yes, there is also a bowl of sauce on the plate.", "is_classification": false, "dataset": "COCO", "img_info": "000000031454.jpg"}
{"img_id": 17236, "query": [["Some men hugging each other and a person with plates on their head and shoulders.", "Two men hugging at a party in an office building.", "Two people are hugging in a classroom setting.", "Two mean getting ready to hug each other while standing in a classroom.", "two men are about to embrace for a hug "], {"tv": [[0.828, 0.73, 0.995, 0.984]], "bottle": [[0.548, 0.869, 0.571, 0.945]], "person": [[0.386, 0.162, 0.8, 1.0], [0.369, 0.246, 0.774, 0.988], [0.059, 0.528, 0.355, 1.0]], "tie": [[0.459, 0.463, 0.556, 1.0]], "cup": [[0.371, 0.666, 0.461, 0.816]], "book": [[0.001, 0.69, 0.11, 0.846], [0.001, 0.857, 0.103, 0.982], [0.061, 0.53, 0.076, 0.595], [0.023, 0.585, 0.03, 0.65], [0.03, 0.577, 0.038, 0.646]]}], "response": "Task: What is the person wearing on their head and on their shoulders in <Region>[0.386, 0.162, 0.8, 1.0]</Region>?\nOutput: The person in the region has a stack of several plates on top of their head and more on their shoulders.", "instruction": "What is the person wearing on their head and on their shoulders in <Region>[0.386, 0.162, 0.8, 1.0]</Region>?", "input": "", "output": "The person in the region has a stack of several plates on top of their head and more on their shoulders.", "is_classification": false, "dataset": "COCO", "img_info": "000000017236.jpg"}
{"img_id": 70861, "query": [["a double deckered bus on a city streed", "A busy street where buses are going in different  directions.", "A bus that is out on the road in the day.", "A double-decked bus parking in a parking lot in front of a brick building.", "A double decker bus driving down a street next to a building."], {"bus": [[0.0, 0.166, 0.203, 0.931], [0.224, 0.094, 0.845, 0.885], [0.838, 0.458, 1.0, 0.719], [0.201, 0.483, 0.237, 0.706]], "person": [[0.415, 0.366, 0.459, 0.668]]}], "response": "Task: What is the approximate height of the bus in <Region>[0.0, 0.166, 0.203, 0.931]</Region> compared to its length?\nOutput: The height of the bus in that region is approximately half of its length.", "instruction": "What is the approximate height of the bus in <Region>[0.0, 0.166, 0.203, 0.931]</Region> compared to its length?", "input": "", "output": "The height of the bus in that region is approximately half of its length.", "is_classification": false, "dataset": "COCO", "img_info": "000000070861.jpg"}
{"img_id": 17203, "query": [["A random clock on the beach reads 8:35. ", "a clock on a pole on a beach near water ", "A tall clocks stands in front of this deserted beach.", "A view of a clock on the beach with waves in the background.", "A clock tower is shown on the beach."], {"clock": [[0.495, 0.24, 0.586, 0.333], [0.458, 0.241, 0.479, 0.344]]}], "response": "Task: What is the time displayed on the clock in <Region>[0.495, 0.24, 0.586, 0.333]</Region>?\nOutput: The time displayed on the clock in the specified region is 8:35.", "instruction": "What is the time displayed on the clock in <Region>[0.495, 0.24, 0.586, 0.333]</Region>?", "input": "", "output": "The time displayed on the clock in the specified region is 8:35.", "is_classification": false, "dataset": "COCO", "img_info": "000000017203.jpg"}
{"img_id": 53823, "query": [["A crowd of people skiing across snow covered ground.", "A group of people skiing on white slope next to trees.", "A group of cross country skiers take off for a trek", "Bunch of people skiing on flat snowy terrain.", "Many people are skiing in the snowy woods."], {"person": [[0.42, 0.148, 0.7, 0.989], [0.335, 0.234, 0.371, 0.376], [0.248, 0.21, 0.357, 0.658], [0.175, 0.207, 0.292, 0.591], [0.121, 0.254, 0.181, 0.479], [0.822, 0.11, 0.992, 0.82], [0.443, 0.229, 0.467, 0.274], [0.727, 0.245, 0.804, 0.46], [0.379, 0.194, 0.527, 0.779], [0.211, 0.24, 0.235, 0.46], [0.187, 0.239, 0.214, 0.408], [0.053, 0.232, 0.093, 0.308], [0.578, 0.149, 0.753, 0.678], [0.0, 0.201, 0.883, 0.549]], "skis": [[0.482, 0.88, 0.67, 1.0], [0.711, 0.737, 1.0, 0.845], [0.262, 0.729, 0.594, 0.813], [0.109, 0.579, 0.301, 0.666], [0.253, 0.627, 0.39, 0.677], [0.757, 0.49, 0.884, 0.506], [0.029, 0.476, 0.169, 0.519], [0.0, 0.481, 0.078, 0.497], [0.66, 0.644, 0.828, 0.693], [0.759, 0.476, 0.815, 0.498], [0.657, 0.626, 0.845, 0.704], [0.737, 0.511, 0.883, 0.538], [0.725, 0.442, 0.804, 0.463], [0.184, 0.437, 0.411, 0.998]], "handbag": [[0.929, 0.37, 0.995, 0.461], [0.674, 0.353, 0.737, 0.401], [0.538, 0.485, 0.642, 0.566]], "backpack": [[0.535, 0.484, 0.638, 0.575]]}], "response": "Task: Among the skis visible in the picture, which set of skis, in the position of <Region>[0.029, 0.476, 0.169, 0.519]</Region>, is closest to a person?\nOutput: The set of skis in the specified region is not very close to any person in the image.", "instruction": "Among the skis visible in the picture, which set of skis, in the position of <Region>[0.029, 0.476, 0.169, 0.519]</Region>, is closest to a person?", "input": "", "output": "The set of skis in the specified region is not very close to any person in the image.", "is_classification": false, "dataset": "COCO", "img_info": "000000053823.jpg"}
{"img_id": 58171, "query": [["A little blonde girl sitting in a chair holding a cell phone.", "A little girl is holding onto a phone and smiling.", "A young girl smiles as she holds a cell phone.", "A smiling blond girl holding a phone and chewing gum.", "A young girl smiling holding a cell phone."], {"cell phone": [[0.322, 0.638, 0.489, 0.729]], "person": [[0.0, 0.006, 0.984, 1.0], [0.0, 0.005, 0.455, 0.82], [0.884, 0.515, 1.0, 1.0]]}], "response": "Task: Where is the little girl located in <Region>[0.0, 0.005, 0.455, 0.82]</Region>?\nOutput: The little blonde girl is located on the left side of the picture sitting in a chair.", "instruction": "Where is the little girl located in <Region>[0.0, 0.005, 0.455, 0.82]</Region>?", "input": "", "output": "The little blonde girl is located on the left side of the picture sitting in a chair.", "is_classification": false, "dataset": "COCO", "img_info": "000000058171.jpg"}
{"img_id": 72536, "query": [["A person is taking a slice of pizza inside.", "A large pizza sitting on top of a white plate.", "A thick crust pizza being eaten by someone.", "this person is eating a slice of pizza", "A child is ready to break apart the pizza. "], {"chair": [[0.001, 0.16, 0.433, 0.447], [0.433, 0.186, 0.524, 0.245]], "dining table": [[0.0, 0.413, 1.0, 0.986]], "person": [[0.408, 0.001, 1.0, 0.659]], "pizza": [[0.005, 0.415, 0.9, 0.987]]}], "response": "Task: What is the most likely position of the person in <Region>[0.408, 0.001, 1.0, 0.659]</Region> when eating the pizza?\nOutput: Based on the table position, the chair position, and the person's posture, the person is most likely seated at the table.", "instruction": "What is the most likely position of the person in <Region>[0.408, 0.001, 1.0, 0.659]</Region> when eating the pizza?", "input": "", "output": "Based on the table position, the chair position, and the person's posture, the person is most likely seated at the table.", "is_classification": false, "dataset": "COCO", "img_info": "000000072536.jpg"}
{"img_id": 53183, "query": [["A lady is eating a salad before digging into a huge pizza sitting in front of her.", "A smiling woman enjoys a salad as an artisan pizza sits on the table before her. ", "A woman sitting at a table in front of a pizza.", "We see a young woman enjoying a meal in a restaurant.", "A person that is eating some food on a table."], {"dining table": [[0.0, 0.524, 0.999, 1.0]], "person": [[0.292, 0.005, 0.858, 0.574], [0.902, 0.281, 1.0, 0.608]], "cup": [[0.072, 0.498, 0.199, 0.628], [0.125, 0.466, 0.225, 0.573]], "fork": [[0.316, 0.425, 0.484, 0.499], [0.236, 0.541, 0.482, 0.597]], "knife": [[0.206, 0.543, 0.458, 0.627], [0.0, 0.802, 0.098, 0.9]], "spoon": [[0.308, 0.424, 0.477, 0.5]], "pizza": [[0.102, 0.654, 0.83, 0.98]], "bowl": [[0.846, 0.683, 1.0, 0.868], [0.927, 0.601, 1.0, 0.691], [0.001, 0.621, 0.185, 0.722], [0.0, 0.737, 0.023, 0.827]], "bench": [[0.0, 0.213, 0.472, 0.542]]}], "response": "Task: What are the objects on the table in <Region>[0.0, 0.524, 0.999, 1.0]</Region>?\nOutput: There are four forks, one knife, one spoon, two cups, one bowl, one pizza, and a person sitting at the table in this region.", "instruction": "What are the objects on the table in <Region>[0.0, 0.524, 0.999, 1.0]</Region>?", "input": "", "output": "There are four forks, one knife, one spoon, two cups, one bowl, one pizza, and a person sitting at the table in this region.", "is_classification": false, "dataset": "COCO", "img_info": "000000053183.jpg"}
{"img_id": 38606, "query": [["A man standing in front of a microphone at a podium.", "Two men looking very bored with their politics.", "A man is speaking into a microphone at a podium.", "A man speaks with two flags in front of him.", "a man in a red tie at a podium a microphone and small flags"], {"tie": [[0.393, 0.47, 0.512, 0.912]], "person": [[0.051, 0.097, 0.735, 0.981], [0.838, 0.727, 0.953, 0.891]]}], "response": "Task: What is the dominant color of the tie worn by the person in <Region>[0.393, 0.47, 0.512, 0.912]</Region>?\nOutput: The person is wearing a red tie with mini blue dots on it.", "instruction": "What is the dominant color of the tie worn by the person in <Region>[0.393, 0.47, 0.512, 0.912]</Region>?", "input": "", "output": "The person is wearing a red tie with mini blue dots on it.", "is_classification": false, "dataset": "COCO", "img_info": "000000038606.jpg"}
{"img_id": 52365, "query": [["A man holding a Nintendo Wii game controller.", "A man removes his shirt near a sheet covered couch. ", "Two men are playing a video game in the a living room.", "Two men playing a wii in a living room by a couch.", "Two guys are playing some sort of video games."], {"couch": [[0.389, 0.546, 0.935, 0.983]], "person": [[0.169, 0.123, 0.412, 0.979], [0.495, 0.225, 0.803, 0.962]], "remote": [[0.137, 0.46, 0.781, 0.787]]}], "response": "Task: What is the man in <Region>[0.169, 0.123, 0.412, 0.979]</Region> doing and what is he holding?\nOutput: The man in the specified region is holding a Nintendo Wii game controller, probably in use, as the image suggests that the two persons on a couch on the left side are playing the game together.", "instruction": "What is the man in <Region>[0.169, 0.123, 0.412, 0.979]</Region> doing and what is he holding?", "input": "", "output": "The man in the specified region is holding a Nintendo Wii game controller, probably in use, as the image suggests that the two persons on a couch on the left side are playing the game together.", "is_classification": false, "dataset": "COCO", "img_info": "000000052365.jpg"}
{"img_id": 58651, "query": [["A green train traveling through a train yard.", "An old-fashioned train going down the tracks with lots of wires overhead.", "Train engine on a train track and releasing steam.", "An old locomotive is on the railroad tracks.", "A locomotive train spewing steam from it's engine."], {"train": [[0.19, 0.486, 0.844, 0.904]], "person": [[0.078, 0.703, 0.088, 0.727], [0.059, 0.697, 0.068, 0.726], [0.202, 0.774, 0.21, 0.846], [0.046, 0.702, 0.056, 0.726], [0.009, 0.742, 0.021, 0.793], [0.032, 0.727, 0.042, 0.791], [0.022, 0.579, 0.035, 0.601], [0.041, 0.599, 0.051, 0.633], [0.918, 0.654, 0.923, 0.689], [0.03, 0.609, 0.036, 0.632], [0.904, 0.655, 0.917, 0.709], [0.025, 0.738, 0.036, 0.793]], "car": [[0.93, 0.64, 0.982, 0.678], [0.913, 0.676, 0.966, 0.725], [0.91, 0.602, 0.974, 0.631], [0.964, 0.651, 1.0, 0.68], [0.981, 0.632, 1.0, 0.652]]}], "response": "Task: What is the position of the train in <Region>[0.19, 0.486, 0.844, 0.904]</Region> in the train yard? Are there any people on the train?\nOutput: The train in the specified region is on the tracks and seems to be departing from a train station. There are no people on it visible in the given image.", "instruction": "What is the position of the train in <Region>[0.19, 0.486, 0.844, 0.904]</Region> in the train yard? Are there any people on the train?", "input": "", "output": "The train in the specified region is on the tracks and seems to be departing from a train station. There are no people on it visible in the given image.", "is_classification": false, "dataset": "COCO", "img_info": "000000058651.jpg"}
{"img_id": 75535, "query": [["A brown cow standing in a  field next to a tree.", "A big cow with horns in front of two big wheels.", "Cow in front of two large decorated wheels.", "A brown and black cow standing in trash next to wooden circles.", "a long horned cow standing next to two decorative circles"], {"cow": [[0.102, 0.32, 0.963, 0.988]], "person": [[0.717, 0.223, 0.766, 0.302], [0.698, 0.233, 0.717, 0.302], [0.667, 0.238, 0.688, 0.297]]}], "response": "Task: What is the color of the cow in <Region>[0.102, 0.32, 0.963, 0.988]</Region> and what is it doing?\nOutput: The cow in the given region is brown in color. It is just standing and there is no information on what it is doing.", "instruction": "What is the color of the cow in <Region>[0.102, 0.32, 0.963, 0.988]</Region> and what is it doing?", "input": "", "output": "The cow in the given region is brown in color. It is just standing and there is no information on what it is doing.", "is_classification": false, "dataset": "COCO", "img_info": "000000075535.jpg"}
{"img_id": 43966, "query": [["a close up of a plate of sliced pizza on a table ", "A plate with a pizza on it and one slice is gone", "A pizza with one slice taken from it.", "a pizza with one piece cut out of it on a white plate", "a pizza with mushrooms and olives sits on a white plate"], {"dining table": [[0.008, 0.003, 0.818, 0.348], [0.133, 0.784, 1.0, 0.982]], "cup": [[0.799, 0.002, 0.987, 0.163]], "pizza": [[0.057, 0.485, 0.619, 0.942], [0.021, 0.099, 0.985, 0.559], [0.579, 0.448, 0.993, 0.925]]}], "response": "Task: What are the toppings present on the pizza in <Region>[0.579, 0.448, 0.993, 0.925]</Region>?\nOutput: The toppings present on the pizza in the specified region are mushrooms and olives.", "instruction": "What are the toppings present on the pizza in <Region>[0.579, 0.448, 0.993, 0.925]</Region>?", "input": "", "output": "The toppings present on the pizza in the specified region are mushrooms and olives.", "is_classification": false, "dataset": "COCO", "img_info": "000000043966.jpg"}
{"img_id": 31041, "query": [["A newly married couple cutting their wedding cake.", "a bride and groom are cutting a white cake", "A newly married couple cutting a triple layer cake.", "A bride and groom cutting their wedding cake.", "A married couple in a spotlight with a wedding cake. "], {"chair": [[0.843, 0.402, 0.955, 0.529], [0.658, 0.415, 0.78, 0.514]], "cake": [[0.39, 0.256, 0.571, 0.514]], "tie": [[0.233, 0.284, 0.311, 0.438]], "knife": [[0.417, 0.453, 0.437, 0.467]], "dining table": [[0.769, 0.518, 1.0, 0.857], [0.256, 0.59, 0.814, 1.0]], "person": [[0.321, 0.182, 0.44, 0.468], [0.124, 0.132, 0.379, 0.905]], "potted plant": [[0.43, 0.464, 0.852, 0.698]]}], "response": "Task: What type of flowers are on the cake in <Region>[0.39, 0.256, 0.571, 0.514]</Region>?\nOutput: There are no flowers on the cake. It is a plain white, triple layer cake without any kind of decoration.", "instruction": "What type of flowers are on the cake in <Region>[0.39, 0.256, 0.571, 0.514]</Region>?", "input": "", "output": "There are no flowers on the cake. It is a plain white, triple layer cake without any kind of decoration.", "is_classification": false, "dataset": "COCO", "img_info": "000000031041.jpg"}
{"img_id": 29801, "query": [["A man at an outdoor table having breakfast.", "An older man eating a bowl of oatmeal with a glass of orange juice.", "a man sitting at a table with a bowl of cereal and a glass of orange juice", "A man is sitting in front of a bowl of food at a table with utensils and two glasses of orange juice with two people in the background siting next to palm trees", "An older man sitting at a table with a bowl full of food and a glass of orange juice."], {"dining table": [[0.183, 0.661, 1.0, 0.99], [0.944, 0.441, 1.0, 0.523], [0.002, 0.436, 0.093, 0.455], [0.0, 0.651, 0.06, 1.0], [0.432, 0.435, 0.531, 0.469]], "person": [[0.59, 0.3, 0.626, 0.373], [0.473, 0.07, 0.974, 0.771], [0.512, 0.292, 0.601, 0.427]], "cup": [[0.305, 0.785, 0.427, 1.0], [0.402, 0.689, 0.49, 0.831], [0.528, 0.42, 0.54, 0.441]], "knife": [[0.542, 0.743, 0.616, 0.845]], "spoon": [[0.613, 0.877, 0.83, 0.936], [0.623, 0.965, 0.87, 1.0], [0.531, 0.653, 0.666, 0.737]], "bowl": [[0.42, 0.823, 0.546, 0.989], [0.609, 0.679, 0.788, 0.823]], "chair": [[0.023, 0.425, 0.12, 0.556], [0.006, 0.553, 0.266, 1.0]]}], "response": "Task: What is the man in <Region>[0.183, 0.661, 1.0, 0.99]</Region> using the knife for, and what is he doing with the bowl?\nOutput: It is not clearly visible what the person is doing with the knife because only a small part of his hand holding the knife is visible in the picture. However, he is most likely using the knife to cut something. He is eating out of the bowl with a spoon.", "instruction": "What is the man in <Region>[0.183, 0.661, 1.0, 0.99]</Region> using the knife for, and what is he doing with the bowl?", "input": "", "output": "It is not clearly visible what the person is doing with the knife because only a small part of his hand holding the knife is visible in the picture. However, he is most likely using the knife to cut something. He is eating out of the bowl with a spoon.", "is_classification": false, "dataset": "COCO", "img_info": "000000029801.jpg"}
{"img_id": 61949, "query": [["A bird perched on top of a banana tree with lots of leaves.", "A very lush green looking tree with bananas on it.", "Bananas are growing on a tree in the jungle.", "Unripe bananas on a tree in a forest.", "a large green leaf is hanging off of a tree"], {"banana": [[0.514, 0.263, 0.658, 0.445], [0.585, 0.186, 0.614, 0.27], [0.605, 0.195, 0.65, 0.318]], "person": [[0.64, 0.837, 0.818, 0.988]]}], "response": "Task: What is the most likely position of the bird mentioned in <Region>[0.514, 0.263, 0.658, 0.445]</Region> with respect to the banana tree?\nOutput: The bird is perched on top of the banana tree on some of its leaves.", "instruction": "What is the most likely position of the bird mentioned in <Region>[0.514, 0.263, 0.658, 0.445]</Region> with respect to the banana tree?", "input": "", "output": "The bird is perched on top of the banana tree on some of its leaves.", "is_classification": false, "dataset": "COCO", "img_info": "000000061949.jpg"}
{"img_id": 2142, "query": [["A man sitting on top of a bench near some pigeons.", "a person sitting on a bench and feeding birds ", "A man sitting on a bench surrounded by birds", "a man sits on a park bench surrounded by pidgeons", "A man on a bench is covered with birds."], {"bird": [[0.542, 0.722, 0.593, 0.843], [0.395, 0.751, 0.545, 0.9], [0.065, 0.804, 0.235, 0.974], [0.94, 0.764, 0.998, 0.923], [0.799, 0.639, 0.828, 0.701], [0.615, 0.623, 0.697, 0.699], [0.538, 0.655, 0.587, 0.693], [0.38, 0.665, 0.415, 0.714], [0.092, 0.48, 0.267, 0.657], [0.337, 0.759, 0.41, 0.886], [0.578, 0.664, 0.643, 0.746], [0.683, 0.691, 0.759, 0.816], [0.235, 0.661, 0.269, 0.712], [0.126, 0.643, 0.18, 0.707], [0.573, 0.756, 0.706, 0.947], [0.0, 0.008, 0.998, 0.997]], "car": [[0.63, 0.295, 0.713, 0.395]], "person": [[0.359, 0.101, 0.626, 0.705]], "bench": [[0.241, 0.368, 0.768, 0.713]]}], "response": "Task: What is probably the activity of the man in <Region>[0.241, 0.368, 0.768, 0.713]</Region> sitting on the bench surrounded by birds?\nOutput: From his posture and the presence of bird around him, it can be assumed that he is feeding the birds.", "instruction": "What is probably the activity of the man in <Region>[0.241, 0.368, 0.768, 0.713]</Region> sitting on the bench surrounded by birds?", "input": "", "output": "From his posture and the presence of bird around him, it can be assumed that he is feeding the birds.", "is_classification": false, "dataset": "COCO", "img_info": "000000002142.jpg"}
{"img_id": 10948, "query": [["A woman and a man holding Nintendo Wii game controllers.", "Two people are playing a game on the Nintendo Wii.", "Two people interacting with a video game system.", "A couple playing the Nintendo WII at home.", "Two people in a living room playing games"], {"cat": [[0.636, 0.611, 0.751, 0.713]], "couch": [[0.0, 0.615, 0.775, 0.998]], "person": [[0.411, 0.125, 0.818, 0.986], [0.071, 0.11, 0.431, 0.987]], "bicycle": [[0.907, 0.73, 1.0, 0.924]], "cup": [[0.793, 0.929, 0.836, 0.999]], "remote": [[0.769, 0.221, 0.8, 0.27], [0.389, 0.32, 0.425, 0.422], [0.355, 0.304, 0.395, 0.381], [0.549, 0.312, 0.571, 0.423]], "clock": [[0.726, 0.197, 0.783, 0.32]]}], "response": "Task: What are the two people playing on the Nintendo Wii in <Region>[0.411, 0.125, 0.818, 0.986]</Region>?\nOutput: The image does not provide information about which game the two people are playing on the Nintendo Wii.", "instruction": "What are the two people playing on the Nintendo Wii in <Region>[0.411, 0.125, 0.818, 0.986]</Region>?", "input": "", "output": "The image does not provide information about which game the two people are playing on the Nintendo Wii.", "is_classification": false, "dataset": "COCO", "img_info": "000000010948.jpg"}
{"img_id": 397, "query": [["A cheesy pizza sitting on top of a table.", "This large pizza has a lot of cheese and tomato sauce.", "A prepared pizza being served at a table.", "A pizza with olive, cheese, tomato and onion.", "Baked pizza with herbs displayed on serving tray at table."], {"chair": [[0.747, 0.0, 0.971, 0.204]], "pizza": [[0.007, 0.092, 0.993, 0.951]], "cup": [[0.0, 0.614, 0.057, 0.763], [0.75, 0.657, 0.986, 0.987]], "fork": [[0.93, 0.843, 1.0, 0.951]], "dining table": [[0.002, 0.727, 0.789, 1.0]], "person": [[0.006, 0.0, 0.358, 0.428]]}], "response": "Task: How much of the pizza in <Region>[0.007, 0.092, 0.993, 0.951]</Region> is covered with cheese?\nOutput: As seen in the image, almost the entire surface of the pizza is covered with cheese.", "instruction": "How much of the pizza in <Region>[0.007, 0.092, 0.993, 0.951]</Region> is covered with cheese?", "input": "", "output": "As seen in the image, almost the entire surface of the pizza is covered with cheese.", "is_classification": false, "dataset": "COCO", "img_info": "000000000397.jpg"}
{"img_id": 7650, "query": [["A person carrying a surfboard on top of a parking lot.", "Two people are walking in a parking lot holding surfboards. ", "Two people with surfboards are standing in a sandy parking lot.", "The surfers are carrying their boards from the lot to the beach.", "this is a man walking down a dirt path"], {"person": [[0.339, 0.515, 0.419, 0.827], [0.081, 0.491, 0.109, 0.559], [0.223, 0.493, 0.25, 0.636]], "surfboard": [[0.226, 0.422, 0.277, 0.639], [0.274, 0.501, 0.418, 0.615]], "backpack": [[0.208, 0.52, 0.226, 0.591]], "car": [[0.918, 0.406, 1.0, 0.586], [0.462, 0.465, 0.571, 0.543], [0.557, 0.424, 0.664, 0.551], [0.641, 0.435, 0.764, 0.559], [0.749, 0.406, 0.935, 0.568], [0.332, 0.469, 0.422, 0.521], [0.264, 0.473, 0.348, 0.53], [0.419, 0.454, 0.513, 0.537], [0.31, 0.479, 0.352, 0.524], [0.166, 0.475, 0.204, 0.522], [0.0, 0.474, 0.068, 0.599], [0.145, 0.48, 0.178, 0.522]]}], "response": "Task: How many people in total are holding surfboards in the picture? What is the position of the backpack relative to the person in <Region>[0.339, 0.515, 0.419, 0.827]</Region>?\nOutput: There are two people holding surfboards in the picture. The backpack is on the back of the person in the specified region.", "instruction": "How many people in total are holding surfboards in the picture? What is the position of the backpack relative to the person in <Region>[0.339, 0.515, 0.419, 0.827]</Region>?", "input": "", "output": "There are two people holding surfboards in the picture. The backpack is on the back of the person in the specified region.", "is_classification": false, "dataset": "COCO", "img_info": "000000007650.jpg"}
{"img_id": 48572, "query": [["A white plate of food and bowls on a table.", "a close up of a plate of food on a table", "A plate and three bowls filled with food sitting on a table.", "A wooden table with a large bowl that has cooked shrimp and salad in it and three small bowls above the main bowl.", "A purple table topped with bowls and a plate of food."], {"dining table": [[0.0, 0.004, 0.993, 0.983]], "fork": [[0.832, 0.222, 1.0, 0.357]], "bowl": [[0.234, 0.053, 0.542, 0.312], [0.002, 0.231, 0.241, 0.512], [0.564, 0.004, 0.91, 0.267]]}], "response": "Task: What seafood is in the largest bowl in <Region>[0.564, 0.004, 0.91, 0.267]</Region> and what is in the smaller bowls above it?\nOutput: It is not clearly identifiable which seafood is in that bowl from the given region, but the larger bowl seems to contain some type of salad and shrimp, while the smaller bowls contain sauce or small pieces of side dishes.", "instruction": "What seafood is in the largest bowl in <Region>[0.564, 0.004, 0.91, 0.267]</Region> and what is in the smaller bowls above it?", "input": "", "output": "It is not clearly identifiable which seafood is in that bowl from the given region, but the larger bowl seems to contain some type of salad and shrimp, while the smaller bowls contain sauce or small pieces of side dishes.", "is_classification": false, "dataset": "COCO", "img_info": "000000048572.jpg"}
{"img_id": 62132, "query": [["Some bears holding a British flag are in a trailer", "A teddy bear with a sign in front of it.", "Teddy bears in a cart send some wishes.", "Two bears laying against wood with a sign.", "Some cute stuffed animals and a sign on a wagon."], {"car": [[0.007, 0.348, 0.177, 0.497], [0.109, 0.366, 0.309, 0.773], [0.0, 0.367, 0.031, 0.474]], "teddy bear": [[0.196, 0.121, 0.604, 0.735], [0.545, 0.226, 0.744, 0.429]]}], "response": "Task: What does the sign in the hands of the teddy bear in <Region>[0.545, 0.226, 0.744, 0.429]</Region> say?\nOutput: There is no sign in the hands of the teddy bear present in the given region.", "instruction": "What does the sign in the hands of the teddy bear in <Region>[0.545, 0.226, 0.744, 0.429]</Region> say?", "input": "", "output": "There is no sign in the hands of the teddy bear present in the given region.", "is_classification": false, "dataset": "COCO", "img_info": "000000062132.jpg"}
{"img_id": 58001, "query": [["A dog asleep on the couch with its paw on the remote ", "A dog with its paw on a remote controller on a couch.", "Small dog sleeping on a couch with remote controls.", "A DOG SLEEPING ON A COUCH WITH HIS PAW ON A REMOTE", "A pug sleeping on two remotes for the TV."], {"dog": [[0.081, 0.024, 0.951, 0.82]], "backpack": [[0.003, 0.0, 0.171, 0.872]], "couch": [[0.0, 0.0, 1.0, 1.0]], "remote": [[0.097, 0.688, 0.352, 0.963], [0.586, 0.761, 0.693, 0.984]]}], "response": "Task: What type of dog breed is sleeping on the couch with its paw on the remote in <Region>[0.081, 0.024, 0.951, 0.82]</Region>?\nOutput: It is not specified what type of dog breed is sleeping on the couch, but based on the description, it is a small dog and specifically mentioned as a pug, which is a breed of dog.", "instruction": "What type of dog breed is sleeping on the couch with its paw on the remote in <Region>[0.081, 0.024, 0.951, 0.82]</Region>?", "input": "", "output": "It is not specified what type of dog breed is sleeping on the couch, but based on the description, it is a small dog and specifically mentioned as a pug, which is a breed of dog.", "is_classification": false, "dataset": "COCO", "img_info": "000000058001.jpg"}
{"img_id": 1697, "query": [["A sandwich on a plate with a side of corn.", "Food wrapped in paper on a plate ", "A foot-long sandwich wrapped in paper on a plate", "A sub sitting on a plate, it appears to be in a restaurant setting.", "A sandwich partially wrapped in wax paper on a plate."], {"cup": [[0.375, 0.0, 0.507, 0.1], [0.738, 0.003, 0.964, 0.314], [0.166, 0.0, 0.312, 0.11]], "fork": [[0.214, 0.105, 0.349, 0.209], [0.245, 0.042, 0.362, 0.128], [0.106, 0.161, 0.239, 0.226]], "sandwich": [[0.489, 0.321, 0.897, 0.784]], "dining table": [[0.0, 0.055, 0.999, 1.0]]}], "response": "Task: What is the position of the fork in <Region>[0.245, 0.042, 0.362, 0.128]</Region> with respect to the food on the plate?\nOutput: The fork in the specified region appears to be on the side of the plate with respect to the sandwich.", "instruction": "What is the position of the fork in <Region>[0.245, 0.042, 0.362, 0.128]</Region> with respect to the food on the plate?", "input": "", "output": "The fork in the specified region appears to be on the side of the plate with respect to the sandwich.", "is_classification": false, "dataset": "COCO", "img_info": "000000001697.jpg"}
{"img_id": 33408, "query": [["a close up of a person holding a cell phone", "Small black cellphone gets help up in a persons hand. ", "A hand is holding a black MP3 player.", "A person holds a Cyon mobile device in their hand.", "A photograph of a small black mp3 player that looks a lot like a car. "], {"cell phone": [[0.207, 0.04, 0.804, 0.912]], "person": [[0.0, 0.35, 0.615, 0.994], [0.465, 0.365, 0.637, 0.57]]}], "response": "Task: What color is the cell phone in <Region>[0.207, 0.04, 0.804, 0.912]</Region>?\nOutput: The given bounding box contains a black cell phone.", "instruction": "What color is the cell phone in <Region>[0.207, 0.04, 0.804, 0.912]</Region>?", "input": "", "output": "The given bounding box contains a black cell phone.", "is_classification": false, "dataset": "COCO", "img_info": "000000033408.jpg"}
{"img_id": 10614, "query": [["A glass platter that has pizza on it.", "A couple of trays with a couple of slices of pizza missing.", "A white plate with a pizza with two slices missing, beside a plate with half a pizza", "Two plates of food are sitting on a table.", "Several partially eaten pizzas on plates sitting on a table."], {"bottle": [[0.169, 0.0, 0.339, 0.339]], "person": [[0.751, 0.041, 0.933, 0.166]], "pizza": [[0.0, 0.321, 0.894, 0.624], [0.577, 0.222, 0.984, 0.396]], "cup": [[0.0, 0.171, 0.105, 0.365], [0.002, 0.065, 0.099, 0.174]], "knife": [[0.862, 0.129, 1.0, 0.189]]}], "response": "Task: What type of drink is in <Region>[0.002, 0.065, 0.099, 0.174]</Region>?\nOutput: The type of drink in the specified region is not determinable as it is not clear from the image.", "instruction": "What type of drink is in <Region>[0.002, 0.065, 0.099, 0.174]</Region>?", "input": "", "output": "The type of drink in the specified region is not determinable as it is not clear from the image.", "is_classification": false, "dataset": "COCO", "img_info": "000000010614.jpg"}
{"img_id": 58619, "query": [["A plate of food and drinks on a table. ", "An orange plate of food includes vegetables and bread.", "A dinner plate loaded with a grilled entre and a beverage.", "A plate of food sits on a table.", "A plate of toasted bread and roasted vegetables."], {"dining table": [[0.0, 0.002, 1.0, 0.998]], "cup": [[0.258, 0.004, 0.443, 0.309]], "fork": [[0.878, 0.279, 0.923, 0.331]], "bowl": [[0.677, 0.319, 0.958, 0.614]], "bottle": [[0.542, 0.002, 0.658, 0.21], [0.431, 0.003, 0.471, 0.223]]}], "response": "Task: What is the position of the cup in <Region>[0.258, 0.004, 0.443, 0.309]</Region>, relative to the position of the bottle?\nOutput: The cup is to the left of the bottle in the specified region.", "instruction": "What is the position of the cup in <Region>[0.258, 0.004, 0.443, 0.309]</Region>, relative to the position of the bottle?", "input": "", "output": "The cup is to the left of the bottle in the specified region.", "is_classification": false, "dataset": "COCO", "img_info": "000000058619.jpg"}
{"img_id": 34223, "query": [["A group of men standing next to each other near two dog toys.", "Four men playing with remote controlled dog toys.", "Four men are watching robotic puppies play soccer", "Several men stand around watching robotic dogs play.", "Microsoft technicians are watching robotic dogs move around on astroturf."], {"chair": [[0.037, 0.323, 0.169, 0.568], [0.177, 0.424, 0.267, 0.513]], "person": [[0.219, 0.0, 0.428, 0.784], [0.462, 0.008, 0.577, 0.763], [0.516, 0.014, 0.738, 0.811], [0.784, 0.016, 0.994, 0.789]], "cup": [[0.867, 0.2, 0.898, 0.279]], "remote": [[0.533, 0.196, 0.549, 0.271], [0.707, 0.135, 0.723, 0.15]], "dining table": [[0.002, 0.358, 0.064, 0.376]], "dog": [[0.16, 0.791, 0.268, 0.943]]}], "response": "Task: What is the number of dogs in the image? And are the two dog toys in <Region>[0.533, 0.196, 0.549, 0.271]</Region> and <Region>[0.707, 0.135, 0.723, 0.15]</Region> remote controlled?\nOutput: There is only one dog in the image. Yes, the two objects in the specified regions are remote controls for dog toys.", "instruction": "What is the number of dogs in the image? And are the two dog toys in <Region>[0.533, 0.196, 0.549, 0.271]</Region> and <Region>[0.707, 0.135, 0.723, 0.15]</Region> remote controlled?", "input": "", "output": "There is only one dog in the image. Yes, the two objects in the specified regions are remote controls for dog toys.", "is_classification": false, "dataset": "COCO", "img_info": "000000034223.jpg"}
{"img_id": 46725, "query": [["A person sitting on a horse walking through sand beach by water with rocky hills with grass in background.", "The woman wearing a helmet smiles riding a horse on the street.", "A women who is riding a horse on a beach.", "A woman is riding a horse in the sand.", "The woman is riding a horse on the beach."], {"horse": [[0.159, 0.51, 0.267, 0.862]], "person": [[0.146, 0.394, 0.296, 0.707], [0.0, 0.419, 0.013, 0.466]]}], "response": "Task: What is happening in the background behind the horse and the person in <Region>[0.159, 0.51, 0.267, 0.862]</Region>?\nOutput: In the background of the person and the horse, there are rocky hills with grass.", "instruction": "What is happening in the background behind the horse and the person in <Region>[0.159, 0.51, 0.267, 0.862]</Region>?", "input": "", "output": "In the background of the person and the horse, there are rocky hills with grass.", "is_classification": false, "dataset": "COCO", "img_info": "000000046725.jpg"}
{"img_id": 45099, "query": [["A woman's reflection of her taking a picture of a sink and toilet.", "A woman is taking a picture of her bathroom. ", "a person taking a photo in a mirror", "A bathroom area has large windows and a counter and a white sink.", "A woman is using a mirror in a bathroom to take a photo"], {"potted plant": [[0.54, 0.225, 0.714, 0.658]], "person": [[0.767, 0.539, 0.891, 1.0]], "bottle": [[0.553, 0.648, 0.576, 0.746]], "wine glass": [[0.376, 0.586, 0.4, 0.662], [0.313, 0.572, 0.338, 0.665]], "toilet": [[0.0, 0.757, 0.183, 0.989]], "sink": [[0.396, 0.73, 0.642, 0.898]], "handbag": [[0.692, 0.77, 0.732, 0.886]], "cup": [[0.429, 0.605, 0.456, 0.664]]}], "response": "Task: What type of plants are in <Region>[0.54, 0.225, 0.714, 0.658]</Region>?\nOutput: The image does not provide adequate information to identify the type of plants.", "instruction": "What type of plants are in <Region>[0.54, 0.225, 0.714, 0.658]</Region>?", "input": "", "output": "The image does not provide adequate information to identify the type of plants.", "is_classification": false, "dataset": "COCO", "img_info": "000000045099.jpg"}
{"img_id": 7304, "query": [["a little in a pink shirt and khaki skirt playing tennis ", "A young girl playing with her friend in a game of tennis.", "A girl is hitting a ball with a tennis racket", "A small girl in action on a court with a racket.", "Young female child playing tennis on outdoor court."], {"sports ball": [[0.694, 0.331, 0.733, 0.364]], "person": [[0.254, 0.273, 0.709, 0.866], [0.422, 0.26, 0.485, 0.375]], "tennis racket": [[0.678, 0.279, 0.889, 0.438]]}], "response": "Task: Which object is in <Region>[0.678, 0.279, 0.889, 0.438]</Region>? \nOutput: The object in the specified region is a tennis racket.", "instruction": "Which object is in <Region>[0.678, 0.279, 0.889, 0.438]</Region>?", "input": "", "output": "The object in the specified region is a tennis racket.", "is_classification": false, "dataset": "COCO", "img_info": "000000007304.jpg"}
{"img_id": 10837, "query": [["Soccor players approach a ball that is in the air.", "Several young male soccer players play on a field as a ball sails overhead.", "a group of people playing soccer in a field", "Soccer players look on as the soccer ball flies in the air.", "Several men playing with a soccer ball in a park."], {"sports ball": [[0.754, 0.071, 0.792, 0.126]], "person": [[0.22, 0.424, 0.343, 0.986], [0.0, 0.446, 0.056, 0.821], [0.858, 0.511, 0.875, 0.557], [0.631, 0.465, 0.805, 0.911], [0.96, 0.72, 1.0, 0.796], [0.183, 0.437, 0.29, 0.922]], "car": [[0.035, 0.525, 0.116, 0.589], [0.107, 0.518, 0.191, 0.578], [0.333, 0.49, 0.428, 0.58], [0.426, 0.51, 0.511, 0.572], [0.503, 0.505, 0.581, 0.579], [0.912, 0.486, 0.995, 0.559], [0.673, 0.498, 0.723, 0.531], [0.749, 0.484, 0.8, 0.551], [0.819, 0.491, 0.914, 0.559], [0.182, 0.519, 0.252, 0.582], [0.481, 0.496, 0.534, 0.531], [0.579, 0.487, 0.656, 0.565], [0.148, 0.505, 0.206, 0.535], [0.03, 0.499, 0.678, 0.546]]}], "response": "Task: Which person is closest to the ball in <Region>[0.754, 0.071, 0.792, 0.126]</Region>?\nOutput: There is no person in the specified region. The ball is probably in mid-air near the soccer players, so it is impossible to tell which person is closest to it.", "instruction": "Which person is closest to the ball in <Region>[0.754, 0.071, 0.792, 0.126]</Region>?", "input": "", "output": "There is no person in the specified region. The ball is probably in mid-air near the soccer players, so it is impossible to tell which person is closest to it.", "is_classification": false, "dataset": "COCO", "img_info": "000000010837.jpg"}
{"img_id": 45524, "query": [["Four people are on this court with a low net.  ", "Four young adults playing a game on a court.", "Adult players on badminton court in public park.", "Four people at an amateur tennis game on a small paved lot.", "Several people on a tennis court with hats on. "], {"person": [[0.457, 0.205, 0.576, 0.65], [0.693, 0.277, 0.732, 0.392], [0.243, 0.265, 0.274, 0.429], [0.002, 0.191, 0.059, 0.745]], "car": [[0.89, 0.283, 0.927, 0.303], [0.862, 0.283, 0.89, 0.298]], "handbag": [[0.245, 0.294, 0.275, 0.356]], "tennis racket": [[0.555, 0.396, 0.563, 0.408]]}], "response": "Task: Is the person holding the tennis racket in <Region>[0.555, 0.396, 0.563, 0.408]</Region> facing towards other people on the court?\nOutput: The image does not contain any other people on the court that the person with the tennis racket could be facing towards.", "instruction": "Is the person holding the tennis racket in <Region>[0.555, 0.396, 0.563, 0.408]</Region> facing towards other people on the court?", "input": "", "output": "The image does not contain any other people on the court that the person with the tennis racket could be facing towards.", "is_classification": false, "dataset": "COCO", "img_info": "000000045524.jpg"}
{"img_id": 69312, "query": [["many tables and chairs under many umbrellas near a building", "Dinning area filled with white tables and chairs. ", "There are a lot of umbrellas, chairs, and tables outside.", "A row of white tables with white umbrellas.", "A lot of white tables and some grass."], {"dining table": [[0.298, 0.845, 0.516, 0.973], [0.238, 0.661, 0.36, 0.717], [0.519, 0.641, 0.621, 0.683], [0.7, 0.76, 0.878, 0.85], [0.262, 0.534, 0.321, 0.549], [0.399, 0.556, 0.472, 0.571], [0.439, 0.588, 0.525, 0.654], [0.243, 0.604, 0.338, 0.631], [0.262, 0.562, 0.321, 0.573], [0.306, 0.505, 0.353, 0.513], [0.407, 0.531, 0.458, 0.558], [0.35, 0.491, 0.366, 0.515], [0.903, 0.52, 0.955, 0.548]], "chair": [[0.426, 0.802, 0.586, 0.999], [0.184, 0.853, 0.326, 1.0], [0.187, 0.664, 0.28, 0.86], [0.348, 0.647, 0.405, 0.834], [0.262, 0.567, 0.303, 0.603], [0.788, 0.827, 0.958, 1.0], [0.793, 0.729, 0.937, 0.927], [0.647, 0.717, 0.761, 0.892], [0.55, 0.667, 0.643, 0.842], [0.448, 0.569, 0.493, 0.592], [0.645, 0.776, 0.77, 1.0], [0.298, 0.775, 0.399, 0.876], [0.214, 0.554, 0.257, 0.663], [0.195, 0.473, 0.68, 0.888]], "umbrella": [[0.167, 0.007, 1.0, 0.983]]}], "response": "Task: How many chairs are under the umbrella in <Region>[0.167, 0.007, 1.0, 0.983]</Region>?\nOutput: It is not clear which exact umbrella is the one being mentioned, but as the region is huge and covers many umbrellas, the number of chairs under the umbrella is unspecific.", "instruction": "How many chairs are under the umbrella in <Region>[0.167, 0.007, 1.0, 0.983]</Region>?", "input": "", "output": "It is not clear which exact umbrella is the one being mentioned, but as the region is huge and covers many umbrellas, the number of chairs under the umbrella is unspecific.", "is_classification": false, "dataset": "COCO", "img_info": "000000069312.jpg"}
{"img_id": 26436, "query": [["High wing, single engine aircraft parked at airfield.", "A train,an aeroplane and a car are seen in the ground.", "A small airplane sits idle near vehicles in a rural area", "A plane sits on the ground by a shop.", "A small plane is sitting in the middle of a field."], {"car": [[0.738, 0.484, 0.832, 0.541], [0.489, 0.493, 0.625, 0.573], [0.907, 0.49, 0.958, 0.53]], "airplane": [[0.142, 0.406, 0.79, 0.649]], "truck": [[0.49, 0.494, 0.62, 0.575]]}], "response": "Task: What kind of vehicle is the airplane in <Region>[0.142, 0.406, 0.79, 0.649]</Region>?\nOutput: The airplane in the specified region is a high wing, single engine aircraft.", "instruction": "What kind of vehicle is the airplane in <Region>[0.142, 0.406, 0.79, 0.649]</Region>?", "input": "", "output": "The airplane in the specified region is a high wing, single engine aircraft.", "is_classification": false, "dataset": "COCO", "img_info": "000000026436.jpg"}
{"img_id": 74590, "query": [["A baby brushing it's teeth with a green toothbrush.", "A baby has a green toothbrush in its tiny mouth.", "A baby putting a toothbrush in to its mouth", "A baby in an adult's arms is gnawing on a toothbrush.", "A baby brushing his first teeth for the camera."], {"toothbrush": [[0.444, 0.577, 0.512, 0.653], [0.481, 0.758, 0.596, 0.996]], "person": [[0.162, 0.497, 1.0, 0.993], [0.0, 0.2, 0.993, 1.0]]}], "response": "Task: What color is the toothbrush in <Region>[0.444, 0.577, 0.512, 0.653]</Region>?\nOutput: The toothbrush in the specified region is green.", "instruction": "What color is the toothbrush in <Region>[0.444, 0.577, 0.512, 0.653]</Region>?", "input": "", "output": "The toothbrush in the specified region is green.", "is_classification": false, "dataset": "COCO", "img_info": "000000074590.jpg"}
{"img_id": 6935, "query": [["A group of people pulling bags behind them ", "Travelers with bags, backpacks and luggage walking down a street.", "Street scene of an adult and two  children walking down sidewalk with luggage.", "Group of people pulling different pieces of luggage down a city street. ", "kids and a man walking down the sidewalk with suitcases\n"], {"car": [[0.151, 0.161, 0.396, 0.317], [0.153, 0.231, 0.38, 0.448], [0.315, 0.188, 0.42, 0.262]], "backpack": [[0.452, 0.207, 0.531, 0.372], [0.573, 0.415, 0.703, 0.633]], "handbag": [[0.093, 0.369, 0.296, 0.758]], "suitcase": [[0.511, 0.678, 0.656, 0.998], [0.373, 0.551, 0.47, 0.777], [0.241, 0.683, 0.354, 0.989]], "person": [[0.0, 0.148, 0.283, 1.0], [0.552, 0.326, 0.711, 0.885], [0.236, 0.408, 0.334, 0.809], [0.244, 0.159, 0.36, 0.555], [0.364, 0.207, 0.436, 0.546], [0.578, 0.185, 0.624, 0.35], [0.508, 0.167, 0.573, 0.455], [0.415, 0.132, 0.57, 0.734], [0.641, 0.129, 0.691, 0.396], [0.548, 0.186, 0.586, 0.384], [0.538, 0.168, 0.558, 0.225], [0.57, 0.16, 0.594, 0.263], [0.584, 0.16, 0.659, 0.357]], "truck": [[0.0, 0.156, 0.044, 0.207], [0.239, 0.135, 0.467, 0.222]], "motorcycle": [[0.0, 0.257, 0.254, 0.428]]}], "response": "Task: Is the person in <Region>[0.244, 0.159, 0.36, 0.555]</Region> walking backwards or forwards?\nOutput: The person in the specified region is walking forwards.", "instruction": "Is the person in <Region>[0.244, 0.159, 0.36, 0.555]</Region> walking backwards or forwards?", "input": "", "output": "The person in the specified region is walking forwards.", "is_classification": false, "dataset": "COCO", "img_info": "000000006935.jpg"}
{"img_id": 74942, "query": [["A thick sandwich on a white plate, a bowl of pickles and a glass of water.", "A sandwich on a plate sitting next to an empty glass.", "a table with a pastrami sandwich and a plate of pickles", "A plate with different foods on it on a table", "A thick sandwich is on a white plate near a plate of pickles."], {"dining table": [[0.002, 0.003, 0.999, 0.988]], "cup": [[0.002, 0.227, 0.222, 0.957], [0.0, 0.0, 0.068, 0.245], [0.633, 0.002, 0.75, 0.126]], "fork": [[0.897, 0.353, 1.0, 0.436]], "bowl": [[0.76, 0.323, 0.996, 0.547], [0.3, 0.335, 0.821, 0.804], [0.292, 0.056, 0.604, 0.275], [0.173, 0.0, 0.333, 0.14], [0.389, 0.0, 0.555, 0.088]], "carrot": [[0.422, 0.11, 0.504, 0.169]], "sandwich": [[0.303, 0.384, 0.796, 0.843], [0.459, 0.247, 0.818, 0.608]]}], "response": "Task: What is the position of the glass of water in <Region>[0.002, 0.003, 0.999, 0.988]</Region> in relation to the sandwich and the bowl of pickles?\nOutput: The glass of water is placed on the left side of the table in the specified region, while the sandwich and the bowl of pickles are not located in this region.", "instruction": "What is the position of the glass of water in <Region>[0.002, 0.003, 0.999, 0.988]</Region> in relation to the sandwich and the bowl of pickles?", "input": "", "output": "The glass of water is placed on the left side of the table in the specified region, while the sandwich and the bowl of pickles are not located in this region.", "is_classification": false, "dataset": "COCO", "img_info": "000000074942.jpg"}
{"img_id": 15097, "query": [["A group of people riding skis while holding ski poles.", "People standing and skiing on a ski slope. ", "People ski down a large hill in the snow.", "Several people gather at the bottom of a ski slope.", "Four people with skis standing at the bottom of a ski lift."], {"person": [[0.207, 0.478, 0.262, 0.565], [0.959, 0.707, 0.999, 0.836], [0.529, 0.596, 0.618, 0.818], [0.683, 0.635, 0.767, 0.878], [0.353, 0.671, 0.489, 0.87], [0.217, 0.603, 0.322, 0.85]], "skis": [[0.65, 0.844, 0.803, 0.888], [0.395, 0.856, 0.448, 0.871], [0.481, 0.787, 0.716, 0.831], [0.241, 0.843, 0.295, 0.881], [0.89, 0.807, 0.976, 0.832], [0.225, 0.822, 0.267, 0.853]]}], "response": "Task: How many people are in <Region>[0.353, 0.671, 0.489, 0.87]</Region> and what is their possible position on the ski slope?\nOutput: There appears to be one person skiing, possibly in a slalom pattern given their form and the curvature of the snow.", "instruction": "How many people are in <Region>[0.353, 0.671, 0.489, 0.87]</Region> and what is their possible position on the ski slope?", "input": "", "output": "There appears to be one person skiing, possibly in a slalom pattern given their form and the curvature of the snow.", "is_classification": false, "dataset": "COCO", "img_info": "000000015097.jpg"}
{"img_id": 47563, "query": [["One cat sits by the umbrella and the other sits.under it.", "Two cats are outside with one under the umbrella", "Two cats in a drive one sitting while the other is sitting under a umbrella.", "A cat sits on the driveway next to a cat under an umbrella.", "a cat sitting under an umbrella in a driveway "], {"potted plant": [[0.688, 0.435, 0.991, 0.982]], "cat": [[0.528, 0.691, 0.608, 0.813], [0.296, 0.683, 0.412, 0.822]], "bottle": [[0.146, 0.539, 0.166, 0.634]], "car": [[0.341, 0.101, 0.696, 0.581]], "umbrella": [[0.469, 0.484, 0.71, 0.828]], "handbag": [[0.197, 0.622, 0.249, 0.728]]}], "response": "Task: Which cat is under the umbrella and which one is sitting next to it in <Region>[0.296, 0.683, 0.412, 0.822]</Region>?\nOutput: The cat in the specified region is the one sitting under the umbrella, and the other cat is sitting next to it.", "instruction": "Which cat is under the umbrella and which one is sitting next to it in <Region>[0.296, 0.683, 0.412, 0.822]</Region>?", "input": "", "output": "The cat in the specified region is the one sitting under the umbrella, and the other cat is sitting next to it.", "is_classification": false, "dataset": "COCO", "img_info": "000000047563.jpg"}
{"img_id": 69523, "query": [["A white sink and window in a small room.", "A small white bathroom with white tiles and a window.", "An empty clean bathroom with  sink and large window.", "A cramped dimly lit bathroom with the window open.", "A bathroom with a white sink sitting next to a toilet."], {"toilet": [[0.718, 0.643, 0.816, 0.806]], "sink": [[0.225, 0.523, 0.491, 0.621]], "bottle": [[0.368, 0.401, 0.427, 0.494]], "toothbrush": [[0.486, 0.407, 0.496, 0.434], [0.482, 0.407, 0.498, 0.483], [0.22, 0.509, 0.234, 0.544]]}], "response": "Task: What personal hygiene item is visible in the image and next to the sink in <Region>[0.225, 0.523, 0.491, 0.621]</Region>?\nOutput: There is a toothbrush located next to the sink in the specified region.", "instruction": "What personal hygiene item is visible in the image and next to the sink in <Region>[0.225, 0.523, 0.491, 0.621]</Region>?", "input": "", "output": "There is a toothbrush located next to the sink in the specified region.", "is_classification": false, "dataset": "COCO", "img_info": "000000069523.jpg"}
{"img_id": 62581, "query": [["A bus frame sitting in a gravel lot with dogs in background.", "The purple bus has no wheels on it.", "A purple and blue bus with no wheels. ", "The broken down bus is missing it's tires. ", "A colorful tour bus has been laid to rest without wheels."], {"dog": [[0.892, 0.459, 0.935, 0.507], [0.942, 0.455, 0.977, 0.499]], "bus": [[0.062, 0.099, 0.881, 0.904]]}], "response": "Task: What is the present condition of the bus in <Region>[0.062, 0.099, 0.881, 0.904]</Region>?\nOutput: From the given information, it can be deduced that the bus is not presently in a working condition since it has no wheels on it.", "instruction": "What is the present condition of the bus in <Region>[0.062, 0.099, 0.881, 0.904]</Region>?", "input": "", "output": "From the given information, it can be deduced that the bus is not presently in a working condition since it has no wheels on it.", "is_classification": false, "dataset": "COCO", "img_info": "000000062581.jpg"}
{"img_id": 21060, "query": [["The computer desk has a small pad of notes, and an mp3 player on it.", "A computer monitor sitting on top of a desk.", "A computer monitor sitting on top of a wooden desk.", "A desk holds a computer, speakers, and sticky notes.", "a computer that is on a wooden desk"], {"tv": [[0.364, 0.144, 0.713, 0.535]], "mouse": [[0.778, 0.596, 0.84, 0.636], [0.738, 0.937, 0.82, 1.0]], "keyboard": [[0.192, 0.861, 0.694, 0.984]], "cup": [[0.159, 0.499, 0.212, 0.592], [0.086, 0.463, 0.151, 0.595]]}], "response": "Task: What can be a probable use for the small pad of notes positioned on the computer desk in <Region>[0.192, 0.861, 0.694, 0.984]</Region>?\nOutput: The small pad of notes is positioned near the keyboard and mouse; therefore, it might be used to note important information or tasks related to computer use.", "instruction": "What can be a probable use for the small pad of notes positioned on the computer desk in <Region>[0.192, 0.861, 0.694, 0.984]</Region>?", "input": "", "output": "The small pad of notes is positioned near the keyboard and mouse; therefore, it might be used to note important information or tasks related to computer use.", "is_classification": false, "dataset": "COCO", "img_info": "000000021060.jpg"}
{"img_id": 65208, "query": [["A man walking while holding an umbrella on a wet sidewalk.", "A gentleman with an umbrella in the rain", "A man walking on a city street holding an open umbrella on a rainy day ", "A man walking down a sidewalk by a traffic light", "A mean about to cross a crosswalk on a yellow light"], {"traffic light": [[0.839, 0.131, 0.896, 0.247], [0.641, 0.363, 0.672, 0.413], [0.79, 0.464, 0.8, 0.476], [0.689, 0.367, 0.717, 0.418]], "fire hydrant": [[0.896, 0.576, 0.956, 0.665]], "person": [[0.387, 0.44, 0.532, 0.719], [0.543, 0.484, 0.586, 0.574], [0.523, 0.5, 0.536, 0.536], [0.539, 0.496, 0.555, 0.546], [0.574, 0.49, 0.596, 0.55]], "umbrella": [[0.419, 0.422, 0.604, 0.481], [0.517, 0.488, 0.557, 0.496], [0.544, 0.466, 0.618, 0.501]]}], "response": "Task: What is the man in <Region>[0.419, 0.422, 0.604, 0.481]</Region> wearing on his feet?\nOutput: The image does not provide enough information about what the man is wearing on his feet.", "instruction": "What is the man in <Region>[0.419, 0.422, 0.604, 0.481]</Region> wearing on his feet?", "input": "", "output": "The image does not provide enough information about what the man is wearing on his feet.", "is_classification": false, "dataset": "COCO", "img_info": "000000065208.jpg"}
{"img_id": 71399, "query": [["Shredded carrots and herbs in a single-serving package. ", "A tray full of carrots with some herbs sprinkled on top.", "A bowl that is filled up with carrots.", "A plastic container filled with sliced carrots next to a yellow object.", "a plastic tray with carrots slices on a black table"], {"spoon": [[0.004, 0.746, 0.681, 0.989]], "bowl": [[0.061, 0.054, 0.804, 0.793], [0.821, 0.585, 1.0, 0.869]], "orange": [[0.002, 0.914, 0.206, 1.0]], "carrot": [[0.136, 0.151, 0.744, 0.729]], "dining table": [[0.0, 0.005, 1.0, 0.985]]}], "response": "Task: Are the carrots in <Region>[0.136, 0.151, 0.744, 0.729]</Region> sliced or shredded?\nOutput: The carrots in the specified region appear to be shredded.", "instruction": "Are the carrots in <Region>[0.136, 0.151, 0.744, 0.729]</Region> sliced or shredded?", "input": "", "output": "The carrots in the specified region appear to be shredded.", "is_classification": false, "dataset": "COCO", "img_info": "000000071399.jpg"}
{"img_id": 35589, "query": [["A baseball player holding a bat on top of a baseball field.", "A pitch that has just been thrown at a baseball game.", "a baseball player is up to bat at a game", "Batter preparing to swing at ball during major baseball game.", "A pitch is in mid air in an MLB baseball game."], {"sports ball": [[0.567, 0.527, 0.575, 0.537]], "person": [[0.638, 0.479, 0.741, 0.747], [0.835, 0.241, 0.897, 0.368], [0.092, 0.358, 0.206, 0.519], [0.589, 0.143, 0.625, 0.214], [0.604, 0.921, 0.664, 0.999], [0.266, 0.871, 0.318, 0.97], [0.499, 0.923, 0.547, 1.0], [0.054, 0.879, 0.106, 0.953], [0.141, 0.91, 0.181, 0.966], [0.182, 0.886, 0.223, 0.958], [0.219, 0.908, 0.247, 0.951], [0.812, 0.55, 0.928, 0.786], [0.0, 0.705, 0.998, 0.998]], "baseball bat": [[0.678, 0.484, 0.722, 0.549]], "baseball glove": [[0.835, 0.292, 0.847, 0.308], [0.611, 0.178, 0.619, 0.191], [0.791, 0.613, 0.815, 0.644], [0.097, 0.404, 0.109, 0.423]]}], "response": "Task: Based on the position of the baseball bat in <Region>[0.678, 0.484, 0.722, 0.549]</Region>, is the person holding the bat right-handed or left-handed?\nOutput: The person holding the bat is most likely right-handed because the bat is placed on the right shoulder.", "instruction": "Based on the position of the baseball bat in <Region>[0.678, 0.484, 0.722, 0.549]</Region>, is the person holding the bat right-handed or left-handed?", "input": "", "output": "The person holding the bat is most likely right-handed because the bat is placed on the right shoulder.", "is_classification": false, "dataset": "COCO", "img_info": "000000035589.jpg"}
{"img_id": 18564, "query": [["A man with a camera walking past a construction truck.", "a man walking down a sidewalk next to a building.", "Someone is carrying equipment by the construction zone.", "A view of a man carrying a camera, next to a truck.", "A photographer with his nice camera walking in a dirty road"], {"truck": [[0.54, 0.009, 1.0, 0.543]], "person": [[0.383, 0.729, 0.659, 1.0]], "bottle": [[0.658, 0.112, 0.719, 0.156], [0.965, 0.068, 1.0, 0.098]]}], "response": "Task: What is on the truck in <Region>[0.54, 0.009, 1.0, 0.543]</Region>?\nOutput: Only a part of the truck is visible in the picture, but a green construction cone and some other signs or equipment can be seen on the bed of the truck.", "instruction": "What is on the truck in <Region>[0.54, 0.009, 1.0, 0.543]</Region>?", "input": "", "output": "nly a part of the truck is visible in the picture, but a green construction cone and some other signs or equipment can be seen on the bed of the truck.", "is_classification": false, "dataset": "COCO", "img_info": "000000018564.jpg"}
{"img_id": 56233, "query": [["A skateboarder doing tricks on his board on the sidewalk.", "A man performing a trick on his skateboard.", "A boy attempts to successfully land on his skateboard", "A skateboarder in mid air performing a trick.", "A man doing a back flip while riding a skateboard."], {"person": [[0.27, 0.171, 0.818, 0.707], [0.536, 0.602, 0.581, 0.691], [0.476, 0.567, 0.508, 0.611], [0.831, 0.642, 0.848, 0.66], [0.846, 0.646, 0.873, 0.671], [0.967, 0.644, 0.986, 0.678]], "skateboard": [[0.283, 0.581, 0.605, 0.824]]}], "response": "Task: What is the skateboarder in <Region>[0.283, 0.581, 0.605, 0.824]</Region> doing to their skateboard?\nOutput: The skateboarder in the specified region is performing a trick, although it is not possible to know the exact name of the trick performed from the information provided by the image.", "instruction": "What is the skateboarder in <Region>[0.283, 0.581, 0.605, 0.824]</Region> doing to their skateboard?", "input": "", "output": "The skateboarder in the specified region is performing a trick, although it is not possible to know the exact name of the trick performed from the information provided by the image.", "is_classification": false, "dataset": "COCO", "img_info": "000000056233.jpg"}
{"img_id": 34842, "query": [["A wooden table topped with a laptop computer and a desktop.", "a desk and a chair with various items on the desk", "A monitor sitting next to a keyboard, headphones and another monitor.", "DESK AND CHAIR IN FRONT OF TWO LARGE WINDOWS", "A empty chair sitting in front of two computers"], {"chair": [[0.537, 0.411, 0.996, 0.936]], "laptop": [[0.012, 0.326, 0.352, 0.517], [0.318, 0.235, 0.546, 0.383]], "keyboard": [[0.376, 0.434, 0.589, 0.479], [0.208, 0.471, 0.345, 0.51], [0.343, 0.333, 0.536, 0.375]], "cup": [[0.218, 0.42, 0.279, 0.473]], "mouse": [[0.61, 0.438, 0.652, 0.455]]}], "response": "Task: What objects are on the wooden table in <Region>[0.537, 0.411, 0.996, 0.936]</Region>?\nOutput: A laptop computer and a desktop are on the wooden table in the specified region.", "instruction": "What objects are on the wooden table in <Region>[0.537, 0.411, 0.996, 0.936]</Region>?", "input": "", "output": "A laptop computer and a desktop are on the wooden table in the specified region.", "is_classification": false, "dataset": "COCO", "img_info": "000000034842.jpg"}
{"img_id": 2346, "query": [["A penny behind three tiny flower vases on a wooden table.", "Green and white flowers sitting next to a penny. ", "A table with a penny and some flowers on it", "A standing penny is adorned by many miniature potted flowers.", "A table with a few vases on it with flowers."], {"vase": [[0.428, 0.481, 0.529, 0.69], [0.206, 0.452, 0.334, 0.693], [0.677, 0.505, 0.747, 0.689], [0.664, 0.497, 0.752, 0.662]], "dining table": [[0.002, 0.434, 0.999, 0.989]]}], "response": "Task: How many flowers are there in total between the vases in <Region>[0.428, 0.481, 0.529, 0.69]</Region>, <Region>[0.206, 0.452, 0.334, 0.693]</Region>, and <Region>[0.677, 0.505, 0.747, 0.689]</Region>?\nOutput: Based on the information provided by the region boxes, it is not possible to count the number of flowers between the vases.", "instruction": "How many flowers are there in total between the vases in <Region>[0.428, 0.481, 0.529, 0.69]</Region>, <Region>[0.206, 0.452, 0.334, 0.693]</Region>, and <Region>[0.677, 0.505, 0.747, 0.689]</Region>?", "input": "", "output": "Based on the information provided by the region boxes, it is not possible to count the number of flowers between the vases.", "is_classification": false, "dataset": "COCO", "img_info": "000000002346.jpg"}
{"img_id": 6860, "query": [["A young man holding a tennis racquet in front of a crowd.", "Close-up of a man on a tennis court preparing to strike an approaching ball.", "A  man is standing in front of a chain link fence holding up a tennis racker.", "A tennis player lines up his racket to hit the ball.", "Tennis player holding his racket looking ahead of him. "], {"person": [[0.313, 0.382, 0.811, 1.0], [0.298, 0.641, 0.413, 0.961], [0.816, 0.785, 0.889, 1.0], [0.761, 0.812, 0.804, 0.935]], "tennis racket": [[0.603, 0.424, 0.975, 0.94]]}], "response": "Task: Is the tennis player in <Region>[0.313, 0.382, 0.811, 1.0]</Region> facing towards the crowd or looking in the other direction?\nOutput: The tennis player is not facing the crowd, he is looking in the opposite direction.", "instruction": "Is the tennis player in <Region>[0.313, 0.382, 0.811, 1.0]</Region> facing towards the crowd or looking in the other direction?", "input": "", "output": "The tennis player is not facing the crowd, he is looking in the opposite direction.", "is_classification": false, "dataset": "COCO", "img_info": "000000006860.jpg"}
{"img_id": 51887, "query": [["A pole with a sign explaining the traffic signs. ", "the cross walk buttons for pedestrians to use", "Cross walk sign indicating when to cross the street. ", "a white crossing sign above a yellow button", "A bunch of street signs hanging from a pole."], {"car": [[0.915, 0.182, 0.997, 0.24], [0.001, 0.222, 0.073, 0.273], [0.045, 0.216, 0.077, 0.257]]}], "response": "Task: Are there any cars in the same image area as the street sign in <Region>[0.915, 0.182, 0.997, 0.24]</Region>?\nOutput: Yes, there is a car in the same image area as the street sign.", "instruction": "Are there any cars in the same image area as the street sign in <Region>[0.915, 0.182, 0.997, 0.24]</Region>?", "input": "", "output": "Yes, there is a car in the same image area as the street sign.", "is_classification": false, "dataset": "COCO", "img_info": "000000051887.jpg"}
{"img_id": 59063, "query": [["A large white and orange passenger jet sitting on a runway.", "A jet airplane is in an airplane lot and not moving.", "jet parked at the gate at the airport", "A large airplane on the tarmac near an airport.  ", "a passenger airplane at an airport with a van behind it"], {"airplane": [[0.008, 0.247, 0.907, 0.735]], "truck": [[0.402, 0.657, 0.535, 0.757], [0.932, 0.657, 0.999, 0.72], [0.001, 0.626, 0.12, 0.723]]}], "response": "Task: What is the position of the truck in <Region>[0.932, 0.657, 0.999, 0.72]</Region> with respect to the airplane?\nOutput: The truck is positioned at the back right-hand side of the airplane relative to the viewer's perspective.", "instruction": "What is the position of the truck in <Region>[0.932, 0.657, 0.999, 0.72]</Region> with respect to the airplane?", "input": "", "output": "The truck is positioned at the back right-hand side of the airplane relative to the viewer's perspective.", "is_classification": false, "dataset": "COCO", "img_info": "000000059063.jpg"}
{"img_id": 58577, "query": [["A person riding a wave on top of a surfboard.", "A man on a surf board near the crest of a wave.", "A person on a surf board, riding a wave.", "A surfer riding a very big wave near some rocks.", "A man is riding his surfboard on a high water wave."], {"person": [[0.295, 0.222, 0.403, 0.353], [0.715, 0.368, 0.791, 0.442]], "surfboard": [[0.322, 0.306, 0.446, 0.355]]}], "response": "Task: How is the person in <Region>[0.295, 0.222, 0.403, 0.353]</Region> positioned on the surfboard they're riding?\nOutput: The person on the surfboard is positioned in a standing posture, which is perpendicular to the length of the surfboard.", "instruction": "How is the person in <Region>[0.295, 0.222, 0.403, 0.353]</Region> positioned on the surfboard they're riding?", "input": "", "output": "The person on the surfboard is positioned in a standing posture, which is perpendicular to the length of the surfboard.", "is_classification": false, "dataset": "COCO", "img_info": "000000058577.jpg"}
{"img_id": 52843, "query": [["A woman playing tennis is bending down to hit the ball.", "a split screen image showing a woman playing tennis.", "A side by side shot of a tennis player swinging a raquet at a ball.", "A female tennis player positions herself to hit a backhand.", "A woman hitting a tennis ball with a racquet."], {"sports ball": [[0.642, 0.448, 0.674, 0.494], [0.148, 0.45, 0.18, 0.496]], "person": [[0.05, 0.223, 0.365, 0.876], [0.54, 0.213, 0.86, 0.872]], "tennis racket": [[0.36, 0.353, 0.486, 0.459], [0.816, 0.348, 0.985, 0.473]]}], "response": "Task: In which hand is the tennis racket that the woman is holding in <Region>[0.816, 0.348, 0.985, 0.473]</Region>?\nOutput: According to the position of the arm and the racket, the woman is holding the racket in her right hand.", "instruction": "In which hand is the tennis racket that the woman is holding in <Region>[0.816, 0.348, 0.985, 0.473]</Region>?", "input": "", "output": "According to the position of the arm and the racket, the woman is holding the racket in her right hand.", "is_classification": false, "dataset": "COCO", "img_info": "000000052843.jpg"}
{"img_id": 41839, "query": [["A bathroom with a walk in shower next to a toilet and a sink.", "A very nice and modern bathroom sports a black and white color scheme.", "A clean bathroom has a square shaped sink and toilet. ", "A white bathroom with two towels laid out.", "The bathroom features upgraded and high cost finishes."], {"toilet": [[0.678, 0.685, 0.923, 0.909]], "sink": [[0.556, 0.815, 0.86, 1.0], [0.368, 0.527, 0.58, 0.563], [0.134, 0.529, 0.27, 0.633]]}], "response": "Task: What is the position and layout of the sink in the <Region>[0.556, 0.815, 0.86, 1.0]</Region> group?\nOutput: The sink in the specified location is rectangular in shape. It is located in the upper right corner of the room and seems to have double faucets.", "instruction": "What is the position and layout of the sink in the <Region>[0.556, 0.815, 0.86, 1.0]</Region> group?", "input": "", "output": "The sink in the specified location is rectangular in shape. It is located in the upper right corner of the room and seems to have double faucets.", "is_classification": false, "dataset": "COCO", "img_info": "000000041839.jpg"}
{"img_id": 33743, "query": [["A group of people standing by baggage claim in airport.", "People are waiting for their luggage in the baggage claim of an airport.", "Small crowd waiting for luggage at an airport.", "An airport baggage claim area with tired looking passengers. ", "Many people are scattered together at the air port. "], {"person": [[0.528, 0.138, 0.673, 0.917], [0.681, 0.263, 0.93, 0.989], [0.731, 0.151, 0.91, 0.412], [0.363, 0.162, 0.449, 0.518], [0.45, 0.141, 0.52, 0.526], [0.262, 0.145, 0.363, 0.486], [0.352, 0.165, 0.398, 0.292], [0.184, 0.152, 0.257, 0.467], [0.05, 0.119, 0.148, 0.506], [0.908, 0.155, 1.0, 0.584], [0.705, 0.166, 0.781, 0.367], [0.434, 0.157, 0.487, 0.281], [0.46, 0.182, 0.576, 0.657], [0.026, 0.133, 0.356, 0.486]], "backpack": [[0.16, 0.196, 0.206, 0.295]], "handbag": [[0.646, 0.524, 0.766, 0.707], [0.269, 0.187, 0.339, 0.35], [0.39, 0.214, 0.45, 0.332]], "suitcase": [[0.1, 0.411, 0.19, 0.54]]}], "response": "Task: Which person is the closest to the handbag in <Region>[0.269, 0.187, 0.339, 0.35]</Region> and what color is the handbag?\nOutput: The person closest to the handbag is not explicitly determinable, but the second person from the left in the third row in the image there is towards that direction. The color of the handbag is black.", "instruction": "Which person is the closest to the handbag in <Region>[0.269, 0.187, 0.339, 0.35]</Region> and what color is the handbag?", "input": "", "output": "The person closest to the handbag is not explicitly determinable, but the second person from the left in the third row in the image there is towards that direction. The color of the handbag is black.", "is_classification": false, "dataset": "COCO", "img_info": "000000033743.jpg"}
{"img_id": 72156, "query": [["The skateboarder in a tan cap balances on his board.", "A man on a skateboard with the front of the board in the air.", "The skateboarder is skating in the cement ground.  ", "A pair of skateboards skateboarding in a parking lot.", "A man on a skateboard that is in the street."], {"bicycle": [[0.854, 0.335, 0.957, 0.448], [0.963, 0.363, 1.0, 0.467]], "skateboard": [[0.152, 0.823, 0.375, 0.963], [0.809, 0.511, 0.847, 0.54]], "backpack": [[0.877, 0.318, 0.889, 0.363]], "truck": [[0.727, 0.173, 1.0, 0.404]], "person": [[0.064, 0.117, 0.554, 0.924], [0.616, 0.252, 0.654, 0.391], [0.662, 0.269, 0.701, 0.359], [0.708, 0.268, 0.736, 0.358], [0.741, 0.262, 0.761, 0.314], [0.801, 0.284, 0.86, 0.521], [0.846, 0.272, 0.89, 0.439], [0.943, 0.279, 1.0, 0.431], [0.977, 0.273, 1.0, 0.415], [0.0, 0.282, 0.018, 0.45]], "potted plant": [[0.544, 0.279, 0.611, 0.381], [0.458, 0.266, 0.525, 0.385]]}], "response": "Task: What type of headwear is the skateboarder in <Region>[0.152, 0.823, 0.375, 0.963]</Region> wearing?\nOutput: The skateboarder in the specified region is wearing a tan cap.", "instruction": "What type of headwear is the skateboarder in <Region>[0.152, 0.823, 0.375, 0.963]</Region> wearing?", "input": "", "output": "The skateboarder in the specified region is wearing a tan cap.", "is_classification": false, "dataset": "COCO", "img_info": "000000072156.jpg"}
{"img_id": 43073, "query": [["A young boy wearing a blue shirt standing next to a woman.", "An adult using a hair dryer on a child", "Woman using a blow dryer on a distracted child.", "a little girl getting her hair dry with a blow dryer", "a child with long wet hair has his hair blow dried"], {"person": [[0.336, 0.19, 0.833, 0.989], [0.005, 0.025, 0.382, 0.651]], "hair drier": [[0.522, 0.016, 0.683, 0.236]]}], "response": "Task: What is the color of the shirt worn by the boy in <Region>[0.336, 0.19, 0.833, 0.989]</Region>?\nOutput: The boy in the specified region is wearing a blue shirt.", "instruction": "What is the color of the shirt worn by the boy in <Region>[0.336, 0.19, 0.833, 0.989]</Region>?", "input": "", "output": "The boy in the specified region is wearing a blue shirt.", "is_classification": false, "dataset": "COCO", "img_info": "000000043073.jpg"}
{"img_id": 59220, "query": [["A surfer is surfing the waves in the ocean. ", "A surfer crouched low over his board on a wave.", "A man who is on a surfboard riding a wave.", "A surfer squats on his board on top of a wave.", "a person riding a surf board on a wave "], {"person": [[0.414, 0.418, 0.507, 0.59], [0.86, 0.687, 1.0, 1.0]], "surfboard": [[0.423, 0.559, 0.515, 0.587]]}], "response": "Task: What is the surfer in <Region>[0.414, 0.418, 0.507, 0.59]</Region> doing on the surfboard?\nOutput: The surfer in the specified region is riding the wave while standing on the surfboard.", "instruction": "What is the surfer in <Region>[0.414, 0.418, 0.507, 0.59]</Region> doing on the surfboard?", "input": "", "output": "The surfer in the specified region is riding the wave while standing on the surfboard.", "is_classification": false, "dataset": "COCO", "img_info": "000000059220.jpg"}
{"img_id": 30471, "query": [["a pole that has a few signs on it", "A stop sign on a utility pole at an intersection. ", "A red stop sign that says all way stop and no parking", "A top sign on top of a no parking sign at a corner. ", "A Stop sign and no parking sign on a wooden pole."], {"stop sign": [[0.395, 0.098, 0.888, 0.436]], "car": [[0.375, 0.809, 0.53, 0.896], [0.854, 0.821, 0.917, 0.853], [0.725, 0.81, 0.826, 0.863], [0.724, 0.821, 0.773, 0.867], [0.759, 0.805, 0.855, 0.859], [0.941, 0.813, 0.987, 0.829], [0.517, 0.826, 0.568, 0.881], [0.937, 0.821, 1.0, 0.837], [0.853, 0.827, 0.876, 0.856], [0.856, 0.838, 1.0, 0.97], [0.917, 0.818, 0.944, 0.844]], "truck": [[0.164, 0.809, 0.429, 0.912]]}], "response": "Task: What other sign is on the pole with the stop sign in <Region>[0.395, 0.098, 0.888, 0.436]</Region>?\nOutput: According to the given description, there is a no parking sign along with the stop sign on the pole.", "instruction": "What other sign is on the pole with the stop sign in <Region>[0.395, 0.098, 0.888, 0.436]</Region>?", "input": "", "output": "According to the given description, there is a no parking sign along with the stop sign on the pole.", "is_classification": false, "dataset": "COCO", "img_info": "000000030471.jpg"}
{"img_id": 33377, "query": [["two people with two dogs on a surf board and one dog swimming ", "A man and woman are holding a surfboard and two dogs are standing on the surfboard. ", "A woman and a man holding a yellow surfboard with two dogs on it.", "A man and woman are balancing their two pug dogs on a surfboard. ", "The man and woman are helping the dog on the surfboard."], {"dog": [[0.0, 0.549, 0.088, 0.658], [0.331, 0.398, 0.443, 0.549], [0.268, 0.352, 0.34, 0.551], [0.38, 0.036, 0.422, 0.128], [0.342, 0.097, 0.371, 0.148], [0.321, 0.092, 0.339, 0.132]], "person": [[0.12, 0.253, 0.31, 0.575], [0.522, 0.231, 0.716, 0.53], [0.0, 0.091, 0.054, 0.17], [0.121, 0.0, 0.15, 0.036], [0.463, 0.104, 0.498, 0.151]], "surfboard": [[0.411, 0.48, 0.726, 0.555]]}], "response": "Task: What is happening with the third dog located in <Region>[0.268, 0.352, 0.34, 0.551]</Region>?\nOutput: It is jumping off the surfboard and swimming in the water.", "instruction": "What is happening with the third dog located in <Region>[0.268, 0.352, 0.34, 0.551]</Region>?", "input": "", "output": "It is jumping off the surfboard and swimming in the water.", "is_classification": false, "dataset": "COCO", "img_info": "000000033377.jpg"}
{"img_id": 21641, "query": [["A little boy sitting on the floor in his pajamas.", "A toddler in pajamas and socks cleaning his teeth", "A very young child wearing pajamas is on the tile floor.", "Person on a tiled floor eating something with back of legs of someone else visible behind them.", "A toddler with blue boots is sitting on the floor"], {"toothbrush": [[0.495, 0.422, 0.595, 0.493]], "person": [[0.057, 0.05, 0.733, 0.998], [0.431, 0.004, 0.724, 0.438]]}], "response": "Task: What is the boy in <Region>[0.057, 0.05, 0.733, 0.998]</Region> doing?\nOutput: It is not clear what the boy is doing from the given information, as his activity is not specified in any of the descriptions and he could be doing anything.", "instruction": "What is the boy in <Region>[0.057, 0.05, 0.733, 0.998]</Region> doing?", "input": "", "output": "It is not clear what the boy is doing from the given information, as his activity is not specified in any of the descriptions and he could be doing anything.", "is_classification": false, "dataset": "COCO", "img_info": "000000021641.jpg"}
{"img_id": 40886, "query": [["A pitcher winding up to throw a pitch in a San Francisco uniform.", "This ballplayer is putting his whole body into that pitch.", "A young man pitching a baseball while standing on a  field.", "A professional baseball pitcher pulls his arm back to throw the ball", "An MLB pitcher for San Francisco is pitching."], {"sports ball": [[0.561, 0.125, 0.579, 0.151]], "person": [[0.721, 0.244, 0.775, 0.33], [0.333, 0.114, 0.784, 0.881], [0.316, 0.243, 0.349, 0.333], [0.615, 0.25, 0.676, 0.333], [0.933, 0.247, 0.991, 0.33], [0.812, 0.254, 0.875, 0.329], [0.36, 0.234, 0.414, 0.328]], "baseball glove": [[0.332, 0.307, 0.385, 0.44]]}], "response": "Task: What is the main color of the uniform worn by the player in <Region>[0.721, 0.244, 0.775, 0.33]</Region>?\nOutput: From the given information, the player in the specified region is likely to be wearing a San Francisco uniform. Therefore, the main color of the uniform is black with white accents.", "instruction": "What is the main color of the uniform worn by the player in <Region>[0.721, 0.244, 0.775, 0.33]</Region>?", "input": "", "output": "From the given information, the player in the specified region is likely to be wearing a San Francisco uniform. Therefore, the main color of the uniform is black with white accents.", "is_classification": false, "dataset": "COCO", "img_info": "000000040886.jpg"}
{"img_id": 65051, "query": [["A person in a park holding a kite.", "A picture of a park and a man tangled up in a kite.", "A person is holdiing a kite in a field.", "Some people stand on the grass with kites. ", "some people playing in a park with a kite"], {"person": [[0.461, 0.392, 0.63, 0.773], [0.199, 0.451, 0.282, 0.594], [0.011, 0.444, 0.03, 0.474], [0.065, 0.444, 0.073, 0.465], [0.097, 0.448, 0.112, 0.479], [0.269, 0.445, 0.276, 0.46], [0.077, 0.444, 0.088, 0.468], [0.025, 0.448, 0.046, 0.465], [0.054, 0.448, 0.066, 0.47], [0.913, 0.449, 0.923, 0.455], [0.9, 0.449, 0.913, 0.468], [0.05, 0.444, 0.061, 0.465], [0.0, 0.448, 0.012, 0.475], [0.115, 0.166, 0.875, 0.604]], "kite": [[0.373, 0.393, 0.613, 0.645]]}], "response": "Task: Is the person holding the kite in <Region>[0.461, 0.392, 0.63, 0.773]</Region> standing by themselves in the park?\nOutput: No, there are other people in the park as well, visible in other regions of the image.", "instruction": "Is the person holding the kite in <Region>[0.461, 0.392, 0.63, 0.773]</Region> standing by themselves in the park?", "input": "", "output": "No, there are other people in the park as well, visible in other regions of the image.", "is_classification": false, "dataset": "COCO", "img_info": "000000065051.jpg"}
{"img_id": 29607, "query": [["A woman sanding in front of a cake on top of a table.", "A family looks down at a pink and white cake.", "A woman surprised at seeing a birthday cake, with other family members.", "Woman and three children standing around a cake on a counter.", "A family looks at a birthday cake inside their home."], {"chair": [[0.044, 0.553, 0.351, 0.827], [0.0, 0.521, 0.073, 0.765], [0.649, 0.745, 0.697, 0.878]], "dining table": [[0.002, 0.722, 0.935, 0.989]], "person": [[0.267, 0.117, 0.589, 0.828], [0.667, 0.341, 0.91, 0.965], [0.785, 0.309, 0.9, 0.619], [0.585, 0.231, 0.784, 0.883]], "cake": [[0.389, 0.751, 0.59, 0.908]]}], "response": "Task: What is the main color of the cake in <Region>[0.389, 0.751, 0.59, 0.908]</Region>?\nOutput: The main color of the cake in the given region is pink.", "instruction": "What is the main color of the cake in <Region>[0.389, 0.751, 0.59, 0.908]</Region>?", "input": "", "output": "The main color of the cake in the given region is pink.", "is_classification": false, "dataset": "COCO", "img_info": "000000029607.jpg"}
{"img_id": 32738, "query": [["A woman holding up a small gray cat in the air.", "Grey cat being help up by a woman in a white shirt. ", "A lady uses both hands to hold a cat in the air.", "A grey cat is being held by a woman at a cat show.", "A woman holding up a cat to look at with while people look on in the background."], {"cat": [[0.369, 0.194, 0.795, 0.988]], "person": [[0.002, 0.118, 0.64, 0.98], [0.757, 0.466, 0.997, 0.989], [0.305, 0.002, 0.607, 0.73], [0.802, 0.002, 1.0, 0.578], [0.622, 0.041, 0.876, 0.809]], "clock": [[0.52, 0.478, 0.558, 0.545]]}], "response": "Task: What is the woman in <Region>[0.369, 0.194, 0.795, 0.988]</Region> wearing?\nOutput: As per the information in the sentences, it is not mentioned what the woman is wearing in this region of the picture.", "instruction": "What is the woman in <Region>[0.369, 0.194, 0.795, 0.988]</Region> wearing?", "input": "", "output": "As per the information in the sentences, it is not mentioned what the woman is wearing in this region of the picture.", "is_classification": false, "dataset": "COCO", "img_info": "000000032738.jpg"}
{"img_id": 46616, "query": [["The two surfers are ready to go out on the water", "Two women by the ocean in front of a surf board. ", "A couple of people on a beach with a surfboard.", "Two women who are standing in the sand near a surfboard.", "a couple of people are standing with a surfboard"], {"person": [[0.592, 0.301, 0.805, 0.746], [0.149, 0.309, 0.359, 0.771], [0.925, 0.365, 0.937, 0.371]], "surfboard": [[0.329, 0.029, 0.635, 0.736]]}], "response": "Task: What is the activity of the persons in <Region>[0.592, 0.301, 0.805, 0.746]</Region> based on their clothing?\nOutput: As they are wearing wetsuits and standing on the sand in front of a surfboard, the persons in the specified region are most likely to be surfers and are preparing to go out in the water.", "instruction": "What is the activity of the persons in <Region>[0.592, 0.301, 0.805, 0.746]</Region> based on their clothing?", "input": "", "output": "As they are wearing wetsuits and standing on the sand in front of a surfboard, the persons in the specified region are most likely to be surfers and are preparing to go out in the water.", "is_classification": false, "dataset": "COCO", "img_info": "000000046616.jpg"}
{"img_id": 18658, "query": [["Three big screen monitors are displayed on a neat desk.", "A desk with three monitors linked up to each other. ", "Three computer monitors sitting on top of a wooden table.", "A desk with papers and three computer screens on it.", "Three computer monitors on a desk with paper and a coke bottle."], {"tv": [[0.319, 0.259, 0.742, 0.621]], "bottle": [[0.002, 0.611, 0.084, 0.943]], "keyboard": [[0.033, 0.972, 0.595, 1.0]], "potted plant": [[0.003, 0.0, 0.141, 0.217]], "laptop": [[0.73, 0.183, 0.999, 0.673], [0.769, 0.817, 1.0, 1.0]]}], "response": "Task: What is the position of the left monitor in <Region>[0.319, 0.259, 0.742, 0.621]</Region> relative to the other monitors?\nOutput: The screen in the region is located in the center, flanked by two equally sized monitors on left and right respectively.", "instruction": "What is the position of the left monitor in <Region>[0.319, 0.259, 0.742, 0.621]</Region> relative to the other monitors?", "input": "", "output": "The screen in the region is located in the center, flanked by two equally sized monitors on left and right respectively.", "is_classification": false, "dataset": "COCO", "img_info": "000000018658.jpg"}
{"img_id": 5288, "query": [["a small dog is holding a brush in its mouth", "A dog holding a brush on a bed with someone laying.", "A dog holding a hair brush beside a person in bed", "A dog holding a brush in its mouth, on a bed.", "A dog standing on a bed with a brush in its mouth."], {"dog": [[0.0, 0.073, 0.629, 0.988]], "person": [[0.532, 0.002, 0.794, 0.283], [0.528, 0.002, 1.0, 0.29]], "bed": [[0.003, 0.0, 1.0, 0.989], [0.78, 0.218, 0.998, 0.374]]}], "response": "Task: Which object is closer to the dog in <Region>[0.0, 0.073, 0.629, 0.988]</Region>, the bed or the person?\nOutput: Both the bed and the person are there in the region, but the bed is closer to the dog compared to the person.", "instruction": "Which object is closer to the dog in <Region>[0.0, 0.073, 0.629, 0.988]</Region>, the bed or the person?", "input": "", "output": "Both the bed and the person are there in the region, but the bed is closer to the dog compared to the person.", "is_classification": false, "dataset": "COCO", "img_info": "000000005288.jpg"}
{"img_id": 50913, "query": [["Many surf boards are lined up in a row at the beach.", "A grouping of surfboards leaning in the sand on a beach.", "Group of surf boards on a beach next to ocean", "A row of surf boards stand up in the sand.", "a rack of surf boards on a beach "], {"person": [[0.084, 0.544, 0.098, 0.571], [0.033, 0.519, 0.066, 0.585], [0.0, 0.522, 0.03, 0.592], [0.225, 0.525, 0.248, 0.572], [0.317, 0.528, 0.327, 0.549], [0.321, 0.532, 0.342, 0.556], [0.298, 0.534, 0.307, 0.544], [0.214, 0.55, 0.229, 0.573], [0.172, 0.534, 0.184, 0.544], [0.306, 0.532, 0.321, 0.547], [0.111, 0.539, 0.12, 0.545], [0.306, 0.548, 0.322, 0.566], [0.133, 0.496, 0.14, 0.503], [0.135, 0.492, 0.16, 0.509]], "surfboard": [[0.595, 0.393, 0.856, 0.798], [0.464, 0.242, 0.737, 0.754], [0.52, 0.348, 0.809, 0.774], [0.417, 0.321, 0.597, 0.594], [0.313, 0.361, 0.53, 0.717], [0.354, 0.338, 0.538, 0.552], [0.451, 0.323, 0.597, 0.543], [0.355, 0.441, 0.533, 0.775], [0.349, 0.432, 0.408, 0.562]]}], "response": "Task: How many surfboards are there in the row or rack on the beach in <Region>[0.313, 0.361, 0.53, 0.717]</Region>?\nOutput: There are 6 surfboards in the row or rack on the beach in the specified region.", "instruction": "How many surfboards are there in the row or rack on the beach in <Region>[0.313, 0.361, 0.53, 0.717]</Region>?", "input": "", "output": "There are 6 surfboards in the row or rack on the beach in the specified region.", "is_classification": false, "dataset": "COCO", "img_info": "000000050913.jpg"}
{"img_id": 20966, "query": [["A herd of bulls walking through a town guided by men.", "A herd of cattle is pushed through a street past people and vendors.", "Cows walking in a row down an Indian street", "A herd of cattle being led down a dirt road.", "a bunch of cows are standing in a pin"], {"cow": [[0.206, 0.434, 0.798, 0.989], [0.621, 0.449, 1.0, 0.922], [0.838, 0.307, 1.0, 0.405], [0.329, 0.304, 0.653, 0.419], [0.175, 0.328, 0.613, 0.458], [0.0, 0.42, 0.166, 0.522], [0.001, 0.389, 0.31, 0.514], [0.014, 0.291, 0.289, 0.417], [0.773, 0.265, 1.0, 0.352], [0.0, 0.479, 0.335, 0.93], [0.775, 0.365, 1.0, 0.506], [0.96, 0.436, 1.0, 0.506], [0.599, 0.303, 0.857, 0.464], [0.003, 0.325, 0.917, 0.55]], "banana": [[0.278, 0.194, 0.323, 0.243]], "apple": [[0.225, 0.224, 0.282, 0.252], [0.233, 0.191, 0.272, 0.218], [0.254, 0.241, 0.282, 0.259], [0.189, 0.162, 0.254, 0.2], [0.209, 0.198, 0.234, 0.212]], "person": [[0.17, 0.199, 0.248, 0.338], [0.107, 0.146, 0.178, 0.31], [0.355, 0.138, 0.413, 0.365], [0.475, 0.184, 0.525, 0.332], [0.052, 0.072, 0.096, 0.263], [0.104, 0.2, 0.18, 0.371], [0.969, 0.154, 0.999, 0.266], [0.034, 0.067, 0.062, 0.243], [0.748, 0.071, 0.779, 0.143], [0.099, 0.084, 0.14, 0.184], [0.0, 0.11, 0.047, 0.321], [0.879, 0.657, 1.0, 0.999], [0.086, 0.078, 0.116, 0.144]], "orange": [[0.287, 0.183, 0.306, 0.201], [0.233, 0.238, 0.29, 0.289], [0.222, 0.171, 0.253, 0.197], [0.261, 0.228, 0.27, 0.239], [0.199, 0.186, 0.228, 0.2], [0.246, 0.231, 0.255, 0.242]], "motorcycle": [[0.73, 0.138, 0.762, 0.229]]}], "response": "Task: How many cows are in the herd in <Region>[0.001, 0.389, 0.31, 0.514]</Region> compared to <Region>[0.775, 0.365, 1.0, 0.506]</Region>?\nOutput: There are many cows in both of the regions, but the first one seems to have more cows, while the second one contains two cows.", "instruction": "How many cows are in the herd in <Region>[0.001, 0.389, 0.31, 0.514]</Region> compared to <Region>[0.775, 0.365, 1.0, 0.506]</Region>?", "input": "", "output": "There are many cows in both of the regions, but the first one seems to have more cows, while the second one contains two cows.", "is_classification": false, "dataset": "COCO", "img_info": "000000020966.jpg"}
{"img_id": 9845, "query": [["a couple of men stand next to a city bus ", "a close up ofa transit bus on a city street ", "Two people walking next to a passenger bus with curtains on the windows.", "A man in front of a bus with eagles drawn on it", "A passenger bus that has two birds drawn on it."], {"bird": [[0.442, 0.364, 0.633, 0.556], [0.329, 0.409, 0.425, 0.542]], "bus": [[0.002, 0.002, 1.0, 0.84]], "person": [[0.555, 0.29, 0.584, 0.337], [0.437, 0.226, 0.502, 0.341], [0.322, 0.147, 0.38, 0.262], [0.798, 0.775, 0.969, 0.997], [0.0, 0.418, 0.146, 0.999]]}], "response": "Task: What is the main color of the birds in <Region>[0.329, 0.409, 0.425, 0.542]</Region> drawn on the bus?\nOutput: The birds drawn on the bus are eagles, whose main color is white.", "instruction": "What is the main color of the birds in <Region>[0.329, 0.409, 0.425, 0.542]</Region> drawn on the bus?", "input": "", "output": "The birds drawn on the bus are eagles, whose main color is white.", "is_classification": false, "dataset": "COCO", "img_info": "000000009845.jpg"}
{"img_id": 50061, "query": [["Two guys in blue aprons cooking in the kitchen.", "Two men prepare food in a commercial kitchen.", "A couple of men that are standing in a kitchen.", "A couple of guys are in the kitchen preparing. ", "Two cooks in a kitchen preparing food for a meal."], {"bottle": [[0.077, 0.646, 0.106, 0.702]], "person": [[0.203, 0.161, 0.764, 0.989], [0.41, 0.299, 0.581, 0.995]], "knife": [[0.572, 0.645, 0.716, 0.754]], "cup": [[0.143, 0.704, 0.177, 0.779]], "bowl": [[0.09, 0.508, 0.139, 0.552], [0.742, 0.425, 0.797, 0.451], [0.744, 0.448, 0.787, 0.457], [0.066, 0.389, 0.138, 0.402], [0.823, 0.458, 0.86, 0.48], [0.067, 0.383, 0.139, 0.392]]}], "response": "Task: What are the items in <Region>[0.572, 0.645, 0.716, 0.754]</Region> and <Region>[0.823, 0.458, 0.86, 0.48]</Region>?\nOutput: The first region contains a knife, and the second region contains a bowl.", "instruction": "What are the items in <Region>[0.572, 0.645, 0.716, 0.754]</Region> and <Region>[0.823, 0.458, 0.86, 0.48]</Region>?", "input": "", "output": "The first region contains a knife, and the second region contains a bowl.", "is_classification": false, "dataset": "COCO", "img_info": "000000050061.jpg"}
{"img_id": 32963, "query": [["A paper plate topped with three different types of Chinese food.", "A noodle and meat dish with broccoli on a white plate.", "The meal has broccoli, and fried noodles on it.", "This plate features noodles, broccoli and spicy chicken", "A plate of cooked food on a table."], {"fork": [[0.664, 0.004, 0.823, 0.154]], "broccoli": [[0.271, 0.566, 0.551, 0.865], [0.628, 0.607, 0.885, 0.833]], "dining table": [[0.0, 0.0, 1.0, 0.987]]}], "response": "Task: What is the most likely position of the food mentioned in <Region>[0.628, 0.607, 0.885, 0.833]</Region> on the plate?\nOutput: The broccoli is on one side of the plate, while the noodles and the spicy chicken are on the other side but are mixed together.", "instruction": "What is the most likely position of the food mentioned in <Region>[0.628, 0.607, 0.885, 0.833]</Region> on the plate?", "input": "", "output": "The broccoli is on one side of the plate, while the noodles and the spicy chicken are on the other side but are mixed together.", "is_classification": false, "dataset": "COCO", "img_info": "000000032963.jpg"}
{"img_id": 37119, "query": [["Black and white cows stand around in a farm yard.", "Multiple black and white cows standing in a cow pen.", "Black and white cows with ear tags facing the camera.", "Holstein calves with tags on their ears stand in a corral", "A group of cows are standing inside a fence."], {"cow": [[0.232, 0.157, 0.521, 0.953], [0.435, 0.116, 0.729, 0.656], [0.846, 0.114, 0.908, 0.37], [0.149, 0.116, 0.236, 0.345]]}], "response": "Task: How many cows are in <Region>[0.435, 0.116, 0.729, 0.656]</Region>?\nOutput: It is not accurately determinable, but approximately there are around 5-6 cows in this region.", "instruction": "How many cows are in <Region>[0.435, 0.116, 0.729, 0.656]</Region>?", "input": "", "output": "It is not accurately determinable, but approximately there are around 5-6 cows in this region.", "is_classification": false, "dataset": "COCO", "img_info": "000000037119.jpg"}
{"img_id": 30711, "query": [["A black dog is sitting on a dining chair.", "A dog sitting in a chair in front of a window.", "A dog on a chair in a room.", "A dog that is sitting in a wooden chair.", "A dog sitting in a chair looking patiently."], {"dog": [[0.207, 0.131, 0.698, 0.885]], "chair": [[0.194, 0.379, 0.603, 0.984]], "cup": [[0.008, 0.419, 0.049, 0.477]], "book": [[0.903, 0.248, 1.0, 0.298], [0.736, 0.475, 0.892, 0.506], [0.924, 0.382, 0.998, 0.416], [0.921, 0.428, 1.0, 0.463], [0.897, 0.448, 1.0, 0.505], [0.901, 0.215, 1.0, 0.264], [0.905, 0.325, 1.0, 0.354], [0.91, 0.348, 1.0, 0.37], [0.0, 0.472, 0.157, 0.509], [0.105, 0.424, 0.228, 0.461]]}], "response": "Task: What is the approximate height of the chair in <Region>[0.194, 0.379, 0.603, 0.984]</Region> compared to an average chair?\nOutput: Since no information is given about an average chair, it is not possible to compare in a precise manner. However, the given chair looks slightly lower than the average chair.", "instruction": "What is the approximate height of the chair in <Region>[0.194, 0.379, 0.603, 0.984]</Region> compared to an average chair?", "input": "", "output": "Since no information is given about an average chair, it is not possible to compare in a precise manner. However, the given chair looks slightly lower than the average chair.", "is_classification": false, "dataset": "COCO", "img_info": "000000030711.jpg"}
{"img_id": 65197, "query": [["A motorcyclist riding on a roadway in the country.", "Two people riding down the road on a motorcycle", "Two people lean into a turn on a chrome motorcycle. ", "Two people on a motorcycle are going around a bed.", "Two people on a motorcycle, driving on a highway in the mountains."], {"motorcycle": [[0.385, 0.463, 0.654, 0.804]], "person": [[0.403, 0.449, 0.512, 0.737], [0.384, 0.438, 0.475, 0.732]]}], "response": "Task: What is the position of the motorcyclist in <Region>[0.385, 0.463, 0.654, 0.804]</Region> in relation to the passenger on the motorcycle?\nOutput: The person in the specified region is the driver or the one controlling the motorcycle, while the other person is a passenger seated behind him/her.", "instruction": "What is the position of the motorcyclist in <Region>[0.385, 0.463, 0.654, 0.804]</Region> in relation to the passenger on the motorcycle?", "input": "", "output": "The person in the specified region is the driver or the one controlling the motorcycle, while the other person is a passenger seated behind him/her.", "is_classification": false, "dataset": "COCO", "img_info": "000000065197.jpg"}
{"img_id": 20908, "query": [["A table is adorned with cake, berries and bagels along with other empty dishes. ", "Plates filled with cakes and deserts on top of a wooden table.", "Dinner table set with different fruits, pies, and breads. ", "A table full of food, plates, and silverware.", "A table is set for dinner, with food and dining plates."], {"bottle": [[0.532, 0.119, 0.628, 0.396]], "chair": [[0.532, 0.13, 0.812, 0.218]], "dining table": [[0.856, 0.266, 0.969, 0.602], [0.056, 0.219, 0.949, 0.951]], "cup": [[0.598, 0.689, 0.765, 0.808], [0.583, 0.318, 0.67, 0.431], [0.685, 0.203, 0.748, 0.274], [0.835, 0.202, 0.926, 0.267], [0.801, 0.169, 0.875, 0.234], [0.202, 0.411, 0.312, 0.486]], "spoon": [[0.718, 0.433, 0.922, 0.487], [0.101, 0.508, 0.193, 0.532], [0.75, 0.231, 0.786, 0.263], [0.704, 0.934, 0.802, 0.967]], "orange": [[0.416, 0.161, 0.47, 0.209]], "person": [[0.065, 0.027, 0.428, 0.352]], "fork": [[0.721, 0.858, 0.954, 0.954], [0.711, 0.409, 0.932, 0.468]], "bowl": [[0.191, 0.463, 0.41, 0.629], [0.308, 0.352, 0.576, 0.501], [0.388, 0.556, 0.573, 0.706], [0.72, 0.472, 0.888, 0.581], [0.448, 0.436, 0.722, 0.599], [0.38, 0.915, 0.616, 0.971], [0.034, 0.631, 0.187, 0.781], [0.676, 0.268, 0.784, 0.329], [0.585, 0.32, 0.665, 0.347], [0.318, 0.16, 0.525, 0.269]], "apple": [[0.335, 0.175, 0.378, 0.208]], "cake": [[0.083, 0.661, 0.524, 0.969]], "knife": [[0.178, 0.388, 0.284, 0.406]]}], "response": "Task: What item(s) can be seen inside the bowl in <Region>[0.318, 0.16, 0.525, 0.269]</Region>?\nOutput: It is not clear from the image what exactly is inside the bowl in the specified region.", "instruction": "What item(s) can be seen inside the bowl in <Region>[0.318, 0.16, 0.525, 0.269]</Region>?", "input": "", "output": "It is not clear from the image what exactly is inside the bowl in the specified region.", "is_classification": false, "dataset": "COCO", "img_info": "000000020908.jpg"}
{"img_id": 66444, "query": [["Two men wearing red coats and brown hats, riding homemade horses.", "Two men in red outfits wearing brown hats and wearing a fake black horse costume. ", "two men in red uniforms on horses being stared at", "Some old guys in funny costumes on some fake horses.", "Two men in red jackets are sitting on wooden horses."], {"person": [[0.333, 0.067, 0.569, 1.0], [0.246, 0.2, 0.358, 0.502], [0.137, 0.636, 0.268, 0.989], [0.664, 0.169, 0.782, 0.423], [0.598, 0.391, 0.656, 0.649], [0.323, 0.175, 0.375, 0.337], [0.056, 0.173, 0.11, 0.393], [0.873, 0.189, 0.997, 0.62], [0.563, 0.196, 0.658, 0.632], [0.673, 0.092, 0.957, 1.0], [0.162, 0.179, 0.218, 0.356], [0.031, 0.189, 0.089, 0.419], [0.367, 0.193, 0.4, 0.276], [0.016, 0.145, 0.952, 0.456]], "horse": [[0.09, 0.379, 0.386, 0.879], [0.619, 0.464, 0.815, 0.999]]}], "response": "Task: Which person in <Region>[0.246, 0.2, 0.358, 0.502]</Region> is farther from their horse in comparison to the other person?\nOutput: As both persons are not very near to their horses, both are at a comparable distance from their respective horses.", "instruction": "Which person in <Region>[0.246, 0.2, 0.358, 0.502]</Region> is farther from their horse in comparison to the other person?", "input": "", "output": "As both persons are not very near to their horses, both are at a comparable distance from their respective horses.", "is_classification": false, "dataset": "COCO", "img_info": "000000066444.jpg"}
{"img_id": 70370, "query": [["The laptop is on a desk near an office chair.", "A small table with a laptop computer and a chair in front of it.", "A computer, lamp, and books sit on a desk.", "a laptop sits on a desk next to a mouse", "A computer desk, chair, table, and laptop. "], {"chair": [[0.054, 0.449, 0.585, 0.991]], "laptop": [[0.399, 0.074, 0.69, 0.389]], "mouse": [[0.749, 0.39, 0.8, 0.451]], "book": [[0.695, 0.189, 0.856, 0.298], [0.068, 0.72, 0.269, 0.836]]}], "response": "Task: What is the position of the mouse in <Region>[0.749, 0.39, 0.8, 0.451]</Region> relative to the laptop, and what other object is present between these two objects?\nOutput: The mouse is on the right side of the laptop in a close distance. There is no object between the mouse and the laptop.", "instruction": "What is the position of the mouse in <Region>[0.749, 0.39, 0.8, 0.451]</Region> relative to the laptop, and what other object is present between these two objects?", "input": "", "output": "The mouse is on the right side of the laptop in a close distance. There is no object between the mouse and the laptop.", "is_classification": false, "dataset": "COCO", "img_info": "000000070370.jpg"}
{"img_id": 31699, "query": [["A man kneeling over a white surfboard on the flatbed of a truck.", "A man is touching a surf board in the back of a truck.", "A man working on his surfboard in the back of a pickup truck.", "a man with eye glasses doing something to a surf board", "A man waxing his surfboard in the back of his truck"], {"person": [[0.51, 0.011, 0.965, 0.84], [0.145, 0.165, 0.171, 0.251], [0.036, 0.126, 0.098, 0.321], [0.161, 0.16, 0.187, 0.252]], "surfboard": [[0.131, 0.448, 0.771, 0.984]], "car": [[0.0, 0.266, 0.056, 0.388], [0.921, 0.255, 0.965, 0.34]], "truck": [[0.292, 0.142, 0.543, 0.309], [0.002, 0.312, 1.0, 0.979]]}], "response": "Task: What is the man doing to the surfboard in <Region>[0.131, 0.448, 0.771, 0.984]</Region>?\nOutput: It is not very clear what he is doing from the image, but possibly he is waxing or sanding the board to maintain or repair it.", "instruction": "What is the man doing to the surfboard in <Region>[0.131, 0.448, 0.771, 0.984]</Region>?", "input": "", "output": "It is not very clear what he is doing from the image, but possibly he is waxing or sanding the board to maintain or repair it.", "is_classification": false, "dataset": "COCO", "img_info": "000000031699.jpg"}
{"img_id": 389, "query": [["A man with a bushy beard and green tie.", "This man is very happy to be at the gathering.", "A woman standing in a large group of people near stairs.", "The man smiles wearing a green neck tie near a crowd of people. ", "A man with a long grey beard in a white shirt and green tie at a pub full of green-shirted patrons."], {"tie": [[0.479, 0.627, 0.581, 1.0]], "person": [[0.0, 0.535, 0.221, 0.998], [0.73, 0.609, 0.883, 1.0], [0.143, 0.52, 0.286, 0.876], [0.886, 0.564, 1.0, 0.824], [0.065, 0.36, 0.087, 0.452], [0.267, 0.594, 0.342, 0.693], [0.671, 0.536, 0.739, 0.636], [0.616, 0.505, 0.667, 0.56], [0.879, 0.775, 1.0, 1.0], [0.22, 0.175, 0.795, 0.987], [0.809, 0.562, 0.822, 0.582], [0.808, 0.562, 0.923, 0.867]], "backpack": [[0.603, 0.555, 0.723, 1.0]]}], "response": "Task: Is the person wearing the green tie in <Region>[0.479, 0.627, 0.581, 1.0]</Region> facing towards the left or right?\nOutput: The man with the green tie in the given region is facing towards the right.", "instruction": "Is the person wearing the green tie in <Region>[0.479, 0.627, 0.581, 1.0]</Region> facing towards the left or right?", "input": "", "output": "The man with the green tie in the given region is facing towards the right.", "is_classification": false, "dataset": "COCO", "img_info": "000000000389.jpg"}
{"img_id": 20460, "query": [["A white truck with an advertisement for \" Buster's 24 HR. Service \" painted on the passenger door.", "a close up of a business truck with writing ", "A 24 hour service truck with the company name Buster's ", "A truck that is advertising Buster's 24 Hr. Service", "Truck on the street with advertisement for a business on it's door"], {"truck": [[0.0, 0.026, 1.0, 0.986]], "person": [[0.04, 0.173, 0.095, 0.446], [0.38, 0.253, 0.465, 0.322]]}], "response": "Task: What is written on the advertisement on the passenger door of the truck in <Region>[0.0, 0.026, 1.0, 0.986]</Region>?\nOutput: The advertisement on the passenger door of the truck in the specified region reads \"Buster's 24 HR. Service\".", "instruction": "What is written on the advertisement on the passenger door of the truck in <Region>[0.0, 0.026, 1.0, 0.986]</Region>?", "input": "", "output": "The advertisement on the passenger door of the truck in the specified region reads \"Buster's 24 HR. Service\".", "is_classification": false, "dataset": "COCO", "img_info": "000000020460.jpg"}
{"img_id": 47972, "query": [["Children gather around a U.S. soldier as he slices a large cake. ", "There is a military man coming home cutting his cake with friends and family", "A group of people standing around a large cake.", "Man in fatigues cutting cake in a room with Thanksgiving decorations, and full of mostly children.", "An army man cutting a cake with young children watching. "], {"knife": [[0.559, 0.589, 0.565, 0.619]], "cake": [[0.474, 0.611, 0.655, 0.761]], "person": [[0.66, 0.256, 0.899, 0.981], [0.377, 0.428, 0.504, 1.0], [0.161, 0.724, 0.343, 1.0], [0.462, 0.285, 0.588, 0.65], [0.586, 0.755, 0.737, 0.986], [0.305, 0.462, 0.383, 0.919], [0.23, 0.496, 0.331, 0.976], [0.575, 0.259, 0.643, 0.428], [0.561, 0.398, 0.658, 0.622], [0.84, 0.282, 1.0, 0.943], [0.949, 0.208, 1.0, 0.45], [0.746, 0.183, 0.797, 0.276], [0.046, 0.21, 0.153, 0.743], [0.0, 0.042, 0.998, 0.998]]}], "response": "Task: Which person in <Region>[0.66, 0.256, 0.899, 0.981]</Region> is the closest to the person cutting the cake?\nOutput: The person closest to the one cutting the cake is the girl in the front holding a white plate, nearest to the cutting soldier's right hand.", "instruction": "Which person in <Region>[0.66, 0.256, 0.899, 0.981]</Region> is the closest to the person cutting the cake?", "input": "", "output": "The person closest to the one cutting the cake is the girl in the front holding a white plate, nearest to the cutting soldier's right hand.", "is_classification": false, "dataset": "COCO", "img_info": "000000047972.jpg"}
{"img_id": 28855, "query": [["a group of children sit on top of a parked motor bike", "Three young children laugh and play on a motorcycle.", "Two little girls and a little boy in the middle, smiling and sitting on a motorcycle with its front headlight on.", "Three girls sitting on a scooter at night ", "Three children are riding on a motorcycle together."], {"bicycle": [[0.282, 0.287, 0.546, 0.602], [0.193, 0.228, 0.472, 0.41], [0.303, 0.245, 0.376, 0.356]], "person": [[0.374, 0.202, 0.892, 0.96], [0.589, 0.22, 0.821, 0.986], [0.748, 0.029, 0.936, 0.982]], "motorcycle": [[0.0, 0.002, 1.0, 0.998]], "car": [[0.462, 0.179, 0.518, 0.298], [0.494, 0.1, 0.701, 0.272]]}], "response": "Task: What is the total number of children riding the motorcycle in <Region>[0.589, 0.22, 0.821, 0.986]</Region>?\nOutput: There are three children riding the motorcycle in the specified region.", "instruction": "What is the total number of children riding the motorcycle in <Region>[0.589, 0.22, 0.821, 0.986]</Region>?", "input": "", "output": "There are three children riding the motorcycle in the specified region.", "is_classification": false, "dataset": "COCO", "img_info": "000000028855.jpg"}
{"img_id": 49984, "query": [["a bathroom simply designed with pale neutral tan colors", "A bathroom with the toilet seat up beside the counter.", "A bathroom that has a towel rack over the toilet.", "The seat is up on a toilet in a white bathroom.", "A warmly colored bathroom sits unoccupied in fluorescent light."], {"bottle": [[0.785, 0.472, 0.823, 0.534], [0.731, 0.469, 0.774, 0.539], [0.823, 0.473, 0.866, 0.537]], "toilet": [[0.313, 0.546, 0.562, 0.925]], "sink": [[0.865, 0.538, 1.0, 0.542]]}], "response": "Task: What is located around the toilet in the bathroom in <Region>[0.313, 0.546, 0.562, 0.925]</Region> based on the information provided?\nOutput: A towel rack is located above the toilet in this bathroom.", "instruction": "What is located around the toilet in the bathroom in <Region>[0.313, 0.546, 0.562, 0.925]</Region> based on the information provided?", "input": "", "output": "A towel rack is located above the toilet in this bathroom.", "is_classification": false, "dataset": "COCO", "img_info": "000000049984.jpg"}
{"img_id": 16665, "query": [["A stop light is next to a glassy building.", "Stoplights glow red on the corner of a city street.", "a traffic light near a street sign next to a building", "View of a traffic light at a street corner from the side of a building.", "A traffic signal that has two red lights on it."], {"traffic light": [[0.291, 0.305, 0.339, 0.399], [0.299, 0.493, 0.347, 0.586]], "handbag": [[0.0, 0.743, 0.047, 0.818]], "car": [[0.156, 0.679, 0.247, 0.725], [0.25, 0.684, 0.315, 0.717], [0.333, 0.68, 0.424, 0.724], [0.026, 0.679, 0.074, 0.732], [0.489, 0.667, 0.564, 0.72], [0.931, 0.674, 0.995, 0.714], [0.419, 0.678, 0.492, 0.711], [0.484, 0.676, 0.51, 0.697], [0.89, 0.675, 0.92, 0.716], [0.749, 0.676, 0.788, 0.724], [0.351, 0.67, 0.453, 0.689], [0.297, 0.677, 0.347, 0.693], [0.001, 0.671, 0.014, 0.693], [0.165, 0.654, 0.901, 0.736]], "person": [[0.003, 0.774, 0.036, 0.919], [0.0, 0.67, 0.053, 0.782]]}], "response": "Task: At what height is the traffic light in <Region>[0.291, 0.305, 0.339, 0.399]</Region> approximately, compared to the buildings around it?\nOutput: Based on the observation of the image, the traffic light pole is approximately on the same level as the second floor windows of the building next to it.", "instruction": "At what height is the traffic light in <Region>[0.291, 0.305, 0.339, 0.399]</Region> approximately, compared to the buildings around it?", "input": "", "output": "Based on the observation of the image, the traffic light pole is approximately on the same level as the second floor windows of the building next to it.", "is_classification": false, "dataset": "COCO", "img_info": "000000016665.jpg"}
{"img_id": 2446, "query": [["a young boy in a soccer uniform kicking a ball", "A little boy kicking a soccer ball on a soccer field.", "A young boy kicking a soccer ball on a green field.", "A boy putting his leg back to kick a soccer ball.", "A boy in a soccer game about to kick the ball"], {"sports ball": [[0.636, 0.787, 0.742, 0.945]], "person": [[0.315, 0.079, 0.969, 0.919], [0.849, 0.002, 0.95, 0.335]]}], "response": "Task: What is the object that the person in <Region>[0.315, 0.079, 0.969, 0.919]</Region> is most likely looking at?\nOutput: It is not precisely determinable where the person in the region is looking at since there is not a clear indication of the direction of the gaze.", "instruction": "What is the object that the person in <Region>[0.315, 0.079, 0.969, 0.919]</Region> is most likely looking at?", "input": "", "output": "It is not precisely determinable where the person in the region is looking at since there is not a clear indication of the direction of the gaze.", "is_classification": false, "dataset": "COCO", "img_info": "000000002446.jpg"}
{"img_id": 57617, "query": [["A PICTURE OF A TOW TRUCK TAKING A WRECKED CAR AWAY ", "Worker looks at the damaged, crush car on the flatbed of his truck", "A car that has been smashed in an accident and is being towed away.", "A wrecker hauls away a heavily damaged car.", "a car being towed by a tow truck on a road"], {"car": [[0.349, 0.398, 0.784, 0.67]], "person": [[0.281, 0.803, 0.412, 1.0], [0.898, 0.828, 1.0, 1.0], [0.215, 0.854, 0.305, 0.989], [0.889, 0.533, 0.965, 0.798]], "truck": [[0.128, 0.405, 0.758, 0.786]]}], "response": "Task: Is the car in <Region>[0.349, 0.398, 0.784, 0.67]</Region> facing the same direction as the tow truck?\nOutput: Yes, the car is facing the same direction as the tow truck.", "instruction": "Is the car in <Region>[0.349, 0.398, 0.784, 0.67]</Region> facing the same direction as the tow truck?", "input": "", "output": "Yes, the car is facing the same direction as the tow truck.", "is_classification": false, "dataset": "COCO", "img_info": "000000057617.jpg"}
{"img_id": 16089, "query": [["A white plate with a sandwich and salad on it.", "a close up of a sandwich and a salad on a plate", "A plate that has a sandwich and some salad on it.", "a white plate holding onto a sandwich and a salad ", "The sandwich was ordered at the restaurant. "], {"sandwich": [[0.462, 0.267, 0.967, 0.903], [0.003, 0.24, 0.512, 0.845], [0.502, 0.16, 0.848, 0.429]], "chair": [[0.846, 0.011, 1.0, 0.245], [0.809, 0.001, 0.852, 0.171]], "dining table": [[0.001, 0.132, 0.999, 0.555]]}], "response": "Task: What is on the <Region>[0.003, 0.24, 0.512, 0.845]</Region> part of the plate and how does it compare to the <Region>[0.462, 0.267, 0.967, 0.903]</Region> part?\nOutput: Both the regions contain the same thing, which is a sandwich and some salad. These two regions are complementary to each other, meaning what one part lacks in terms of sandwich and salad, the other part has it covered.", "instruction": "What is on the <Region>[0.003, 0.24, 0.512, 0.845]</Region> part of the plate and how does it compare to the <Region>[0.462, 0.267, 0.967, 0.903]</Region> part?", "input": "", "output": "Both the regions contain the same thing, which is a sandwich and some salad. These two regions are complementary to each other, meaning what one part lacks in terms of sandwich and salad, the other part has it covered.", "is_classification": false, "dataset": "COCO", "img_info": "000000016089.jpg"}
{"img_id": 23623, "query": [["His and her bathroom sinks underneath a large mirror.", "A bathroom vanity with two sinks and a large mirror over it.", "Two sinks are shown on a bathroom counter.", "a bathroom with two sinks at the counter with a large mirror on top", "A BATHROOM WITH A SINK AND A MIRROR "], {"sink": [[0.552, 0.465, 0.792, 0.553], [0.005, 0.569, 0.343, 0.796]]}], "response": "Task: Are the sink and the mirror in <Region>[0.552, 0.465, 0.792, 0.553]</Region> shared by multiple people?\nOutput: Based on the information given, the presence of two separate sinks in the image suggests that the sink and mirror in that specific region are intended for use by two people rather than one.", "instruction": "Are the sink and the mirror in <Region>[0.552, 0.465, 0.792, 0.553]</Region> shared by multiple people?", "input": "", "output": "Based on the information given, the presence of two separate sinks in the image suggests that the sink and mirror in that specific region are intended for use by two people rather than one.", "is_classification": false, "dataset": "COCO", "img_info": "000000023623.jpg"}
{"img_id": 22724, "query": [["The building has a tall tower with a clock.", "a tower that has a large clock at the top", "a large clock is on a tower on a building", "A view of a clock tower from across the way.", "A clock tower jutting up above several other buildings."], {"clock": [[0.363, 0.504, 0.396, 0.558], [0.426, 0.507, 0.457, 0.563]]}], "response": "Task: Are the clocks in <Region>[0.363, 0.504, 0.396, 0.558]</Region> and <Region>[0.426, 0.507, 0.457, 0.563]</Region> showing the same time?\nOutput: Based on the image, the two clocks appear to be showing the same time.", "instruction": "Are the clocks in <Region>[0.363, 0.504, 0.396, 0.558]</Region> and <Region>[0.426, 0.507, 0.457, 0.563]</Region> showing the same time?", "input": "", "output": "Based on the image, the two clocks appear to be showing the same time.", "is_classification": false, "dataset": "COCO", "img_info": "000000022724.jpg"}
{"img_id": 3103, "query": [["A man riding a skateboard up the side of a wooden ramp.", "A man is grinding a ramp on a skateboard", "Man riding on edge of an oval ramp with a skate board.", "A man in a helmet skateboarding before an audience.", "A man on a skateboard is doing a trick."], {"clock": [[0.577, 0.435, 0.601, 0.456]], "person": [[0.127, 0.043, 0.966, 0.769], [0.416, 0.76, 0.579, 0.989], [0.179, 0.187, 0.296, 0.29], [0.62, 0.717, 0.728, 0.839], [0.4, 0.584, 0.496, 0.692], [0.274, 0.556, 0.383, 0.691]], "skateboard": [[0.135, 0.472, 0.511, 0.818]]}], "response": "Task: What is the maximum height the man reaches in <Region>[0.135, 0.472, 0.511, 0.818]</Region> while riding his skateboard on the ramp? \nOutput: The given image does not provide enough visual cues to provide a confident answer to this question.", "instruction": "What is the maximum height the man reaches in <Region>[0.135, 0.472, 0.511, 0.818]</Region> while riding his skateboard on the ramp?", "input": "", "output": "The given image does not provide enough visual cues to provide a confident answer to this question.", "is_classification": false, "dataset": "COCO", "img_info": "000000003103.jpg"}
{"img_id": 9062, "query": [["This is a burger with bacon, cheese, onions and fries.", "A hamburger with fried kielbasi, cheese and onion with a side of fries. ", "A grilled sandwich with onions and cheese rests beside home fries.", "A close-up photo of a hamburger with bacon and french fries.", "The hamburger has sausage, cheese, and onions on it."], {"sandwich": [[0.219, 0.043, 0.996, 0.995]], "dining table": [[0.003, 0.005, 1.0, 0.989]], "hot dog": [[0.302, 0.447, 0.664, 0.849], [0.206, 0.357, 0.435, 0.697]]}], "response": "Task: Is there bacon on the sandwich in <Region>[0.219, 0.043, 0.996, 0.995]</Region>?\nOutput: The description mentions bacon, but it is difficult to confirm its presence in the image as there is no clear view of the sandwich.", "instruction": "Is there bacon on the sandwich in <Region>[0.219, 0.043, 0.996, 0.995]</Region>?", "input": "", "output": "The description mentions bacon, but it is difficult to confirm its presence in the image as there is no clear view of the sandwich.", "is_classification": false, "dataset": "COCO", "img_info": "000000009062.jpg"}
{"img_id": 5245, "query": [["A man with at hot on that is in the water.", "Man in a hat shirt behind an animal carrying cargo.", "a man in a canoe boat in the back of a elephant ", "People sitting on small boats in the creek. ", "a man riding an elephant that is walking in the river"], {"person": [[0.182, 0.448, 0.928, 0.989], [0.523, 0.266, 0.559, 0.287], [0.49, 0.261, 0.533, 0.298]], "elephant": [[0.473, 0.308, 0.571, 0.379], [0.093, 0.901, 0.95, 0.987]]}], "response": "Task: What is the man in <Region>[0.182, 0.448, 0.928, 0.989]</Region> doing on the river?\nOutput: The man in the specified region is most likely to be in a hot spring or bath on the river, as his posture is relaxed and sitting in the water.", "instruction": "What is the man in <Region>[0.182, 0.448, 0.928, 0.989]</Region> doing on the river?", "input": "", "output": "The man in the specified region is most likely to be in a hot spring or bath on the river, as his posture is relaxed and sitting in the water.", "is_classification": false, "dataset": "COCO", "img_info": "000000005245.jpg"}
